{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic regression applies to cases where:\n",
    "\n",
    "* $Y$ is a random qualitative variable with 2 categories (a binary variable by convention, $Y = 0$ if the event does not occur, and $Y = 1$ if it does),\n",
    "* $X_1,\\ldots,X_k$ are non-random qualitative or quantitative variables ($K$ explanatory variables in total).\n",
    "\n",
    "* $(Y, X_1,\\ldots,X_k)$ represent the population variables, from which a sample of $n$ individuals $(i)$ is drawn, and $(y, x_i)$ is the vector of observed realizations of $(Y_i, X_i)$ for each individual in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike simple linear regression, logistic regression estimates **the probability** of an event occurring, rather than predicting a specific numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $Y_i$ follow a Bernoulli distribution with parameter $p_i$ representing the probability that $Y_i=1$.    \n",
    "\n",
    "$$Y_i \\sim B(p_i)$$\n",
    "\n",
    "\n",
    "$$P(Y_i=1) = p_i \\quad, \\quad P(Y_i = 0) = 1 - p_i$$\n",
    "\n",
    "which is equivalent to: \n",
    "\n",
    "$$P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \\quad \\text{for k} \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear LOGIT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the expected value of $Y, E(Y)$, only takes values between 0 and 1, we use the logistic function:  \n",
    "\n",
    "$$f(x) = \\dfrac{\\text{exp(x)}}{1 + \\text{exp(x)}} = p$$\n",
    "\n",
    "or similarly:  \n",
    "\n",
    "$$f(x) = \\dfrac{1}{1 + \\text{exp(-x)}} = p$$\n",
    "\n",
    "This guarantees that $0 < f(x) < 1$, so $E[Y]$ can represent a valid probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit function is used to transform a probability $p$ into an **unrestricted real value**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad \\text{Notations:} \\quad X = (1,X_1, \\ldots, X_k) \\quad \\text{and} \\quad \\beta = (\\beta_0,\\beta_1, \\ldots, \\beta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1 - p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta .X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{log}\\left( \\dfrac{p}{1-p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p = \\frac{1}{1 + \\exp(-\\beta .X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions for Generalizability of the logit model\n",
    "\n",
    "* **Linearity of Log-Odds:** The relationship between each continuous predictor and the log-odds of $Y=1$ is linear. If this assumption is violated (e.g., non-linear effects), the interpretation of $\\beta_1$​ may not hold.  \n",
    "* **No Multicollinearity:** Predictors should not be highly correlated, as this can distort the interpretation of individual coefficients.  \n",
    "* **Additivity:** The effect of each predictor on the log-odds is additive. There should be no significant interaction effects unless explicitly modeled.  \n",
    "* **Independence of Observations:** The model assumes that observations are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Odds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds are defined by:  \n",
    "\n",
    "$$\\text{Odds} = \\dfrac{p}{1-p}$$\n",
    "\n",
    "\n",
    "$\\text{Where} \\quad p = P(target=1|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are '3 to 1':_ $\\text{Odds} = \\dfrac{3/4}{1/4}=3$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Notation:**\n",
    "$$\\text{Odds}(Y=1|X=0)=\\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Odds Ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio comparing the **probability of $target=1$** between individuals with value $X$ and those without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{\\text{Odds}(Y=1|X=1)}{\\text{Odds}(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \\dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that logit is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1-p}) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting the Intercept**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept $\\beta_0$​ represents the **log-odds of the outcome $Y=1$ when all predictors are equal to zero**.  \n",
    "$\\beta_0$​ defines the **baseline probability** of the outcome when all predictors are zero.  \n",
    "\n",
    "⚠️ **Caveat**:  \n",
    "This interpretation of $\\beta_0$ is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, $\\beta_0$​ is primarily a mathematical component of the model and is rarely interpreted in isolation.  \n",
    "  \n",
    "  \n",
    "* **Odds for the baseline group:**  \n",
    "\n",
    "$$\\text{Odds}(Y=1∣X_1=0)=\\exp⁡(\\beta_0)$$\n",
    "\n",
    "* **Probability for the baseline group:**\n",
    "$$P(Y=1∣X_1=0)=\\dfrac{\\exp⁡(\\beta_0)}{1 + \\exp(\\beta_0)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If $X_1$​ is \"smoking status\" ($0$ = non-smoker, $1$ = smoker), then \n",
    ">* $\\beta_0$​ gives the **log-odds** of the outcome for non-smokers  \n",
    ">* $\\exp(\\beta_0)$ gives the **odds** of the outcome for non-smokers.  \n",
    ">\n",
    ">If $\\beta_0 = -1$, then: $$\\exp(\\beta_0) = \\exp(-1) \\approx 0.37$$\n",
    ">\n",
    ">$$P(Y=1∣X_1=0)= \\dfrac{0.37}{1 + 0.37} \\approx 0.27$$\n",
    ">\n",
    ">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.  \n",
    ">It is the observed proportion of lung cancer for non-smokers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting the Slope**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a model with multiple predictors, each $\\beta_i$​ (and its corresponding odds ratio $\\exp(\\beta_i)$ represents the effect of that predictor on the log-odds of $Y=1$, holding all other predictors constant.  \n",
    "This is the key assumption of multivariable regression: ceteris paribus (all else being equal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient $\\beta_1$​ represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$​. The odds ratio $\\exp(\\beta_1)$​ quantifies how the odds of $Y=1$ change with $X_1$​.\n",
    "\n",
    "**General Formula for Odds Ratio**\n",
    "\n",
    "For any type of predictor $X_1$, the odds ratio for a one-unit increase is:  \n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)$$\n",
    "\n",
    "📌 **Note:**  \n",
    "$\\exp(\\beta_1​)$ compares the odds of $Y=1$ between $X_1=1$ and $X_1=0$, controlling for all other variables in the model (all others features constant).\n",
    "\n",
    "* Case: $X_1$ is Binary\n",
    "\n",
    "For a binary predictor $X_1$​ (e.g., $0$ = non-smoker, $1$ = smoker), the odds ratio $\\exp(\\beta_1)$​ compares the odds of $Y=1$ between the two groups.\n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 1_{\\{X_1 = 1\\}}​$$\n",
    "\n",
    "* **Odds ratio:**\n",
    "$$\\text{Odds Ratio} = \\dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \\dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \\exp(\\beta_1)$$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* If $\\exp(\\beta_1) = 1$: No effect of the feature $X_1$​ on the odds of $Y=1$.\n",
    "* If $\\exp(\\beta_1)>1$: The odds of $Y=1$ are higher when $X_1​=1$. The feature $X_1$​ is **positively associated** with the outcome.\n",
    "* If $\\exp(\\beta_1) < 1$: The odds of $Y=1$ are lower when $X_1​=1$. The feature $X_1$​ is **negatively associated** with the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Example:**  \n",
    ">If $\\beta_1 = 0.7 \\rightarrow \\exp(\\beta_1) \\approx 2.01$. The odds of lung cancer for smokers $(X_1=1)$ are twice as high as for non-smokers $(X_1=0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Case: $X_1$ is Categorical\n",
    "\n",
    "For a categorical predictor $X_1$ with more than two levels (e.g., color = red, green, blue), you use **dummy variables**. \n",
    "\n",
    "* **The logistic regression model becomes:**\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}1_{\\{X_1 = \\text{green}\\}} + \\beta_{blue}1_{\\{X_1 = \\text{blue}\\}}$$\n",
    "\n",
    "* **Reference Category (\"red\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0$$\n",
    "\n",
    "**Interpretation:** This means $\\beta_0$​ represents the log-odds of $Y=1$ for the reference category (\"red\").\n",
    "\n",
    "* **Category (\"green\"):** When $1_{\\{X_1 = \\text{green}\\}}=1$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}$$\n",
    "\n",
    "* **Category (\"blue\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=1$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{blue}$$\n",
    "\n",
    "The **odds ratio for \"blue\" relative to the reference \"red\"** is:\n",
    "$$\\exp(\\beta_{blue}) = \\dfrac{\\text{Odds}(Y=1 | \\text{blue})}{\\text{Odds}(Y=1 | \\text{red})}$$\n",
    "\n",
    "The same way, $\\exp(\\beta_{\\text{green}})$​ compares the odds for \"green\" vs. the \"red\" reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Interpretation:**\n",
    ">\n",
    ">If $\\exp(\\beta_{\\text{green}})​=1.5$, the odds of $Y=1$ are $1.5$ times higher for \"green\" compared to \"red\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Case: $X_1$ is Quantitative\n",
    "\n",
    "For a continuous predictor $X_1$ (e.g., age, blood pressure), the odds ratio $\\exp(\\beta_1)$​ represents the multiplicative change in the odds of $Y=1$ for a one-unit increase in $X_1$​.  \n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 X_1​$$\n",
    "\n",
    "* **Odds ratio for a one-unit increase:**\n",
    "\n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)​$$\n",
    "\n",
    "**In short**: $\\beta_1$​ captures the **constant log-odds** change per unit increase in $X_1$, so $\\exp(\\beta_1)$​ is the **odds ratio** for that one-unit change.\n",
    "\n",
    "This holds regardless of the starting value of $X_1$​ because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Interpretation:**\n",
    ">\n",
    ">* If $\\beta_1=0.095 \\rightarrow \\exp(\\beta_1)=1.1$, the odds of $Y=1$ increase by 10% for each one-unit increase in $X_1$​.\n",
    ">* If $X_1$​ is \"years of smoking\" and $\\beta_1 = 0.7 \\rightarrow  \\exp(\\beta_1) \\approx 2.01$. For each additional year of smoking, the odds of lung cancer double.\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "| Type of $X_1$         | Interpretation of $\\exp(\\beta_1)$                                       |\n",
    "|-----------------------|-------------------------------------------------------------------------|\n",
    "| Binary                | Compares odds of $Y=1$ between $X_1=1$ and $X_1=0$                      |\n",
    "| Categorical           | Compares odds of $Y=1$ for a given category relative to the reference.  |\n",
    "| Quantitative          | Multiplicative change in odds of $Y=1$ for a one-unit increase in $X_1$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cost function to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de coût de la régression logistique, également appelée fonction de perte logistique ou **log loss**, est une mesure de l'erreur de prédiction d'un modèle de régression logistique.  \n",
    "Elle permet d'évaluer la qualité de l'ajustement du modèle aux données d'entraînement et est utilisée pour optimiser les paramètres du modèle pendant l'apprentissage.\n",
    "\n",
    "La fonction de coût de la régression logistique est définie comme suit :\n",
    "\n",
    "$J(θ) = -1/n * \\sum(Cost(h_\\theta(x_i), y_i)$\n",
    "\n",
    "$J(θ) = -1/n * \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$\n",
    "\n",
    "où :\n",
    "\n",
    "- $n$ est le nombre d'exemples dans l'ensemble d'entraînement;  \n",
    "- $y_i$ est la valeur réelle de la variable dépendante (0 ou 1) pour l'exemple $i$;  \n",
    "- $h_\\theta(x_i)$ est la prédiction du modèle de régression logistique pour l'exemple $i$, en fonction des paramètres $\\theta$ et des variables explicatives $x_i$;  \n",
    "\n",
    "$log$ désigne le logarithme naturel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convexité\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexité est une propriété importante en optimisation, car elle garantit qu'un minimum local est également un minimum global. Cela signifie qu'il est plus facile de trouver la solution optimale lors de l'utilisation de méthodes d'optimisation telles que la descente de gradient.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, sa convexité provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexité est une propriété importante en optimisation, car elle garantit qu'un minimum local est également un minimum global. Cela signifie qu'il est plus facile de trouver la solution optimale lors de l'utilisation de méthodes d'optimisation telles que la descente de gradient.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, sa convexité provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexité d'une fonction garantit qu'un point stationnaire (c'est-à-dire un point où la dérivée est nulle) est un minimum global, mais elle ne garantit pas à elle seule l'existence d'un tel point.\n",
    "\n",
    "Pour prouver l'existence d'un extremum, il faut également montrer que la fonction est bornée inférieurement et qu'elle atteint cette borne inférieure. En d'autres termes, il faut montrer qu'il existe une valeur minimale que la fonction peut atteindre et qu'il existe au moins un point où la fonction prend cette valeur.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, on peut montrer qu'elle est bornée inférieurement par zéro (puisque le logarithme d'un nombre positif est toujours positif) et qu'elle atteint cette borne inférieure lorsque les prédictions sont parfaitement correctes (c'est-à-dire lorsque la probabilité prédite pour la classe correcte est égale à 1).\n",
    "\n",
    "En combinant la convexité de la fonction Log Loss avec le fait qu'elle est bornée inférieurement et qu'elle atteint sa borne inférieure, on peut conclure que la fonction atteint un minimum global lorsque les prédictions sont parfaitement correctes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Log-Loss was determined ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss pénalise les erreurs de prédiction en fonction de la probabilité estimée par le modèle.  \n",
    "La fonction de coût attribue une pénalité plus élevée aux exemples pour lesquels la prédiction est loin de la valeur réelle, c'est-à-dire lorsque la probabilité estimée est proche de 0 ou de 1 alors que la valeur réelle est respectivement 1 ou 0.\n",
    "\n",
    "- Si la valeur réelle $y_i$ est égale à 1 (c'est-à-dire que l'exemple appartient à la classe positive), la pénalité est égale à $-log(h_\\theta(x_i))$, où $h_\\theta(x_i)$ est la probabilité estimée par le modèle que l'exemple appartienne à la classe positive. Plus la probabilité estimée $(h_\\theta(x_i))$ est proche de 0 (et donc s'éloigne de l'observation'), plus la pénalité est élevée ($-\\log(0^+) \\approx +\\infty$).  \n",
    " \n",
    "- Si la valeur réelle $y_i$ est égale à 0 (c'est-à-dire que l'exemple appartient à la classe négative), la pénalité est égale à $-log(1 - h_\\theta(x_i))$, où $h_\\theta(x_i)$ est la probabilité estimée par le modèle que l'exemple appartienne à la classe positive. Plus la probabilité estimée est proche de 1, plus la pénalité est élevée ($-\\log(1 - 1^-) \\approx +\\infty$).  \n",
    "\n",
    "On résume les deux conditions \"si\" précédentes en une seule formule : $y_i * log(h_\\theta(x_i)) + (1 - y_i) * log(1 - h_\\theta(x_i))$ pour l'observation $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'objectif de l'apprentissage d'un modèle de régression logistique est de minimiser la fonction de coût en ajustant les paramètres $\\theta$.**\n",
    "\n",
    "Pour ce faire, on utilisera une méthode d'optimisation itérative, **la descente de gradient**, pour trouver les valeurs de $\\theta$ qui minimisent la fonction de coût sur l'ensemble d'entraînement.\n",
    "\n",
    "La fonction de coût de la régression logistique est alors définie comme la moyenne des pénalités sur tous les exemples de l'ensemble d'entraînement.\n",
    "\n",
    "En minimisant la fonction de coût, on cherche à trouver les paramètres $\\theta$ qui permettent de minimiser la somme des pénalités $\\underset{\\theta} min(J(\\theta))$  sur tous les exemples, c'est-à-dire de maximiser la probabilité d'observer les données d'entraînement en fonction des paramètres du modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $J(\\theta) = \\frac{\\partial}{\\partial \\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by transforming the expression of  $J(\\theta) = -\\frac{1}{n} \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$\n",
    "\n",
    "$\\log(h_\\theta(x_i)) = \\log(\\frac{1}{1 + \\exp(-\\theta^Tx_i)}) = -\\log(1 + \\exp(-\\theta^Tx_i))$  \n",
    "\n",
    "and \n",
    "\n",
    "$\\log(1 - h_\\theta(x_i)) = \\log(1 - \\frac{1}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\frac{1 + \\exp(-\\theta^Tx_i) - 1}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\frac{\\exp(-\\theta^Tx_i)}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\exp(-\\theta^Tx_i)) - \\log({1 + \\exp(-\\theta^Tx_i)}) = -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log(1 + exp(-\\theta^Tx_i)))]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) -\\theta^Tx_i - \\log(1 + \\exp(-\\theta^Tx_i)) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[- y_i \\log(1 + \\exp(-\\theta^Tx_i)) -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[\\cancel{- y_i \\log(1 + \\exp(-\\theta^Tx_i))} -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + \\cancel{y_i \\log(1 + \\exp(-\\theta^Tx_i))}]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  ]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with:\n",
    "\n",
    "$-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = - \\log(\\exp(\\theta^T x_i)) - \\log(1 + \\exp(-\\theta^Tx_i)) = -(\\log(\\exp(\\theta^T x_i)) + \\log(1 + \\exp(-\\theta^Tx_i)))$  \n",
    "\n",
    "$= -\\log[\\exp(\\theta^T x_i)(1 + \\exp(-\\theta^Tx_i))] = -\\log(\\exp(\\theta^T x_i + 1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(\\exp(\\theta^T x_i + 1)) ]$\n",
    "\n",
    "$\\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(1 + \\exp(\\theta^T x_i)) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i)  - \\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that: $\\theta^Tx_i = \\theta_1 {x_i}^{(1)} + \\theta_2 {x_i}^{(2)} + \\ldots + \\theta_k {x_i}^{(k)}$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i) = x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) \\underset{\\log(u)^{'} = \\frac{u^{'}}{u}} = \\frac{\\frac{\\partial}{\\partial \\theta_j}(1 + \\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = \\frac{\\frac{\\partial}{\\partial \\theta_j}(\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} \\underset{\\exp(u)^{'} = u^{'}\\exp(u)} =  \\frac{\\frac{\\partial}{\\partial \\theta_j}(\\theta^T x_i) * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$  \n",
    "\n",
    "$=  \\frac{x_i^{(j)} * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = x_i^{(j)} h_\\theta(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i x_i^{(j)}  - x_i^{(j)} h_\\theta(x_i) ] = -\\frac{1}{n} \\sum[y_i - h_\\theta(x_i) ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\iff  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sachant que l'expression de la descente de Gradient pour mettre à jour les pondérations est pour le poids $\\theta_j$ :\n",
    "\n",
    "$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "où $\\alpha$ est le learning rate, on obtient:\n",
    "\n",
    "$\\theta_j = \\theta_j - \\frac{\\alpha}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Algorithm steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $\\theta = (w,b)$   \n",
    "\n",
    "with $h_\\theta(x) = \\frac{1}{1 + \\exp(-w x + b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize weights as zero\n",
    "- Initialize bias as zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict result by using $\\hat{y} = \\frac{1}{1 + \\exp(-wx+b)}$\n",
    "- Calculate the error\n",
    "- Use Gradient descent to figure out new weights and bias values\n",
    "- Repeat n times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data point:  \n",
    "- Put the values from the data point into the equation $\\hat{y} = \\frac{1}{1 + \\exp(-w+b)}$\n",
    "- Choose the label based on the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs:\n",
    "\n",
    "$$X=\\begin{pmatrix} x_{1,1} & \\ldots & x_{1,k} \\\\ x_{2,1} & \\ldots & x_{2,k} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{n,1} & \\ldots & x_{n,k} \\end{pmatrix}, w=\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_k \\end{pmatrix}, b = \\text{constant}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model:\n",
    "\n",
    "$$X.w + b = \\begin{pmatrix} x_{1,1} & \\ldots & x_{1,k} \\\\ x_{2,1} & \\ldots & x_{2,k} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{n,1} & \\ldots & x_{n,k} \\end{pmatrix}.\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_k \\end{pmatrix} + b = \\begin{pmatrix} x_{1,1}w_1 + & \\ldots & + x_{1,k}w_k + b \\\\ x_{2,1}w_1 + & \\ldots & + x_{2,k}w_k + b \\\\ \\ldots & \\ldots &   \\ldots \\\\ x_{n,1}w_1 + & \\ldots & + x_{n,k}w_k + b \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction (output) is given by:\n",
    "\n",
    "$$\\text{sigmoid}(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p} = h_\\omega(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updates of the weights and bias are given by:\n",
    "\n",
    "$$\\omega_j = \\omega_j - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ] x_{i,j}$$\n",
    "\n",
    "$$b = b - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\omega$ using linear algebra formula:\n",
    "\n",
    "$$\\omega = X^t.(\\hat{p} - y) = \\begin{pmatrix} x_{1,1} & \\ldots & x_{1,n} \\\\ x_{2,1} & \\ldots & x_{2,n} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{k,1} & \\ldots & x_{k,n} \\end{pmatrix}.\\begin{pmatrix} \\hat{p_1} - y_1 \\\\ \\hat{p_2} - y_2 \\\\ \\ldots \\\\ \\hat{p_n} - y_n \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $b$ using linear algebra formula:\n",
    "\n",
    "$$b = \\sum(\\hat{p} - y) = \\sum\\begin{pmatrix} \\hat{p_1} - y_1 \\\\ \\hat{p_2} - y_2 \\\\ \\ldots \\\\ \\hat{p_n} - y_n \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and bias are given by:\n",
    "\n",
    "$$\\text{sigmoid}(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
    "        '''Initiate the constructor\n",
    "            INPUT:\n",
    "                learning_rate: magnitude of the step\n",
    "                n_iter: number of iterations\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Train the model\n",
    "        INPUTS:\n",
    "            X: the dataset of the features\n",
    "            y: the target\n",
    "        OUTPUTS:\n",
    "            The model\n",
    "        '''\n",
    "\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # initialize the parameters:\n",
    "        self.weights = np.zeros(self.n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            return self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        '''Update of the weights with Gradient descent'''\n",
    "\n",
    "        # we compute the prediction (the probability)\n",
    "        y_pred = 1 / (1 + np.exp( - (np.dot(self.X, self.weights) + self.bias)))\n",
    "\n",
    "        # update the weights:\n",
    "        # w_j = w_j - (alpha / n) * S(p_hat - y_i)xij\n",
    "        # b = b - (alpha / n) * S(p_hat - y_i)\n",
    "        dw = (1 / self.n_samples) * np.dot(self.X.T, (y_pred - self.y))\n",
    "        db = (1 / self.n_samples) * np.sum(y_pred - self.y)\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate*dw\n",
    "        self.bias = self.bias - self.learning_rate*db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = 1 / (1 + np.exp( - (X.dot(self.weights) + self.bias)))\n",
    "        y_pred = np.where(y_pred > 0.5 , 1 , 0)\n",
    "        return y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "rep = '/Users/davidtbo/Documents/Data_Science/99_Data'\n",
    "\n",
    "filename = os.path.join(rep, 'diabetes.csv')\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(features)\n",
    "\n",
    "standardized_data = scaler.transform(features)\n",
    "\n",
    "features = standardized_data\n",
    "target= df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Logistic_Regression(learning_rate=0.01, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754071661237785"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "training_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person is diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predictive system\n",
    "\n",
    "input_data = (5, 166, 72, 19, 175, 25.8, 0.587, 51)\n",
    "\n",
    "# to numpy array\n",
    "input_data_array = np.asarray(input_data)\n",
    "\n",
    "# Reshape\n",
    "input_data_reshape = input_data_array.reshape(1, -1)\n",
    "\n",
    "# Standardized the data\n",
    "input_data_std = scaler.transform(input_data_reshape)\n",
    "\n",
    "pred = classifier.predict(input_data_std)\n",
    "\n",
    "if pred:\n",
    "    print('The person is diabetic')\n",
    "else:\n",
    "    print('The person is not diabetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_breast_cancer()\n",
    "X, y = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = sum(y_true==y_pred)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Logistic_Regression(learning_rate=0.0001)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3986013986013986"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration:** The coefficient $\\beta_1$​ represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$​ quantitative feature.  \n",
    "\n",
    "Notations:  \n",
    "* $\\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))$ \n",
    "* $\\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))$   \n",
    "\n",
    "We know that:  \n",
    "\n",
    "* $\\text{log(Odds)}(Y=1|X=x+1)=\\beta_0 + \\beta_1 \\times (x+1)$\n",
    "* $\\text{log(Odds)}(Y=1|X=x)=\\beta_0 + \\beta_1 \\times x$   \n",
    "\n",
    "By difference:  \n",
    "* $\\text{log(Odds)}(Y=1|X=x+1) - \\text{log(Odds)}(Y=1|X=x) =\\beta_0 + \\beta_1 \\times (x+1) - (\\beta_0 + \\beta_1 \\times x) = \\beta_1$  \n",
    "\n",
    "* $\\text{log}\\left(\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)}\\right) =\\beta_1$  \n",
    "\n",
    "**CQFD**  \n",
    "\n",
    "Note:  \n",
    "\n",
    "* $\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)} = \\exp(\\beta_1)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Newton-Raphson algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compute the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de la méthode de Newton-Raphson est utilisé pour trouver les coefficients de la régression logistique en maximisant la fonction de vraisemblance.  \n",
    "La régression logistique est un modèle de régression utilisé pour prédire la probabilité d'un événement binaire en fonction d'une ou plusieurs variables prédictives.  \n",
    "\n",
    "Dans le cas de la régression logistique, la fonction de vraisemblance est une fonction convexe et peut être maximisée à l'aide de l'algorithme de Newton-Raphson.   \n",
    "L'algorithme de Newton-Raphson est une méthode itérative pour trouver le maximum d'une fonction en utilisant la dérivée et la dérivée seconde de la fonction.\n",
    "\n",
    "Dans le cas de la régression logistique, la fonction de vraisemblance est donnée par:\n",
    "\n",
    "$L(\\omega | X, y) = prod(p(yi | xi, beta)^{yi} * (1 - p(yi | xi, beta))^{(1 - yi)})$\n",
    "\n",
    "où $\\omega$ est le vecteur de coefficients de la régression logistique, $X$ est la matrice de variables prédictives, $y$ est le vecteur de variables réponses binaires et $p(yi | xi, \\omega)$ est la probabilité prédite de l'événement binaire pour l'observation $i$.\n",
    "\n",
    "Pour maximiser la fonction de vraisemblance, nous pouvons utiliser l'algorithme de Newton-Raphson. À chaque itération, l'algorithme met à jour le vecteur de coefficients beta en utilisant la formule suivante:\n",
    "\n",
    "\n",
    "$\\omega_{i+1} = \\omega_i - H^{-1} * g$  \n",
    "\n",
    "où  \n",
    "\n",
    "- $H = \\frac{\\partial^2L}{\\partial \\omega \\partial\\omega'}$ est la matrice hessienne de la fonction de vraisemblance,  \n",
    "\n",
    "- $g = \\frac{\\partial L}{\\partial \\omega }$ est le vecteur gradient de la fonction de vraisemblance et  \n",
    "\n",
    "- $\\omega_i$ est le vecteur de coefficients à l'itération $i$.\n",
    "\n",
    "La matrice hessienne et le vecteur gradient de la fonction de vraisemblance peuvent être calculés à l'aide des dérivées partielles de la fonction de vraisemblance par rapport aux coefficients beta.\n",
    "\n",
    "Donc, dans le cas de la régression logistique, l'algorithme de la méthode de Newton-Raphson est utilisé pour trouver les coefficients de la régression logistique en maximisant la fonction de vraisemblance.  \n",
    "\n",
    "Cela permet d'estimer les probabilités de l'événement binaire en fonction des variables prédictives.  \n",
    "\n",
    "NB: les itérations sont interrompues lorsque la différence entre deux vecteurs de solution successifs est négligeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients significativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wald statistic allows to test the coefficients significativity $\\hat{w_j}$. Wald statistic is given by::    \n",
    "\n",
    "$(\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2$  \n",
    "\n",
    "Under $H_0 : \\{\\hat{w_j} = 0 \\} \\Longrightarrow \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} $ ~ $\\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added-value of the variable $X_j$ is only real if the Wald statistic > 4 $(3.84 = 1.96^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Wald > 4$    \n",
    "\n",
    "$\\iff (\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2 > 4$  \n",
    "\n",
    "$\\iff \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} > 2$  \n",
    "\n",
    "$\\iff \\hat{w_j} > 2\\sigma(\\hat{w_j}) $  \n",
    "\n",
    "$\\iff \\hat{w_j} - 2\\sigma(\\hat{w_j}) > 0$  \n",
    "\n",
    "$\\iff \\hat{w_j}$ se trouve à plus de 2 écarts-type de 0  \n",
    "\n",
    "$\\iff $ l'intervalle de confiance de $\\hat{w_j}$ ne contient pas 0 à 95%  \n",
    "\n",
    "CQFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality mesure (Deviance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cf. S.Tufféry p.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n:$ number of observations  \n",
    "$k:$ number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\omega_k)$ Likelihood of the \"modèle ajusté\"  \n",
    "\n",
    "$L(\\omega_0)$ Likelihood of the \"modèle réduit à la constante\"  \n",
    "\n",
    "$L(\\omega_{max})$ Likelihood of the \"modèle saturé\". The one the model will compare.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance formula:  \n",
    "\n",
    "$D(\\omega_k) = -2[log(L(\\omega_k)) - log(L(\\omega_{max}))]$  $^{(*)}$\n",
    "\n",
    "As the target is 0 or 1 $\\Longrightarrow L(\\omega_{max})=1 \\Longrightarrow log(L(\\omega_{max}))=0$  \n",
    "\n",
    "$\\Longrightarrow D(\\omega_k) = -2[log(L(\\omega_k))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) $D(\\omega_k) = (\\frac{log(L(\\omega_k))}{log(L(\\omega_{max}))}^2)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance is equivalent to the SCE for the linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

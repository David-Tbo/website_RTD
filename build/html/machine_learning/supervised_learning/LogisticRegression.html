

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Logistic Regression: Theory, Implementation, and Application. &mdash; website Machine Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=e031e9a9"></script>
      <script src="../../_static/doctools.js?v=888ff710"></script>
      <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Supervised learning" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            website Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../probabilities/index.html">Probabilities</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Machine Leanring</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/index.html">Unsupervised learning</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Supervised learning</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#"><strong>Logistic Regression: Theory, Implementation, and Application</strong>.</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Table-of-Contents"><strong>Table of Contents</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#Introduction-to-Logistic-Regression">Introduction to Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Theory-and-Fundamentals">Theory and Fundamentals</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Practical-Implementation-of-Logistic-Regression">Practical Implementation of Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Modification-of-the-initial-dataset">Modification of the initial dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Coefficients-interpretation">Coefficients interpretation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#APPENDIX">APPENDIX</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Coefficients-significativity">Coefficients significativity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Model-quality-mesure-(Deviance)">Model quality mesure (Deviance)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">website Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Machine Leanring</a></li>
          <li class="breadcrumb-item"><a href="index.html">Supervised learning</a></li>
      <li class="breadcrumb-item active"><strong>Logistic Regression: Theory, Implementation, and Application</strong>.</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/machine_learning/supervised_learning/LogisticRegression.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="Logistic-Regression:-Theory,-Implementation,-and-Application.">
<h1><strong>Logistic Regression: Theory, Implementation, and Application</strong>.<a class="headerlink" href="#Logistic-Regression:-Theory,-Implementation,-and-Application." title="Permalink to this heading"></a></h1>
<div class="line-block">
<div class="line"><strong>Author</strong>: David Thébault.</div>
<div class="line"><strong>Date</strong>: October 18, 2025.</div>
<div class="line"><strong>Objective</strong>: Understand, implement, and apply logistic regression for diabetes prediction.</div>
</div>
<section id="Table-of-Contents">
<h2><strong>Table of Contents</strong><a class="headerlink" href="#Table-of-Contents" title="Permalink to this heading"></a></h2>
<ol class="arabic">
<li><p><a class="reference internal" href="#Introduction-to-Logistic-Regression"><span class="std std-ref">Introduction to Logistic Regression</span></a></p></li>
<li><p><a class="reference internal" href="#Theory-and-Fundamentals"><span class="std std-ref">Theory and Fundamentals</span></a></p>
<ul class="simple">
<li><p>Model and Assumptions</p></li>
<li><p>Coefficient Interpretation</p></li>
<li><p>Cost Function and Optimization</p></li>
</ul>
</li>
<li><p><cite>Implementation with ``statsmodels`</cite> &lt;#implementation-with-statsmodels&gt;`__</p>
<ul class="simple">
<li><p>Data Preparation</p></li>
<li><p>Training and Interpretation</p></li>
</ul>
</li>
<li><p>Implementation “From Scratch”</p>
<ul class="simple">
<li><p>Gradient Descent Algorithm</p></li>
<li><p>Testing and Validation</p></li>
</ul>
</li>
<li><p>Application: Diabetes Prediction</p>
<ul class="simple">
<li><p>Data Preparation</p></li>
<li><p>Prediction and Interpretation</p></li>
</ul>
</li>
<li><p>Appendices</p>
<ul class="simple">
<li><p>Mathematical Demonstrations</p></li>
<li><p>Wald Statistic and Deviance</p></li>
</ul>
</li>
<li><p class="rubric" id="references">References</p>
</li>
</ol>
</section>
<section id="Introduction-to-Logistic-Regression">
<h2>Introduction to Logistic Regression<a class="headerlink" href="#Introduction-to-Logistic-Regression" title="Permalink to this heading"></a></h2>
<p>The Logistic regression applies to cases where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> is a random qualitative variable with 2 categories (a binary variable by convention, <span class="math notranslate nohighlight">\(Y = 0\)</span> if the event does not occur, and <span class="math notranslate nohighlight">\(Y = 1\)</span> if it does),</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1,\ldots,X_k\)</span> are non-random qualitative or quantitative variables (<span class="math notranslate nohighlight">\(K\)</span> explanatory variables in total).</p></li>
<li><p><span class="math notranslate nohighlight">\((Y, X_1,\ldots,X_k)\)</span> represent the population variables, from which a sample of <span class="math notranslate nohighlight">\(n\)</span> individuals <span class="math notranslate nohighlight">\((i)\)</span> is drawn, and <span class="math notranslate nohighlight">\((y, x_i)\)</span> is the vector of observed realizations of <span class="math notranslate nohighlight">\((Y_i, X_i)\)</span> for each individual in the sample.</p></li>
</ul>
<p>Unlike simple linear regression, logistic regression estimates <strong>the probability</strong> of an event occurring, rather than predicting a specific numerical value.</p>
</section>
<section id="Theory-and-Fundamentals">
<h2>Theory and Fundamentals<a class="headerlink" href="#Theory-and-Fundamentals" title="Permalink to this heading"></a></h2>
<section id="Probabilistic-Model">
<h3>Probabilistic Model<a class="headerlink" href="#Probabilistic-Model" title="Permalink to this heading"></a></h3>
<p>This section describes the underlying probabilistic framework of the model, focusing on the random variable and its distribution.</p>
<p><strong>The Model</strong></p>
<p>The variable <span class="math notranslate nohighlight">\(Y_i\)</span> follows a <strong>Bernoulli distribution</strong> with parameter <span class="math notranslate nohighlight">\(p_i\)</span> representing the probability that <span class="math notranslate nohighlight">\(Y_i=1\)</span>.</p>
<div class="math notranslate nohighlight">
\[Y_i \sim B(p_i)\]</div>
<p>The probabilities are defined as:</p>
<div class="math notranslate nohighlight">
\[P(Y_i=1) = p_i \quad, \quad P(Y_i = 0) = 1 - p_i\]</div>
<p>This can also be written as:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>.. math:: P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \quad \text{for k} \in \{0, 1\}
</pre></div>
</div>
</section>
<section id="LOGIT-Model">
<h3>LOGIT Model<a class="headerlink" href="#LOGIT-Model" title="Permalink to this heading"></a></h3>
<p>This section focuses on the specific form of the logistic regression model, which is a type of probabilistic model.</p>
<p><strong>The Linear Logit Model</strong></p>
<p>To ensure that the expected value of <span class="math notranslate nohighlight">\(Y, E(Y)\)</span>, only takes values between 0 and 1, we use the <strong>logistic function</strong>:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{\text{exp(x)}}{1 + \text{exp(x)}} = p\]</div>
<p>or equivalently:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{1 + \text{exp(-x)}} = p\]</div>
<p>This guarantees that <span class="math notranslate nohighlight">\(0 &lt; f(x) &lt; 1\)</span>, so <span class="math notranslate nohighlight">\(E[Y]\)</span> can represent a valid probability.</p>
<p>The <strong>logit function</strong> transforms a probability <span class="math notranslate nohighlight">\(p\)</span> into an <strong>unrestricted real value</strong>:</p>
<p><strong>Notations</strong>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(X = (1,X_1, \ldots, X_k)\)</span> vector of predictors, including the intercept term.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta = (\beta_0,\beta_1, \ldots, \beta_k)\)</span> vector of coefficients.</p></li>
</ul>
<p><strong>Logit Transformation</strong></p>
<p>The logit of a probability <span class="math notranslate nohighlight">\(p\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \text{log}(\dfrac{p}{1 - p})\]</div>
<p>In the context of logistic regression, the logit of the probability is modeled as a linear combination of the predictors:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \beta .X\]</div>
<p>Explicitly, this can be written as:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]</div>
<p>Or equivalently:</p>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{p}{1-p} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]</div>
<p><strong>Inverse Transformation: Logistic Function</strong>.</p>
<p>To convert the linear predictor back into a probability, we use <strong>the logistic function (the inverse of the logit function)</strong>:</p>
<div class="math notranslate nohighlight">
\[p = \frac{1}{1 + \exp(-\beta .X)}\]</div>
<p>Demonstration:</p>
<div class="math notranslate nohighlight">
\[p(x) = \dfrac{1}{1 + \exp(-\beta x)}\]</div>
<div class="math notranslate nohighlight">
\[\underset{inverse}   \iff \dfrac{1}{p} = 1 + \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1}{p} - 1 = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1}{p} - \dfrac{p}{p} = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1-p}{p} = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \log(\dfrac{1-p}{p}) = -\beta x\]</div>
<div class="math notranslate nohighlight">
\[\iff \log(\dfrac{p}{1-p}) = \beta x\]</div>
<p>For simplicity, we often write <span class="math notranslate nohighlight">\(p\)</span> instead of <span class="math notranslate nohighlight">\(p(x)\)</span> when the context is clear.</p>
</section>
<section id="Model-Assumptions">
<h3>Model Assumptions<a class="headerlink" href="#Model-Assumptions" title="Permalink to this heading"></a></h3>
<p>For the <strong>logistic regression model</strong> to be valid and generalizable, the following key assumptions must be satisfied. Violations of these assumptions can lead to biased or inefficient estimates, and may compromise the interpretability of the model.</p>
<hr class="docutils" />
<section id="1.-Linearity-of-Log-Odds">
<h4>1. Linearity of Log-Odds<a class="headerlink" href="#1.-Linearity-of-Log-Odds" title="Permalink to this heading"></a></h4>
<p><strong>Assumption:</strong> The relationship between <strong>each continuous predictor</strong> (<span class="math notranslate nohighlight">\(X_j\)</span>) and the <strong>log-odds</strong> of the outcome (<span class="math notranslate nohighlight">\(Y=1\)</span>) must be <strong>linear</strong>. Mathematically, this means:</p>
<div class="math notranslate nohighlight">
\[\log\left(\frac{P(Y=1|X)}{1 - P(Y=1|X)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_k X_k\]</div>
<p><strong>Implications:</strong></p>
<ul class="simple">
<li><p>If the true relationship is non-linear (e.g., quadratic or U-shaped), the linear assumption will not hold.</p></li>
<li><p><strong>How to check:</strong> Plot the log-odds against each continuous predictor. If the relationship appears curved, consider adding polynomial terms or splines.</p></li>
</ul>
<p><strong>Example:</strong> If <code class="docutils literal notranslate"><span class="pre">age</span></code> is a predictor, the log-odds of diabetes should increase or decrease linearly with <code class="docutils literal notranslate"><span class="pre">age</span></code>. If the relationship is non-linear, you might need to add a quadratic term (e.g., <code class="docutils literal notranslate"><span class="pre">age²</span></code>).</p>
</section>
<hr class="docutils" />
<section id="2.-No-Multicollinearity">
<h4>2. No Multicollinearity<a class="headerlink" href="#2.-No-Multicollinearity" title="Permalink to this heading"></a></h4>
<p><strong>Assumption:</strong> Predictor variables should <strong>not be highly correlated</strong> with each other. High multicollinearity can inflate the variance of coefficient estimates, making them unstable and difficult to interpret.</p>
<p><strong>Implications:</strong></p>
<ul class="simple">
<li><p>Multicollinearity does not bias the model, but it reduces the precision of the estimated coefficients.</p></li>
<li><p><strong>How to check:</strong> Use <strong>Variance Inflation Factor (VIF)</strong>. A VIF &gt; 5 or 10 indicates problematic multicollinearity.</p></li>
</ul>
<p><strong>Example:</strong> If your model includes both <code class="docutils literal notranslate"><span class="pre">blood_pressure</span></code> and <code class="docutils literal notranslate"><span class="pre">blood_pressure_squared</span></code>, these variables are likely to be highly correlated. Consider removing one or using regularization (e.g., Lasso or Ridge).</p>
</section>
<hr class="docutils" />
<section id="3.-Additivity">
<h4>3. Additivity<a class="headerlink" href="#3.-Additivity" title="Permalink to this heading"></a></h4>
<p><strong>Assumption:</strong> The effect of each predictor on the log-odds of the outcome is <strong>additive</strong>. This means the contribution of each predictor to the log-odds is independent of the values of the other predictors.</p>
<p><strong>Implications:</strong></p>
<ul class="simple">
<li><p>If there are <strong>interaction effects</strong> (e.g., the effect of <code class="docutils literal notranslate"><span class="pre">age</span></code> on diabetes depends on <code class="docutils literal notranslate"><span class="pre">BMI</span></code>), they must be explicitly modeled.</p></li>
<li><p><strong>How to check:</strong> Test for interactions by adding product terms (e.g., <code class="docutils literal notranslate"><span class="pre">age</span> <span class="pre">×</span> <span class="pre">BMI</span></code>) to the model and evaluating their significance.</p></li>
</ul>
<p><strong>Example:</strong> If the effect of <code class="docutils literal notranslate"><span class="pre">glucose</span></code> on diabetes risk depends on <code class="docutils literal notranslate"><span class="pre">age</span></code>, you should include an interaction term like <code class="docutils literal notranslate"><span class="pre">glucose</span> <span class="pre">×</span> <span class="pre">age</span></code> in your model.</p>
</section>
<hr class="docutils" />
<section id="4.-Independence-of-Observations">
<h4>4. Independence of Observations<a class="headerlink" href="#4.-Independence-of-Observations" title="Permalink to this heading"></a></h4>
<p><strong>Assumption:</strong> The model assumes that <strong>observations are independent</strong> of each other. This is critical for the validity of standard errors and hypothesis tests.</p>
<p><strong>Implications:</strong></p>
<ul class="simple">
<li><p>If observations are correlated (e.g., repeated measurements from the same individual or clustered data), standard errors will be underestimated, leading to inflated Type I error rates.</p></li>
<li><p><strong>How to check:</strong> Look for patterns in residuals or use tests for autocorrelation (e.g., Durbin-Watson test for time-series data).</p></li>
</ul>
<p><strong>Example:</strong> If your dataset includes multiple measurements from the same patient over time, you should use a <strong>mixed-effects logistic regression</strong> or <strong>GEE (Generalized Estimating Equations)</strong> to account for within-patient correlation.</p>
</section>
<hr class="docutils" />
<section id="Summary-Table-of-Assumptions">
<h4>Summary Table of Assumptions<a class="headerlink" href="#Summary-Table-of-Assumptions" title="Permalink to this heading"></a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Assumption</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>How to Check</p></th>
<th class="head"><p>Solution if Violated</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>No Multicollinearity</strong></p></td>
<td><p>Predictors are not highly correlated.</p></td>
<td><p>Calculate VIF.</p></td>
<td><p>Remove predictors or use regularization.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Linearity of Log-Odds</strong></p></td>
<td><p>Continuous predictors have a linear relationship with the log-odds of the outcome.</p></td>
<td><p>Plot log-odds vs. predictors.</p></td>
<td><p>Add polynomial/spline terms.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Additivity</strong></p></td>
<td><p>The effect of each predictor is independent of other predictors.</p></td>
<td><p>Test for interactions.</p></td>
<td><p>Add interaction terms.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Independence</strong></p></td>
<td><p>Observations are independent (no clustering or repeated measures).</p></td>
<td><p>Check residuals for patterns.</p></td>
<td><p>Use mixed-effects models or GEE.</p></td>
</tr>
</tbody>
</table>
<hr class="docutils" />
</section>
</section>
</section>
<section id="Practical-Implementation-of-Logistic-Regression">
<h2>Practical Implementation of Logistic Regression<a class="headerlink" href="#Practical-Implementation-of-Logistic-Regression" title="Permalink to this heading"></a></h2>
<ol class="arabic simple">
<li><p><strong>Always check assumptions</strong> before interpreting your model.</p></li>
<li><p><strong>Use diagnostic plots</strong> (e.g., residual plots, VIF) to identify violations.</p></li>
<li><p><strong>Consider regularization</strong> (Lasso/Ridge) if multicollinearity is an issue.</p></li>
<li><p><strong>Model interactions</strong> if additivity is violated.</p></li>
<li><p><strong>Use robust standard errors</strong> or mixed-effects models if observations are not independent.</p></li>
</ol>
<section id="Data-Preprocessing">
<h3>Data Preprocessing<a class="headerlink" href="#Data-Preprocessing" title="Permalink to this heading"></a></h3>
<p>This notebook aims to provide a <strong>comprehensive overview of logistic regression</strong>, including its theoretical foundations, implementation, and practical application to real-world datasets.</p>
<div class="line-block">
<div class="line"><strong>Note on Data Preprocessing:</strong></div>
<div class="line">For simplicity and clarity, the data preprocessing steps in this notebook are applied to the <strong>entire dataset before the train-test split</strong>.</div>
<div class="line">However, it is important to note that in a real-world scenario, preprocessing (such as standardization, encoding, and imputation) should be performed only on the training dataset to avoid data leakage.</div>
<div class="line">The same preprocessing steps should then be applied to the test dataset using the parameters (e.g., mean, standard deviation) derived from the training dataset.</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[393]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The libraries imports</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[394]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">pointbiserialr</span>

<span class="c1"># Correlation Point-Biserial</span>
<span class="k">def</span> <span class="nf">calculate_point_biserial</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">numeric_vars</span><span class="p">,</span> <span class="n">dummy_vars</span><span class="p">):</span>
    <span class="n">point_biserial_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">numeric_vars</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">dummy_vars</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">num_var</span> <span class="ow">in</span> <span class="n">numeric_vars</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">dummy_var</span> <span class="ow">in</span> <span class="n">dummy_vars</span><span class="p">:</span>
            <span class="n">corr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pointbiserialr</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">num_var</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="n">dummy_var</span><span class="p">])</span>
            <span class="n">point_biserial_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num_var</span><span class="p">,</span> <span class="n">dummy_var</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>
    <span class="k">return</span> <span class="n">point_biserial_results</span>

<span class="c1"># Eigenvalues &amp; Condition Number</span>
<span class="k">def</span> <span class="nf">calculate_eigenvalues_condition_number</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
    <span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">)</span>
    <span class="n">condition_number</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">eigenvalues</span><span class="p">,</span> <span class="n">condition_number</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[395]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">calculate_gvif</span><span class="p">(</span><span class="n">dataframe</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">categorical_vars</span><span class="p">:</span> <span class="nb">list</span><span class="p">,</span> <span class="n">degrees_of_freedom</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate Generalized Variance Inflation Factor (GVIF) for categorical variables and</span>
<span class="sd">    Variance Inflation Factor (VIF) for continuous variables in a DataFrame.</span>

<span class="sd">    GVIF adjusts for the degrees of freedom associated with categorical variables, providing</span>
<span class="sd">    a more accurate measure of multicollinearity for dummy variables.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    dataframe : pd.DataFrame</span>
<span class="sd">        The DataFrame containing the predictors (both continuous and dummy variables).</span>
<span class="sd">    categorical_vars : list</span>
<span class="sd">        A list of column names corresponding to the dummy variables derived from categorical variables.</span>
<span class="sd">    degrees_of_freedom : int</span>
<span class="sd">        The degrees of freedom for the categorical variables (k - 1, where k is the number of categories).</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    pd.DataFrame</span>
<span class="sd">        A DataFrame with two columns: &quot;Variable&quot; and &quot;GVIF^(1/(2*d))&quot;, containing the GVIF/VIF values</span>
<span class="sd">        for each variable in the input DataFrame.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Create a copy of the input DataFrame and add a constant for the intercept</span>
    <span class="n">dataframe_with_const</span> <span class="o">=</span> <span class="n">dataframe</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">dataframe_with_const</span><span class="p">[</span><span class="s1">&#39;const&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>

    <span class="c1"># Initialize a DataFrame to store the results</span>
    <span class="n">vif_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
    <span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;Variable&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataframe_with_const</span><span class="o">.</span><span class="n">columns</span>
    <span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;GVIF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>  <span class="c1"># Initialize with 1</span>

    <span class="c1"># Calculate GVIF for categorical variables and VIF for continuous variables</span>
    <span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">variable</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataframe_with_const</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">variable</span> <span class="ow">in</span> <span class="n">categorical_vars</span> <span class="ow">or</span> <span class="n">variable</span> <span class="o">==</span> <span class="s1">&#39;const&#39;</span><span class="p">:</span>
            <span class="c1"># For categorical variables, calculate GVIF^(1/(2*d))</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">degrees_of_freedom</span> <span class="k">if</span> <span class="n">variable</span> <span class="o">!=</span> <span class="s1">&#39;const&#39;</span> <span class="k">else</span> <span class="mi">1</span>
            <span class="n">vif</span> <span class="o">=</span> <span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">dataframe_with_const</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
            <span class="n">gif</span> <span class="o">=</span> <span class="n">vif</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d</span><span class="p">))</span> <span class="k">if</span> <span class="n">variable</span> <span class="o">!=</span> <span class="s1">&#39;const&#39;</span> <span class="k">else</span> <span class="n">vif</span>
            <span class="n">vif_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;Variable&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">variable</span><span class="p">,</span> <span class="s2">&quot;GVIF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gif</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># For continuous variables, calculate VIF</span>
            <span class="n">vif</span> <span class="o">=</span> <span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">dataframe_with_const</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">index</span><span class="p">)</span>
            <span class="n">vif_data</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">vif_data</span><span class="p">[</span><span class="s2">&quot;Variable&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="n">variable</span><span class="p">,</span> <span class="s2">&quot;GVIF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">vif</span>

    <span class="k">return</span> <span class="n">vif_data</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[396]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">chi2_contingency</span>

<span class="k">def</span> <span class="nf">cramers_v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate Cramer&#39;s V statistic for categorical-categorical association.&quot;&quot;&quot;</span>
    <span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">crosstab</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">chi2</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">chi2_contingency</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">)</span>
    <span class="n">n</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">phi2</span> <span class="o">=</span> <span class="n">chi2</span> <span class="o">/</span> <span class="n">n</span>
    <span class="n">r</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">phi2corr</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">phi2</span> <span class="o">-</span> <span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">r</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">r_corr</span> <span class="o">=</span> <span class="n">r</span> <span class="o">-</span> <span class="p">((</span><span class="n">r</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">k_corr</span> <span class="o">=</span> <span class="n">k</span> <span class="o">-</span> <span class="p">((</span><span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">phi2corr</span> <span class="o">/</span> <span class="nb">min</span><span class="p">((</span><span class="n">k_corr</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="n">r_corr</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

<span class="k">def</span> <span class="nf">cramers_v_matrix</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">categorical_columns</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate Cramer&#39;s V matrix for a list of categorical columns.&quot;&quot;&quot;</span>
    <span class="n">n_columns</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">categorical_columns</span><span class="p">)</span>
    <span class="n">cramersv_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_columns</span><span class="p">,</span> <span class="n">n_columns</span><span class="p">))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_columns</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_columns</span><span class="p">):</span>
            <span class="n">cramersv_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">cramers_v</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">categorical_columns</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">df</span><span class="p">[</span><span class="n">categorical_columns</span><span class="p">[</span><span class="n">j</span><span class="p">]])</span>

    <span class="n">cramersv_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cramersv_matrix</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">categorical_columns</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">categorical_columns</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cramersv_df</span>
</pre></div>
</div>
</div>
<p>We will use the diabetes dataset to show how to perform logistic regression in Python.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[397]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">data_path</span> <span class="o">=</span> <span class="s1">&#39;/Users/davidtbo/Library/Mobile Documents/com~apple~CloudDocs/data/external&#39;</span>
<span class="n">file_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_path</span><span class="p">,</span> <span class="s1">&#39;diabetes.csv&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">file_path</span><span class="p">)</span>

<span class="c1"># Standardize column names to lowercase</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[397]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pregnancies</th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
      <th>age</th>
      <th>outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</section>
</section>
<section id="Modification-of-the-initial-dataset">
<h2>Modification of the initial dataset<a class="headerlink" href="#Modification-of-the-initial-dataset" title="Permalink to this heading"></a></h2>
<div class="line-block">
<div class="line">The initial dataset only contains numerical variables. For pedagogical purposes, we will incorporate categorical variables in a logistic regression model to predict diabetes.</div>
<div class="line">Note tha categorize continuous variables can help to better capture non-linear relationships and improve interpretability.</div>
</div>
<p><strong>Age:</strong> the age variable will be categorized into three distinct groups:</p>
<ul class="simple">
<li><p>Young</p></li>
<li><p>Middle-aged</p></li>
<li><p>Senior</p></li>
</ul>
<p><strong>Number of pregnancies:</strong> this variable will be categorized based on medical thresholds into the following groups:</p>
<ul class="simple">
<li><p>None (0 pregnancies)</p></li>
<li><p>Low (1-2 pregnancies)</p></li>
<li><p>Moderate (3-5 pregnancies)</p></li>
<li><p>High (6 or more pregnancies)</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[398]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Categorize &#39;age&#39; into 3 categories</span>
<span class="n">bins_age</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
<span class="n">labels_age</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Young&#39;</span><span class="p">,</span> <span class="s1">&#39;Middle-aged&#39;</span><span class="p">,</span> <span class="s1">&#39;Senior&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;age_category&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins_age</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels_age</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Check the distribution of age categories</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;age_category&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[398]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
age_category
Young          417
Middle-aged    270
Senior          81
Name: count, dtype: int64
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[399]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Categorize &#39;number of pregnancies&#39; into 3 categories</span>
<span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;None&#39;</span><span class="p">,</span> <span class="s1">&#39;Low&#39;</span><span class="p">,</span> <span class="s1">&#39;Moderate&#39;</span><span class="p">,</span> <span class="s1">&#39;High&#39;</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;pregnancies_category&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;pregnancies&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>

<span class="c1"># Check the distribution of age categories</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;pregnancies_category&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[399]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
pregnancies_category
Low         238
High        219
Moderate    200
None        111
Name: count, dtype: int64
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[400]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="s1">&#39;pregnancies&#39;</span><span class="p">],</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[400]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
      <th>outcome</th>
      <th>age_category</th>
      <th>pregnancies_category</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>1</td>
      <td>Middle-aged</td>
      <td>High</td>
    </tr>
    <tr>
      <th>1</th>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>0</td>
      <td>Middle-aged</td>
      <td>Low</td>
    </tr>
    <tr>
      <th>2</th>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>1</td>
      <td>Middle-aged</td>
      <td>High</td>
    </tr>
    <tr>
      <th>3</th>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>0</td>
      <td>Young</td>
      <td>Low</td>
    </tr>
    <tr>
      <th>4</th>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>1</td>
      <td>Middle-aged</td>
      <td>None</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[401]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<section id="Train-test-split">
<h3>Train test split<a class="headerlink" href="#Train-test-split" title="Permalink to this heading"></a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[402]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>The model is fitted using the training data (X_train) and tested on the test data (X_test).</strong></p>
<p><strong>Variable encoding and transformation:</strong></p>
<ol class="arabic simple">
<li><p>Categorical variables: after categorization, <strong>dummy encoding</strong> (one-hot encoding) will be used to transform categorical variables into a format suitable for logistic regression.</p></li>
</ol>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[403]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert categorical variable to dummy variables</span>
<span class="n">X_train_dum</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;age_category&#39;</span><span class="p">,</span><span class="s1">&#39;pregnancies_category&#39;</span><span class="p">],</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">col</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">,</span><span class="s1">&#39;_&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X_train_dum</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span>

<span class="c1"># Ensure all columns are numeric</span>
<span class="n">X_train_dum</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">pd</span><span class="o">.</span><span class="n">to_numeric</span><span class="p">,</span> <span class="n">errors</span><span class="o">=</span><span class="s1">&#39;coerce&#39;</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[403]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
      <th>age_category_middle_aged</th>
      <th>age_category_senior</th>
      <th>pregnancies_category_low</th>
      <th>pregnancies_category_moderate</th>
      <th>pregnancies_category_high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>602</th>
      <td>124</td>
      <td>74</td>
      <td>36</td>
      <td>0</td>
      <td>27</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>429</th>
      <td>95</td>
      <td>82</td>
      <td>25</td>
      <td>180</td>
      <td>35</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>623</th>
      <td>94</td>
      <td>70</td>
      <td>27</td>
      <td>115</td>
      <td>43</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>209</th>
      <td>184</td>
      <td>84</td>
      <td>33</td>
      <td>0</td>
      <td>35</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>589</th>
      <td>73</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>21</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[404]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We transform non categorical features to float to separate them from dummies features for correlation purposes</span>
<span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;glucose&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;glucose&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;bloodpressure&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;bloodpressure&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;skinthickness&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;skinthickness&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;insulin&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;insulin&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;bmi&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
<span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;diabetespedigreefunction&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="p">[</span><span class="s1">&#39;diabetespedigreefunction&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[405]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_dum</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[405]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
glucose                          float64
bloodpressure                    float64
skinthickness                    float64
insulin                          float64
bmi                              float64
diabetespedigreefunction         float64
age_category_middle_aged           int64
age_category_senior                int64
pregnancies_category_low           int64
pregnancies_category_moderate      int64
pregnancies_category_high          int64
dtype: object
</pre></div></div>
</div>
<p><strong>Variable encoding and transformation:</strong></p>
<ol class="arabic simple" start="2">
<li><p>Continuous variables: This can be used as-is or <strong>standardized</strong> if they exhibit vastly different scales, ensuring that each variable contributes equally to the model.</p></li>
</ol>
<p>Standardizing continuous variables in logistic regression improves model performance and interpretability by scaling variables to have a mean of 0 and a standard deviation of 1. This helps with convergence and coefficient interpretation.</p>
<p>We will standardize variables if their scales differ significantly, detected using the <strong>describe()</strong> method.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[406]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_dum</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="n">col</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">X_train_dum</span> <span class="k">if</span> <span class="p">(</span><span class="s1">&#39;age&#39;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="s1">&#39;pregnancies&#39;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="s1">&#39;outcome&#39;</span> <span class="ow">in</span> <span class="n">col</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[406]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>614.000000</td>
      <td>614.000000</td>
      <td>614.000000</td>
      <td>614.000000</td>
      <td>614.000000</td>
      <td>614.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>121.936482</td>
      <td>69.060261</td>
      <td>20.258958</td>
      <td>82.048860</td>
      <td>31.374593</td>
      <td>0.076547</td>
    </tr>
    <tr>
      <th>std</th>
      <td>31.829186</td>
      <td>19.515785</td>
      <td>16.191645</td>
      <td>120.170912</td>
      <td>7.794029</td>
      <td>0.283886</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>99.000000</td>
      <td>62.500000</td>
      <td>0.000000</td>
      <td>0.000000</td>
      <td>27.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>117.000000</td>
      <td>72.000000</td>
      <td>23.000000</td>
      <td>22.500000</td>
      <td>32.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>142.000000</td>
      <td>80.000000</td>
      <td>32.000000</td>
      <td>130.000000</td>
      <td>36.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>199.000000</td>
      <td>122.000000</td>
      <td>99.000000</td>
      <td>846.000000</td>
      <td>67.000000</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>When analyzing the range and the standard deviation of the variables, <strong>Insulin</strong> should be standardized and in a lower proportion, <strong>glucose</strong> could also be standardized but it is not strictly necessary.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[407]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># Variables to standardize</span>
<span class="n">continuous_vars_to_standardize</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;insulin&#39;</span><span class="p">]</span>

<span class="c1"># Standardization</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_dum_scale</span> <span class="o">=</span> <span class="n">X_train_dum</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">X_train_dum_scale</span><span class="p">[</span><span class="n">continuous_vars_to_standardize</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_dum</span><span class="p">[</span><span class="n">continuous_vars_to_standardize</span><span class="p">])</span>
<span class="n">X_train_dum_scale</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[407]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
      <th>age_category_middle_aged</th>
      <th>age_category_senior</th>
      <th>pregnancies_category_low</th>
      <th>pregnancies_category_moderate</th>
      <th>pregnancies_category_high</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>602</th>
      <td>124.0</td>
      <td>74.0</td>
      <td>36.0</td>
      <td>-0.683325</td>
      <td>27.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>429</th>
      <td>95.0</td>
      <td>82.0</td>
      <td>25.0</td>
      <td>0.815763</td>
      <td>35.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>623</th>
      <td>94.0</td>
      <td>70.0</td>
      <td>27.0</td>
      <td>0.274426</td>
      <td>43.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>209</th>
      <td>184.0</td>
      <td>84.0</td>
      <td>33.0</td>
      <td>-0.683325</td>
      <td>35.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>589</th>
      <td>73.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.683325</td>
      <td>21.0</td>
      <td>0.0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
</section>
<section id="Checking-the-first-model-assumption:-No-Multicolinearity">
<h3><strong>Checking the first model assumption: No Multicolinearity</strong><a class="headerlink" href="#Checking-the-first-model-assumption:-No-Multicolinearity" title="Permalink to this heading"></a></h3>
<p><strong>1. :math:`text{VIF}` for numeric variables</strong></p>
<div class="line-block">
<div class="line">VIF (Variance Inflation Factor) is the most common method to detect multicollinearity and works well for continuous variables.</div>
<div class="line">Fit a linear regression model for each predictor against all other predictors and calculate VIF as <span class="math notranslate nohighlight">\(R^2\)</span> inverse.</div>
</div>
<p>Interpretation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{VIF} &lt; 5\)</span>: No problematic multicollinearity.</p></li>
<li><p><span class="math notranslate nohighlight">\(5 \leq \text{VIF} &lt; 10\)</span>: Moderate multicollinearity, monitor closely.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{VIF} \geq 10\)</span>: High multicollinearity, needs correction.</p></li>
</ul>
<p><strong>1’. :math:`text{GVIF}` for categorical variables (dummy variables)</strong></p>
<p>For categorical variables, use Generalized Variance Inflation Factor (<span class="math notranslate nohighlight">\(\text{GVIF}\)</span>) instead of <span class="math notranslate nohighlight">\(\text{VIF}\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\text{GVIF}\)</span> provides a more accurate measure of multicollinearity for categorical variables because it takes into account the degrees of freedom <span class="math notranslate nohighlight">\((\text{d})\)</span> associated with categorical variables.</p>
<p>Interpretation:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{GVIF} = \text{VIF}^{\frac{1}{2\text{d}}} &lt; 2\)</span>: No problematic multicollinearity.</p></li>
<li><p><span class="math notranslate nohighlight">\(\text{GVIF} = \text{VIF}^{\frac{1}{2\text{d}}} \geq 2\)</span>: Indicates potential multicollinearity.</p></li>
</ul>
<div class="line-block">
<div class="line"><strong>Degrees of freedom :math:`(d)`</strong> refer to the number of independent pieces of information available to estimate a parameter.</div>
<div class="line">For a categorical variable with <span class="math notranslate nohighlight">\(k\)</span> levels, the degrees of freedom are <span class="math notranslate nohighlight">\(k−1\)</span> because one level is used as the reference category.</div>
</div>
<div class="line-block">
<div class="line">When we create dummy variables from a categorical variable, the dummy variables are not independent of each other.</div>
<div class="line">To avoid ta perfect multicollinearity, we drop one dummy variable (reference category), leaving you with <span class="math notranslate nohighlight">\(k−1\)</span> dummy variables.</div>
</div>
<div class="line-block">
<div class="line"><strong>VIF</strong> treats each dummy variable as an independent predictor, which can overestimate multicollinearity because it doesn’t account for the shared variance among dummy variables from the same categorical variable.</div>
<div class="line"><strong>GVIF</strong> adjusts for this by considering the degrees of freedom of the categorical variable. It essentially “penalizes” the VIF value to account for the fact that the dummy variables are related.</div>
</div>
<p><strong>VIF</strong> overestimates multicollinearity for <strong>dummy variables</strong> because it treats them as independent predictors.</p>
<p><strong>3. Pearson correlation for numeric variables</strong></p>
<p>Use Pearson correlation to detect multicolinearity amoung numeric variables.</p>
<p>Interpretation:</p>
<ul class="simple">
<li><p>Values close to 1 or -1 indicate strong linear relationships.</p></li>
</ul>
<p><strong>4. Point-Biserial correlation</strong></p>
<p>Measure the association (pairwise correlation) between numeric and dummy variables. It is essentially a special case of Pearson correlation where one variable is binary.</p>
<p>Interpretation:</p>
<ul class="simple">
<li><p>High absolute values (close to 1 or -1) indicate a strong association between the numeric and dummy variables.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[415]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the corrélation Point-Biserial</span>
<span class="n">point_biserial_results</span> <span class="o">=</span> <span class="n">calculate_point_biserial</span><span class="p">(</span><span class="n">X_train_dum_scale</span><span class="p">,</span> <span class="n">continuous_vars</span><span class="p">,</span> <span class="n">dummy_vars</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Point-Biserial Correlation:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">point_biserial_results</span><span class="o">.</span><span class="n">to</span><span class="o">!</span>string<span class="o">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
  <span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[415]</span><span class="ansi-green-fg">, line 4</span>
<span class="ansi-red-fg">    </span><span class="ansi-red-fg">print(point_biserial_results.to!string())</span>
                                   ^
<span class="ansi-red-fg">SyntaxError</span><span class="ansi-red-fg">:</span> invalid syntax

</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[409]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the eigenvalues and the condition number</span>
<span class="n">eigenvalues</span><span class="p">,</span> <span class="n">condition_number</span> <span class="o">=</span> <span class="n">calculate_eigenvalues_condition_number</span><span class="p">(</span><span class="n">X_train_dum_scale</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Condition Number: </span><span class="si">{</span><span class="n">condition_number</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Visualiser les eigenvalues</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scree Plot of Eigenvalues&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Eigenvalue Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Eigenvalue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

Condition Number: 13.13
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_56_1.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_56_1.png" />
</div>
</div>
<p><strong>5. Eigenvalues and Condition Number</strong></p>
<p>Check the eigenvalues of the correlation matrix of numeric and dummy variables to detect multicollinearity.</p>
<p>This analysis applies to the entire set of predictors (both numeric and dummy variables) included in the model. Ensure all variables are numeric (dummy variables are already numeric, encoded as 0 or 1).</p>
<p>Interpretation:</p>
<p>Condition Number:</p>
<ul class="simple">
<li><p>A high condition number (e.g., &gt; 30) indicates potential multicollinearity.</p></li>
</ul>
<p>Eigenvalues:</p>
<ul class="simple">
<li><p>A scree plot with some eigenvalues close to zero suggests multicollinearity.</p></li>
<li><p>If one or more eigenvalues are near zero, it indicates that some predictors are linear combinations of others.</p></li>
</ul>
<p><strong>6. Pair Plot for Dummy Variables</strong></p>
<p>Visualize the relationships between dummy variables using a pair plot.</p>
<p>Interpretation:</p>
<p>Look for patterns that indicate strong associations between dummy variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[410]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define degrees of freedom for each categorical variable</span>
<span class="n">degrees_of_freedom_dict</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;age_category_middle_aged&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>  <span class="c1"># 3 levels - 1</span>
    <span class="s1">&#39;age_category_senior&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>        <span class="c1"># 3 levels - 1</span>
    <span class="s1">&#39;pregnancies_category_low&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>        <span class="c1"># 4 levels - 1</span>
    <span class="s1">&#39;pregnancies_category_moderate&#39;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>        <span class="c1"># 4 levels - 1</span>
    <span class="s1">&#39;pregnancies_category_high&#39;</span><span class="p">:</span> <span class="mi">3</span>        <span class="c1"># 4 levels - 1</span>
<span class="p">}</span>

<span class="n">dummy_vars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age_category_middle_aged&#39;</span><span class="p">,</span><span class="s1">&#39;age_category_senior&#39;</span><span class="p">,</span><span class="s1">&#39;pregnancies_category_low&#39;</span><span class="p">,</span><span class="s1">&#39;pregnancies_category_moderate&#39;</span><span class="p">,</span><span class="s1">&#39;pregnancies_category_high&#39;</span><span class="p">]</span>
<span class="n">continuous_vars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;glucose&#39;</span><span class="p">,</span> <span class="s1">&#39;bloodpressure&#39;</span><span class="p">,</span> <span class="s1">&#39;skinthickness&#39;</span><span class="p">,</span> <span class="s1">&#39;insulin&#39;</span><span class="p">,</span> <span class="s1">&#39;bmi&#39;</span><span class="p">,</span> <span class="s1">&#39;diabetespedigreefunction&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[411]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">statsmodels.stats.outliers_influence</span> <span class="kn">import</span> <span class="n">variance_inflation_factor</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">calculate_multicollinearity</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">continuous_vars</span><span class="p">,</span> <span class="n">dummy_vars</span><span class="p">,</span> <span class="n">degrees_of_freedom_dict</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate VIF for continuous variables and GVIF for dummy variables.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    -----------</span>
<span class="sd">    df : pd.DataFrame</span>
<span class="sd">        DataFrame containing predictors.</span>
<span class="sd">    continuous_vars : list</span>
<span class="sd">        List of continuous variable names.</span>
<span class="sd">    dummy_vars : list</span>
<span class="sd">        List of dummy variable names.</span>
<span class="sd">    degrees_of_freedom_dict : dict</span>
<span class="sd">        Dictionary mapping dummy variables to their degrees of freedom.</span>

<span class="sd">    Returns:</span>
<span class="sd">    --------</span>
<span class="sd">    pd.DataFrame</span>
<span class="sd">        DataFrame with multicollinearity metrics for each variable.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df_with_const</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span><span class="n">const</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">df_with_const</span><span class="o">.</span><span class="n">columns</span><span class="p">):</span>
        <span class="n">vif</span> <span class="o">=</span> <span class="n">variance_inflation_factor</span><span class="p">(</span><span class="n">df_with_const</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">dummy_vars</span><span class="p">:</span>
            <span class="n">d</span> <span class="o">=</span> <span class="n">degrees_of_freedom_dict</span><span class="p">[</span><span class="n">var</span><span class="p">]</span>
            <span class="n">gif</span> <span class="o">=</span> <span class="n">vif</span> <span class="o">**</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">d</span><span class="p">))</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;Variable&quot;</span><span class="p">:</span> <span class="n">var</span><span class="p">,</span> <span class="s2">&quot;Metric&quot;</span><span class="p">:</span> <span class="s2">&quot;GVIF&quot;</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">:</span> <span class="n">gif</span><span class="p">})</span>
        <span class="k">elif</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">continuous_vars</span> <span class="ow">or</span> <span class="n">var</span> <span class="o">==</span> <span class="s1">&#39;const&#39;</span><span class="p">:</span>
            <span class="n">results</span><span class="o">.</span><span class="n">append</span><span class="p">({</span><span class="s2">&quot;Variable&quot;</span><span class="p">:</span> <span class="n">var</span><span class="p">,</span> <span class="s2">&quot;Metric&quot;</span><span class="p">:</span> <span class="s2">&quot;VIF&quot;</span><span class="p">,</span> <span class="s2">&quot;Value&quot;</span><span class="p">:</span> <span class="n">vif</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">results</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[412]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">multicollinearity_data</span> <span class="o">=</span> <span class="n">calculate_multicollinearity</span><span class="p">(</span><span class="n">X_train_dum_scale</span><span class="p">,</span> <span class="n">continuous_vars</span><span class="p">,</span> <span class="n">dummy_vars</span><span class="p">,</span> <span class="n">degrees_of_freedom_dict</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">multicollinearity_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
                         Variable Metric      Value
0                         glucose    VIF   1.362408
1                   bloodpressure    VIF   1.209473
2                   skinthickness    VIF   1.535867
3                         insulin    VIF   1.457967
4                             bmi    VIF   1.356810
5        diabetespedigreefunction    VIF   1.057445
6        age_category_middle_aged   GVIF   1.154213
7             age_category_senior   GVIF   1.098110
8        pregnancies_category_low   GVIF   1.149052
9   pregnancies_category_moderate   GVIF   1.146484
10      pregnancies_category_high   GVIF   1.199571
11                          const    VIF  45.962850
</pre></div></div>
</div>
<div class="line-block">
<div class="line">A <span class="math notranslate nohighlight">\(\text{VIF}\)</span> of 1.36 for glucose means that the variance of the regression coefficient for glucose is multiplied by 1.36 due to multicollinearity with all other variables (both continuous and dummy) in the model.</div>
<div class="line">This indicates a low level of multicollinearity overall. Given a <span class="math notranslate nohighlight">\(\text{VIF}\)</span> of 1.36 for glucose, we can work backward to find <span class="math notranslate nohighlight">\(R^2_j\)</span>:</div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{VIF}_j = \dfrac{1}{1 - R^2_j}=1.36\]</div>
</div></blockquote>
<p>Solving for <span class="math notranslate nohighlight">\(R^2_j\)</span>:</p>
<div class="math notranslate nohighlight">
\[R^2_j \approx 0.2647\]</div>
<ul class="simple">
<li><p>This means that about 26.47% of the variance in the glucose variable is explained by all the other predictor variables in the model.</p></li>
<li><p>This is a relatively low percentage, indicating that glucose is not highly collinear with the other predictors. <span class="math notranslate nohighlight">\(\text{VIF} = 1.36\)</span>:</p></li>
</ul>
<p>The variance of the regression coefficient for glucose is inflated by a factor of 1.36 due to its relationship with all other predictors. This is considered a low <span class="math notranslate nohighlight">\(\text{VIF}\)</span> value, indicating that multicollinearity is not a significant issue for glucose in this model.</p>
<p><strong>Definition of :math:`text{VIF}`</strong></p>
<div class="line-block">
<div class="line">The Variance Inflation Factor <span class="math notranslate nohighlight">\((\text{VIF})\)</span> measures the increase in the variance of regression coefficients due to multicollinearity with all other variables in the model.</div>
<div class="line">It does not specifically measure multicollinearity with a single variable but rather the cumulative effect of all other variables on a given variable.</div>
</div>
<p><strong>Mathematical formula of :math:`text{VIF}`</strong></p>
<p><span class="math notranslate nohighlight">\(\text{VIF}\)</span> for a predictor variable <span class="math notranslate nohighlight">\(X_j\)</span>​ in a regression model is defined as:</p>
<div class="math notranslate nohighlight">
\[\text{VIF}_j = \dfrac{1}{1 - R^2_j}\]</div>
<p>Where <span class="math notranslate nohighlight">\(R^2_j\)</span> is the coefficient of determination obtained by regressing the predictor <span class="math notranslate nohighlight">\(X_j\)</span> on all the other predictor variables in the model. It ranges from 0 to 1.</p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(R^2_j\)</span> is close to 0, it means <span class="math notranslate nohighlight">\(X_j\)</span> is not well-explained by the other predictors, and <span class="math notranslate nohighlight">\(\text{VIF}\)</span> will be close to 1, indicating no multicolinearity.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(R^2_j\)</span> is close to 1, it means <span class="math notranslate nohighlight">\(X_j\)</span> is well-explained by the other predictors, and <span class="math notranslate nohighlight">\(\text{VIF}\)</span> will be large, indicating high multicolinearity.</p></li>
</ul>
<p>To calculate <span class="math notranslate nohighlight">\(R^2_j\)</span>, you regress <span class="math notranslate nohighlight">\(X_j\)</span> on all other predictors in the model. Treat <span class="math notranslate nohighlight">\(X_j\)</span> as the dependent variable. Treat all other predictors in the model as independent variables.</p>
<p>Fit a linear regression model to predict <span class="math notranslate nohighlight">\(X_j\)</span>​ using the other predictors.</p>
<div class="math notranslate nohighlight">
\[X_j = \beta_0 + \beta_0 X_1 + \beta_2 X_2 + \ldots + \beta_{j-1}X_{j−1}+ \beta_{j+1} X{j+1} + \ldots + \beta_p X_p + \epsilon\]</div>
<p><strong>1. :math:`text{VIF}` for continuous variables</strong></p>
<p><strong>2. Pearson correlation for numeric variables</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[413]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_float</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="s1">&#39;float&#39;</span><span class="p">)</span>

<span class="c1"># Calculate Pearson correlation matrix</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">X_train_float</span><span class="o">.</span><span class="n">corr</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;pearson&#39;</span><span class="p">)</span>

<span class="c1"># Visualize the correlation matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Correlation Matrix&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_70_0.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_70_0.png" />
</div>
</div>
<p><strong>4. Point-Biserial correlation</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[414]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">pointbiserialr</span>

<span class="c1"># Example: Calculate Point-Biserial correlation between a numeric and a dummy variable</span>
<span class="n">numeric_vars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;glucose&#39;</span><span class="p">,</span> <span class="s1">&#39;bloodpressure&#39;</span><span class="p">,</span> <span class="s1">&#39;skinthickness&#39;</span><span class="p">,</span> <span class="s1">&#39;insulin&#39;</span><span class="p">,</span> <span class="s1">&#39;bmi&#39;</span><span class="p">,</span> <span class="s1">&#39;diabetespedigreefunction&#39;</span><span class="p">]</span>
<span class="n">dummy_vars</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;age_category_middle_aged&#39;</span><span class="p">,</span> <span class="s1">&#39;age_category_senior&#39;</span><span class="p">,</span> <span class="s1">&#39;pregnancies_category_low&#39;</span><span class="p">,</span> <span class="s1">&#39;pregnancies_category_moderate&#39;</span><span class="p">,</span> <span class="s1">&#39;pregnancies_category_high&#39;</span><span class="p">]</span>

<span class="n">point_biserial_results</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span><span class="o">=</span><span class="n">numeric_vars</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">dummy_vars</span><span class="p">)</span>

<span class="k">for</span> <span class="n">num_var</span> <span class="ow">in</span> <span class="n">numeric_vars</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">dummy_var</span> <span class="ow">in</span> <span class="n">dummy_vars</span><span class="p">:</span>
        <span class="n">corr</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pointbiserialr</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">num_var</span><span class="p">],</span> <span class="n">X_train</span><span class="p">[</span><span class="n">dummy_var</span><span class="p">])</span>
        <span class="n">point_biserial_results</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">num_var</span><span class="p">,</span> <span class="n">dummy_var</span><span class="p">]</span> <span class="o">=</span> <span class="n">corr</span>

<span class="c1"># Visualize the correlation matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">point_biserial_results</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">float</span><span class="p">),</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Point-Biserial Correlation between Numeric and Dummy Variables&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
<span class="ansi-red-fg">---------------------------------------------------------------------------</span>
<span class="ansi-red-fg">KeyError</span>                                  Traceback (most recent call last)
<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">~/Library/Mobile Documents/com~apple~CloudDocs/website_rtd/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812</span>, in <span class="ansi-cyan-fg">Index.get_loc</span><span class="ansi-blue-fg">(self, key)</span>
<span class="ansi-green-fg">   3811</span> <span class="ansi-bold" style="color: rgb(0,135,0)">try</span>:
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">3812</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span class="ansi-yellow-bg" style="color: rgb(0,135,0)">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">_engine</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">get_loc</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">casted_key</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   3813</span> <span class="ansi-bold" style="color: rgb(0,135,0)">except</span> <span class="ansi-bold" style="color: rgb(215,95,95)">KeyError</span> <span class="ansi-bold" style="color: rgb(0,135,0)">as</span> err:

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">pandas/_libs/index.pyx:167</span>, in <span class="ansi-cyan-fg">pandas._libs.index.IndexEngine.get_loc</span><span class="ansi-blue-fg">()</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">pandas/_libs/index.pyx:196</span>, in <span class="ansi-cyan-fg">pandas._libs.index.IndexEngine.get_loc</span><span class="ansi-blue-fg">()</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">pandas/_libs/hashtable_class_helper.pxi:7088</span>, in <span class="ansi-cyan-fg">pandas._libs.hashtable.PyObjectHashTable.get_item</span><span class="ansi-blue-fg">()</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">pandas/_libs/hashtable_class_helper.pxi:7096</span>, in <span class="ansi-cyan-fg">pandas._libs.hashtable.PyObjectHashTable.get_item</span><span class="ansi-blue-fg">()</span>

<span class="ansi-red-fg">KeyError</span>: &#39;age_category_middle_aged&#39;

The above exception was the direct cause of the following exception:

<span class="ansi-red-fg">KeyError</span>                                  Traceback (most recent call last)
<span class="ansi-cyan-fg">Cell</span><span class="ansi-cyan-fg"> </span><span class="ansi-green-fg">In[414]</span><span class="ansi-green-fg">, line 11</span>
<span class="ansi-green-fg">      9</span> <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> num_var <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> numeric_vars:
<span class="ansi-green-fg">     10</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> dummy_var <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> dummy_vars:
<span class="ansi-green-fg">---&gt; </span><span class="ansi-green-fg">11</span>         corr, _ = pointbiserialr(X_train[num_var], <span class="ansi-yellow-bg">X_train</span><span class="ansi-yellow-bg">[</span><span class="ansi-yellow-bg">dummy_var</span><span class="ansi-yellow-bg">]</span>)
<span class="ansi-green-fg">     12</span>         point_biserial_results.loc[num_var, dummy_var] = corr
<span class="ansi-green-fg">     14</span> <span style="color: rgb(95,135,135)"># Visualize the correlation matrix</span>

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">~/Library/Mobile Documents/com~apple~CloudDocs/website_rtd/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4113</span>, in <span class="ansi-cyan-fg">DataFrame.__getitem__</span><span class="ansi-blue-fg">(self, key)</span>
<span class="ansi-green-fg">   4111</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">self</span>.columns.nlevels &gt; <span class="ansi-green-fg">1</span>:
<span class="ansi-green-fg">   4112</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">return</span> <span style="color: rgb(0,135,0)">self</span>._getitem_multilevel(key)
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">4113</span> indexer = <span class="ansi-yellow-bg" style="color: rgb(0,135,0)">self</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">columns</span><span class="ansi-yellow-bg">.</span><span class="ansi-yellow-bg">get_loc</span><span class="ansi-yellow-bg">(</span><span class="ansi-yellow-bg">key</span><span class="ansi-yellow-bg">)</span>
<span class="ansi-green-fg">   4114</span> <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> is_integer(indexer):
<span class="ansi-green-fg">   4115</span>     indexer = [indexer]

<span class="ansi-cyan-fg">File </span><span class="ansi-green-fg">~/Library/Mobile Documents/com~apple~CloudDocs/website_rtd/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819</span>, in <span class="ansi-cyan-fg">Index.get_loc</span><span class="ansi-blue-fg">(self, key)</span>
<span class="ansi-green-fg">   3814</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">if</span> <span style="color: rgb(0,135,0)">isinstance</span>(casted_key, <span style="color: rgb(0,135,0)">slice</span>) <span class="ansi-bold" style="color: rgb(175,0,255)">or</span> (
<span class="ansi-green-fg">   3815</span>         <span style="color: rgb(0,135,0)">isinstance</span>(casted_key, abc.Iterable)
<span class="ansi-green-fg">   3816</span>         <span class="ansi-bold" style="color: rgb(175,0,255)">and</span> <span style="color: rgb(0,135,0)">any</span>(<span style="color: rgb(0,135,0)">isinstance</span>(x, <span style="color: rgb(0,135,0)">slice</span>) <span class="ansi-bold" style="color: rgb(0,135,0)">for</span> x <span class="ansi-bold" style="color: rgb(175,0,255)">in</span> casted_key)
<span class="ansi-green-fg">   3817</span>     ):
<span class="ansi-green-fg">   3818</span>         <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> InvalidIndexError(key)
<span class="ansi-green-fg">-&gt; </span><span class="ansi-green-fg">3819</span>     <span class="ansi-bold" style="color: rgb(0,135,0)">raise</span> <span class="ansi-bold" style="color: rgb(215,95,95)">KeyError</span>(key) <span class="ansi-bold" style="color: rgb(0,135,0)">from</span><span style="color: rgb(188,188,188)"> </span><span class="ansi-blue-intense-fg ansi-bold">err</span>
<span class="ansi-green-fg">   3820</span> <span class="ansi-bold" style="color: rgb(0,135,0)">except</span> <span class="ansi-bold" style="color: rgb(215,95,95)">TypeError</span>:
<span class="ansi-green-fg">   3821</span>     <span style="color: rgb(95,135,135)"># If we have a listlike key, _check_indexing_error will raise</span>
<span class="ansi-green-fg">   3822</span>     <span style="color: rgb(95,135,135)">#  InvalidIndexError. Otherwise we fall through and re-raise</span>
<span class="ansi-green-fg">   3823</span>     <span style="color: rgb(95,135,135)">#  the TypeError.</span>
<span class="ansi-green-fg">   3824</span>     <span style="color: rgb(0,135,0)">self</span>._check_indexing_error(key)

<span class="ansi-red-fg">KeyError</span>: &#39;age_category_middle_aged&#39;
</pre></div></div>
</div>
<p><strong>5. Eigenvalues and Condition Number</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_eigen</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c1"># Check for NaN values</span>
<span class="k">if</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">isnull</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;NaN values found. Dropping rows with NaN values.&quot;</span><span class="p">)</span>
    <span class="n">X_train_eigen</span> <span class="o">=</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Check for Inf values</span>
<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X_train_eigen</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Inf values found. Replacing Inf with NaN and dropping rows with NaN values.&quot;</span><span class="p">)</span>
    <span class="n">X_train_eigen</span> <span class="o">=</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">,</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">inf</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">)</span>
    <span class="n">X_train_eigen</span> <span class="o">=</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>

<span class="c1"># Check variance of each feature</span>
<span class="k">if</span> <span class="p">(</span><span class="n">X_train_eigen</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Features with zero variance found. Dropping these features.&quot;</span><span class="p">)</span>
    <span class="n">X_train_eigen</span> <span class="o">=</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">var</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># Calculate eigenvalues and condition number</span>
<span class="n">corr_matrix</span> <span class="o">=</span> <span class="n">X_train_eigen</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">eigenvalues</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvals</span><span class="p">(</span><span class="n">corr_matrix</span><span class="p">)</span>
<span class="n">condition_number</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Condition Number: </span><span class="si">{</span><span class="n">condition_number</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eigenvalues</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Scree Plot of Eigenvalues&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Eigenvalue Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Eigenvalue&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Condition Number: 13.13
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_74_1.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_74_1.png" />
</div>
</div>
<p><strong>6. Pair Plot for Dummy Variables</strong></p>
<p>Visualize the relationships between dummy variables using a pair plot.</p>
<p>Interpretation:</p>
<p>Look for patterns that indicate strong associations between dummy variables.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Pair plot for dummy variables</span>
<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">dummy_vars</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Pair Plot for Dummy Variables&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_76_0.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_76_0.png" />
</div>
</div>
<p><strong>7. V-Cramer for categorical variables</strong></p>
<div class="line-block">
<div class="line"><strong>Cramer’s V</strong> is a measure of association between two nominal variables, useful for assessing multicollinearity among dummy variables.</div>
<div class="line">It ranges from 0 (no association) to 1 (perfect association).</div>
</div>
<p>Interpretation:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_categ</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">select_dtypes</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="s1">&#39;float&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train_categ</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>

<span class="n">categorical_columns</span> <span class="o">=</span> <span class="n">X_train_categ</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">cramersv_matrix</span> <span class="o">=</span> <span class="n">cramers_v_matrix</span><span class="p">(</span><span class="n">X_train_categ</span><span class="p">,</span> <span class="n">categorical_columns</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">cramersv_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cramer</span><span class="se">\&#39;</span><span class="s1">s V Correlation Matrix&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cramersv_matrix</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
     age_category_middle_aged  age_category_senior  pregnancies_category_low  pregnancies_category_moderate  pregnancies_category_high
602                         0                    0                         1                              0                          0
429                         1                    0                         1                              0                          0
623                         0                    0                         0                              0                          0
209                         1                    0                         0                              0                          1
589                         0                    0                         0                              0                          0
</pre></div></div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_79_1.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_79_1.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
                               age_category_middle_aged  age_category_senior  pregnancies_category_low  pregnancies_category_moderate  pregnancies_category_high
age_category_middle_aged                       0.996392             0.241145                  0.314978                       0.000000                   0.456066
age_category_senior                            0.241145             0.991383                  0.137756                       0.000000                   0.200564
pregnancies_category_low                       0.314978             0.137756                  0.996172                       0.390097                   0.422312
pregnancies_category_moderate                  0.000000             0.000000                  0.390097                       0.995767                   0.374867
pregnancies_category_high                      0.456066             0.200564                  0.422312                       0.374867                   0.996051
</pre></div></div>
</div>
<p><strong>8. Pearson correlation for dummy variables</strong></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate the correlation matrix for dummy variables</span>
<span class="n">dummy_corr_matrix</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[</span><span class="n">categorical_columns</span><span class="p">]</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>

<span class="c1"># Visualize the correlation matrix</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">dummy_corr_matrix</span><span class="p">,</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s2">&quot;.2f&quot;</span><span class="p">,</span> <span class="n">vmin</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">vmax</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Correlation Matrix for Dummy Variables&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_81_0.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_81_0.png" />
</div>
</div>
<section id="Fit-the-logistic-regression-model">
<h4><strong>Fit the logistic regression model</strong><a class="headerlink" href="#Fit-the-logistic-regression-model" title="Permalink to this heading"></a></h4>
<section id="Logistic-Regression-with-scipy">
<h5>Logistic Regression with scipy<a class="headerlink" href="#Logistic-Regression-with-scipy" title="Permalink to this heading"></a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Define X (mix of continuous and dummy variables) and y</span>
<span class="n">X_train_dum</span> <span class="o">=</span> <span class="n">X_train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">,</span> <span class="s1">&#39;pregnancies&#39;</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">])</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>  <span class="c1"># Add a constant for the intercept</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df_dummies</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">]</span>
<span class="n">X</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Add a constant for statsmodels (already done above, but included here for clarity)</span>

<span class="c1"># Logistic Regression</span>
<span class="n">logit_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">X_train</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">logit_model</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Display the summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Optimization terminated successfully.
         Current function value: 0.467587
         Iterations 6
                           Logit Regression Results
==============================================================================
Dep. Variable:                outcome   No. Observations:                  614
Model:                          Logit   Df Residuals:                      602
Method:                           MLE   Df Model:                           11
Date:                Sun, 19 Oct 2025   Pseudo R-squ.:                  0.2864
Time:                        19:25:56   Log-Likelihood:                -287.10
converged:                       True   LL-Null:                       -402.31
Covariance Type:            nonrobust   LLR p-value:                 3.452e-43
=================================================================================================
                                    coef    std err          z      P&gt;|z|      [0.025      0.975]
-------------------------------------------------------------------------------------------------
const                            -7.8714      0.875     -8.995      0.000      -9.587      -6.156
glucose                           0.0361      0.004      8.489      0.000       0.028       0.044
bloodpressure                    -0.0151      0.006     -2.546      0.011      -0.027      -0.003
skinthickness                     0.0058      0.008      0.743      0.457      -0.009       0.021
insulin                          -0.1195      0.114     -1.044      0.296      -0.344       0.105
bmi                               0.0892      0.018      5.077      0.000       0.055       0.124
diabetespedigreefunction          0.5566      0.393      1.415      0.157      -0.214       1.328
age_category_middle_aged          0.8625      0.278      3.108      0.002       0.319       1.407
age_category_senior               0.4932      0.385      1.281      0.200      -0.261       1.248
pregnancies_category_low          0.0859      0.366      0.235      0.814      -0.631       0.803
pregnancies_category_moderate     0.5141      0.367      1.401      0.161      -0.205       1.233
pregnancies_category_high         0.8088      0.397      2.039      0.041       0.031       1.586
=================================================================================================
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the odds ratios</span>
<span class="n">odds_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Odds Ratios:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">odds_ratios</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Odds Ratios:
const                            0.000381
glucose                          1.036720
bloodpressure                    0.985053
skinthickness                    1.005788
insulin                          0.887386
bmi                              1.093266
diabetespedigreefunction         1.744730
age_category_middle_aged         2.369158
age_category_senior              1.637534
pregnancies_category_low         1.089717
pregnancies_category_moderate    1.672058
pregnancies_category_high        2.245315
dtype: float64
</pre></div></div>
</div>
</section>
</section>
<section id="Linearity-of-Log-Odds">
<h4><strong>Linearity of Log-Odds</strong><a class="headerlink" href="#Linearity-of-Log-Odds" title="Permalink to this heading"></a></h4>
<p>Predicted Probabilities and Log-Odds:</p>
<ul class="simple">
<li><p>The linearity of log-odds assumption in logistic regression refers to the relationship between continuous predictors and the log-odds of the outcome.</p></li>
<li><p>To assess this, you need the predicted probabilities from a fitted logistic regression model. These probabilities are then transformed into log-odds.</p></li>
</ul>
<p>Visual Inspection:</p>
<ul class="simple">
<li><p>You plot the continuous predictors against the predicted log-odds to visually inspect if the relationship is linear.</p></li>
<li><p>If the relationship is not linear, you may need to transform the predictor (e.g., using polynomials, splines, or categorization).</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="o">.</span><span class="n">dtypes</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
const                            float64
glucose                          float64
bloodpressure                    float64
skinthickness                    float64
insulin                          float64
bmi                              float64
diabetespedigreefunction         float64
age_category_middle_aged           int64
age_category_senior                int64
pregnancies_category_low           int64
pregnancies_category_moderate      int64
pregnancies_category_high          int64
dtype: object
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Compute the predicted probabilities</span>
<span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_train</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>const</th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
      <th>age_category_middle_aged</th>
      <th>age_category_senior</th>
      <th>pregnancies_category_low</th>
      <th>pregnancies_category_moderate</th>
      <th>pregnancies_category_high</th>
      <th>predicted_prob</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>602</th>
      <td>1.0</td>
      <td>0.097192</td>
      <td>74</td>
      <td>36</td>
      <td>-0.692891</td>
      <td>27</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.150572</td>
    </tr>
    <tr>
      <th>429</th>
      <td>1.0</td>
      <td>-0.810425</td>
      <td>82</td>
      <td>25</td>
      <td>0.870031</td>
      <td>35</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0.172112</td>
    </tr>
    <tr>
      <th>623</th>
      <td>1.0</td>
      <td>-0.841722</td>
      <td>70</td>
      <td>27</td>
      <td>0.305642</td>
      <td>43</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.170485</td>
    </tr>
    <tr>
      <th>209</th>
      <td>1.0</td>
      <td>1.975021</td>
      <td>84</td>
      <td>33</td>
      <td>-0.692891</td>
      <td>35</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0.928543</td>
    </tr>
    <tr>
      <th>589</th>
      <td>1.0</td>
      <td>-1.498962</td>
      <td>0</td>
      <td>0</td>
      <td>-0.692891</td>
      <td>21</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.036140</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<section id="Calculate-Predicted-Probabilities-and-Log-Odds">
<h5>Calculate Predicted Probabilities and Log-Odds<a class="headerlink" href="#Calculate-Predicted-Probabilities-and-Log-Odds" title="Permalink to this heading"></a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># # Compute the predicted probabilities</span>
<span class="c1"># X_train[&#39;predicted_prob&#39;] = result.predict(X_train_with_const)</span>

<span class="c1"># # Calculate predicted log-odds</span>
<span class="c1"># X_train[&#39;predicted_log_odds&#39;] = np.log(X_train[&#39;predicted_prob&#39;] / (1 - X_train[&#39;predicted_prob&#39;]))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># X_train[&#39;predicted_log_odds&#39;] = np.log(X_train[&#39;predicted_prob&#39;] / (1 - X_train[&#39;predicted_prob&#39;]))</span>

<span class="c1"># # Plot log-odds vs each continuous predictor</span>
<span class="c1"># for predictor in [&#39;pregnancies&#39;,&#39;glucose&#39;, &#39;bloodpressure&#39;, &#39;skinthickness&#39;, &#39;insulin&#39;, &#39;bmi&#39;, &#39;diabetespedigreefunction&#39;]:</span>
<span class="c1">#     plt.figure(figsize=(8, 5))</span>
<span class="c1">#     plt.scatter(X_train[predictor], X_train[&#39;predicted_log_odds&#39;], alpha=0.5)</span>
<span class="c1">#     plt.title(f&#39;Log-Odds vs {predictor}&#39;)</span>
<span class="c1">#     plt.xlabel(predictor)</span>
<span class="c1">#     plt.ylabel(&#39;Log-Odds of Y=1&#39;)</span>
<span class="c1">#     plt.axhline(0, color=&#39;red&#39;, linestyle=&#39;--&#39;)</span>
<span class="c1">#     plt.show()</span>
</pre></div>
</div>
</div>
</section>
<section id="Plot-Log-Odds-vs.-Continuous-Predictors">
<h5>Plot Log-Odds vs. Continuous Predictors<a class="headerlink" href="#Plot-Log-Odds-vs.-Continuous-Predictors" title="Permalink to this heading"></a></h5>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># continuous_predictors = [&#39;glucose&#39;, &#39;bloodpressure&#39;, &#39;skinthickness&#39;, &#39;insulin&#39;, &#39;bmi&#39;, &#39;diabetespedigreefunction&#39;]</span>

<span class="c1"># for predictor in continuous_predictors:</span>
<span class="c1">#     plt.figure(figsize=(8, 5))</span>
<span class="c1">#     plt.scatter(X_train[predictor], X_train[&#39;predicted_log_odds&#39;], alpha=0.5)</span>
<span class="c1">#     plt.title(f&#39;Log-Odds vs {predictor}&#39;)</span>
<span class="c1">#     plt.xlabel(predictor)</span>
<span class="c1">#     plt.ylabel(&#39;Log-Odds of Y=1&#39;)</span>
<span class="c1">#     plt.axhline(0, color=&#39;red&#39;, linestyle=&#39;--&#39;)</span>
<span class="c1">#     plt.grid(True)</span>
<span class="c1">#     plt.show()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Check the hypothesis of linearity of log-odds for continuous predictors using plots.</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="k">for</span> <span class="n">predictor</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;pregnancies&#39;</span><span class="p">,</span><span class="s1">&#39;glucose&#39;</span><span class="p">,</span><span class="s1">&#39;bloodpressure&#39;</span><span class="p">,</span><span class="s1">&#39;skinthickness&#39;</span><span class="p">,</span><span class="s1">&#39;insulin&#39;</span><span class="p">,</span><span class="s1">&#39;bmi&#39;</span><span class="p">,</span><span class="s1">&#39;diabetespedigreefunction&#39;</span><span class="p">]:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">predictor</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">X_train</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">])),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Log-Odds vs </span><span class="si">{</span><span class="n">predictor</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="n">predictor</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Log-Odds of Y=1&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_0.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_0.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_1.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_1.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_2.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_2.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_3.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_3.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_4.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_4.png" />
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_5.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_5.png" />
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_100_6.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_100_6.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visualize the distributio of the categories</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">countplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;pregnancies_category&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Distribution of Pregnancies Categories&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Pregnancies Category&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Count&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/machine_learning_supervised_learning_LogisticRegression_102_0.png" src="../../_images/machine_learning_supervised_learning_LogisticRegression_102_0.png" />
</div>
</div>
<p><strong>Example with categorical features</strong></p>
<ul class="simple">
<li><p>Dataset preparation</p></li>
</ul>
<ul class="simple">
<li><p>Fit the logit model</p></li>
</ul>
<ul class="simple">
<li><p>Check the hypothesis of linearity of log-odds for continuous predictors using plots.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate predicted probabilities</span>
<span class="n">data_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Calculate predicted log-odds</span>
<span class="n">data_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_log_odds&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">data_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">data_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]))</span>

<span class="c1"># Add the original category back for plotting</span>
<span class="n">data_dummies</span><span class="p">[</span><span class="s1">&#39;category&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="s1">&#39;category&#39;</span><span class="p">]</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>
</section>
</section>
<section id="Coefficients-interpretation">
<h2>Coefficients interpretation<a class="headerlink" href="#Coefficients-interpretation" title="Permalink to this heading"></a></h2>
<p><strong>The Odds</strong></p>
<p>The odds are defined by:</p>
<div class="math notranslate nohighlight">
\[\text{Odds} = \dfrac{p}{1-p}\]</div>
<p><span class="math notranslate nohighlight">\(\text{Where} \quad p = P(target=1|X)\)</span></p>
<blockquote>
<div><p><em>If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are ‘3 to 1’:</em> <span class="math notranslate nohighlight">\(\text{Odds} = \dfrac{3/4}{1/4}=3\)</span></p>
</div></blockquote>
<ul>
<li><p><strong>Notation:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=0)=\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}\]</div>
</li>
</ul>
<section id="The-Odds-Ratio">
<h3><strong>The Odds Ratio</strong><a class="headerlink" href="#The-Odds-Ratio" title="Permalink to this heading"></a></h3>
<p>The odds ratio comparing the <strong>probability of :math:`target=1`</strong> between individuals with value <span class="math notranslate nohighlight">\(X\)</span> and those without it.</p>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{\text{Odds}(Y=1|X=1)}{\text{Odds}(Y=1|X=0)}\]</div>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}\]</div>
<p>We know that logit is given by:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \text{log}(\dfrac{p}{1-p}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_k x_{ik}\]</div>
</section>
<section id="Interpreting-the-Intercept">
<h3><strong>Interpreting the Intercept</strong><a class="headerlink" href="#Interpreting-the-Intercept" title="Permalink to this heading"></a></h3>
<div class="line-block">
<div class="line">The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>​ represents the <strong>log-odds of the outcome :math:`Y=1` when all predictors are equal to zero</strong>.</div>
<div class="line"><span class="math notranslate nohighlight">\(\beta_0\)</span>​ defines the <strong>baseline probability</strong> of the outcome when all predictors are zero.</div>
</div>
<div class="line-block">
<div class="line">⚠️ <strong>Caveat</strong>:</div>
<div class="line">This interpretation of <span class="math notranslate nohighlight">\(\beta_0\)</span> is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, <span class="math notranslate nohighlight">\(\beta_0\)</span>​ is primarily a mathematical component of the model and is rarely interpreted in isolation.</div>
</div>
<ul class="simple">
<li><p><strong>Odds for the baseline group:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1∣X_1=0)=\exp⁡(\beta_0)\]</div>
<ul>
<li><p><strong>Probability for the baseline group:</strong></p>
<div class="math notranslate nohighlight">
\[P(Y=1∣X_1=0)=\dfrac{\exp⁡(\beta_0)}{1 + \exp(\beta_0)}\]</div>
</li>
</ul>
<blockquote>
<div><p>If <span class="math notranslate nohighlight">\(X_1\)</span>​ is “smoking status” (<span class="math notranslate nohighlight">\(0\)</span> = non-smoker, <span class="math notranslate nohighlight">\(1\)</span> = smoker), then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span>​ gives the <strong>log-odds</strong> of the outcome for non-smokers</p></li>
<li><p><span class="math notranslate nohighlight">\(\exp(\beta_0)\)</span> gives the <strong>odds</strong> of the outcome for non-smokers.</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(\beta_0 = -1\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\exp(\beta_0) = \exp(-1) \approx 0.37\]</div>
<div class="math notranslate nohighlight">
\[P(Y=1∣X_1=0)= \dfrac{0.37}{1 + 0.37} \approx 0.27\]</div>
<div class="line-block">
<div class="line">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.</div>
<div class="line">It is the observed proportion of lung cancer for non-smokers.</div>
</div>
</div></blockquote>
</section>
<section id="Interpreting-the-Slope">
<h3><strong>Interpreting the Slope</strong><a class="headerlink" href="#Interpreting-the-Slope" title="Permalink to this heading"></a></h3>
<div class="line-block">
<div class="line">In a model with multiple predictors, each <span class="math notranslate nohighlight">\(\beta_i\)</span>​ (and its corresponding odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_i)\)</span> represents the effect of that predictor on the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span>, holding all other predictors constant.</div>
<div class="line">This is the key assumption of multivariable regression: ceteris paribus (all else being equal).</div>
</div>
<p>The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>​ represents the <strong>change in the log-odds of</strong> <span class="math notranslate nohighlight">\(Y=1\)</span> for a <strong>one-unit change</strong> in <span class="math notranslate nohighlight">\(X_1\)</span>​. The odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ quantifies how the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> change with <span class="math notranslate nohighlight">\(X_1\)</span>​.</p>
<p><strong>General Formula for Odds Ratio</strong></p>
<div class="line-block">
<div class="line">For any type of predictor <span class="math notranslate nohighlight">\(X_1\)</span>, the odds ratio for a one-unit increase is:</div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \frac{\text{Odds}(Y=1 | X_1 = x+1)}{\text{Odds}(Y=1 | X_1 = x)} = \exp(\beta_1)\]</div>
</div></blockquote>
<div class="line-block">
<div class="line">📌 <strong>Note:</strong></div>
<div class="line"><span class="math notranslate nohighlight">\(\exp(\beta_1​)\)</span> compares the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_1=0\)</span>, controlling for all other variables in the model (all others features constant).</div>
</div>
<ul class="simple">
<li><p>Case: <span class="math notranslate nohighlight">\(X_1\)</span> is Binary</p></li>
</ul>
<p>For a binary predictor <span class="math notranslate nohighlight">\(X_1\)</span>​ (e.g., <span class="math notranslate nohighlight">\(0\)</span> = non-smoker, <span class="math notranslate nohighlight">\(1\)</span> = smoker), the odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ compares the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between the two groups.</p>
<ul class="simple">
<li><p><strong>Logistic regression equation:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\log\left(\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\right) = \beta_0 + \beta_1 1_{\{X_1 = 1\}}​\]</div>
<ul>
<li><p><strong>Odds ratio:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \exp(\beta_1)\]</div>
</li>
</ul>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1) = 1\)</span>: No effect of the feature <span class="math notranslate nohighlight">\(X_1\)</span>​ on the odds of <span class="math notranslate nohighlight">\(Y=1\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1)&gt;1\)</span>: The odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are higher when <span class="math notranslate nohighlight">\(X_1​=1\)</span>. The feature <span class="math notranslate nohighlight">\(X_1\)</span>​ is <strong>positively associated</strong> with the outcome.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1) &lt; 1\)</span>: The odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are lower when <span class="math notranslate nohighlight">\(X_1​=1\)</span>. The feature <span class="math notranslate nohighlight">\(X_1\)</span>​ is <strong>negatively associated</strong> with the outcome.</p></li>
</ul>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>Example:</strong></div>
<div class="line">If <span class="math notranslate nohighlight">\(\beta_1 = 0.7 \rightarrow \exp(\beta_1) \approx 2.01\)</span>. The odds of lung cancer for smokers <span class="math notranslate nohighlight">\((X_1=1)\)</span> are twice as high as for non-smokers <span class="math notranslate nohighlight">\((X_1=0)\)</span>.</div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>Case: <span class="math notranslate nohighlight">\(X_1\)</span> is Categorical</p></li>
</ul>
<p>For a categorical predictor <span class="math notranslate nohighlight">\(X_1\)</span> with more than two levels (e.g., color = red, green, blue), you use <strong>dummy variables</strong>.</p>
<ul class="simple">
<li><p><strong>The logistic regression model becomes:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{green}1_{\{X_1 = \text{green}\}} + \beta_{blue}1_{\{X_1 = \text{blue}\}}\]</div>
<ul class="simple">
<li><p><strong>Reference Category (“red”):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=0\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=0\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0\]</div>
<p><strong>Interpretation:</strong> This means <span class="math notranslate nohighlight">\(\beta_0\)</span>​ represents the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for the reference category (“red”).</p>
<ul class="simple">
<li><p><strong>Category (“green”):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=1\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=0\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{green}\]</div>
<ul class="simple">
<li><p><strong>Category (“blue”):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=0\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=1\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{blue}\]</div>
<p>The <strong>odds ratio for “blue” relative to the reference “red”</strong> is:</p>
<div class="math notranslate nohighlight">
\[\exp(\beta_{blue}) = \dfrac{\text{Odds}(Y=1 | \text{blue})}{\text{Odds}(Y=1 | \text{red})}\]</div>
<p>The same way, <span class="math notranslate nohighlight">\(\exp(\beta_{\text{green}})\)</span>​ compares the odds for “green” vs. the “red” reference.</p>
<blockquote>
<div><p><strong>Interpretation:</strong></p>
<p>If <span class="math notranslate nohighlight">\(\exp(\beta_{\text{green}})​=1.5\)</span>, the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are <span class="math notranslate nohighlight">\(1.5\)</span> times higher for “green” compared to “red”.</p>
</div></blockquote>
<ul>
<li><p class="rubric" id="case-x-1-is-quantitative">Case: <span class="math notranslate nohighlight">\(X_1\)</span> is Quantitative</p>
</li>
</ul>
<p>For a continuous predictor <span class="math notranslate nohighlight">\(X_1\)</span> (e.g., age, blood pressure), the odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ represents the multiplicative change in the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>​.</p>
<ul class="simple">
<li><p><strong>Logistic regression equation:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\log\left(\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\right) = \beta_0 + \beta_1 X_1​\]</div>
<ul class="simple">
<li><p><strong>Odds ratio for a one-unit increase:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \frac{\text{Odds}(Y=1 | X_1 = x+1)}{\text{Odds}(Y=1 | X_1 = x)} = \exp(\beta_1)​\]</div>
<p><strong>In short</strong>: <span class="math notranslate nohighlight">\(\beta_1\)</span>​ captures the <strong>constant log-odds</strong> change per unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>, so <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ is the <strong>odds ratio</strong> for that one-unit change.</p>
<p>This holds regardless of the starting value of <span class="math notranslate nohighlight">\(X_1\)</span>​ because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression).</p>
<blockquote>
<div><p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\beta_1=0.095 \rightarrow \exp(\beta_1)=1.1\)</span>, the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> increase by 10% for each one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>​.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_1\)</span>​ is “years of smoking” and <span class="math notranslate nohighlight">\(\beta_1 = 0.7 \rightarrow  \exp(\beta_1) \approx 2.01\)</span>. For each additional year of smoking, the odds of lung cancer double. .</p></li>
</ul>
</div></blockquote>
<p><strong>Summary</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Type of <span class="math notranslate nohighlight">\(X_1\)</span></p></th>
<th class="head"><p>Interpretation of <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary</p></td>
<td><p>Compares odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_1=0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Categorical</p></td>
<td><p>Compares odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a given category relative to the reference.</p></td>
</tr>
<tr class="row-even"><td><p>Quantitative</p></td>
<td><p>Multiplicative change in odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span></p></td>
</tr>
</tbody>
</table>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate predicted probabilities</span>
<span class="n">diabetes_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Calculate predicted log-odds</span>
<span class="n">diabetes_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_log_odds&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">diabetes_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">diabetes_dummies</span><span class="p">[</span><span class="s1">&#39;predicted_prob&#39;</span><span class="p">]))</span>
<br/></pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the log-odds for each age category</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;age_category&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;predicted_log_odds&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">diabetes_dummies</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-Odds by Age Category&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age Category&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Log-Odds&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot the log-odds for each glucose category</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">sns</span><span class="o">.</span><span class="n">boxplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;glucose_category&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;predicted_log_odds&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">diabetes_dummies</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">y</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Log-Odds by Glucose Category&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Glucose Category&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Predicted Log-Odds&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<ol class="arabic simple" start="6">
<li><p>Interpretation of the Plots</p></li>
</ol>
<ul class="simple">
<li><p>Consistency of Log-Odds: The boxplot will show the distribution of predicted log-odds for each category. If the medians of the log-odds for each category are distinct, it suggests that the categorical predictor is having an effect on the log-odds of the outcome.</p></li>
<li><p>Effect of Categories: The difference in log-odds between categories should reflect the coefficients in the logistic regression output.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">odds_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">odds_ratios</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
const                       0.000218
pregnancies                 1.174037
glucose                     1.038217
bloodpressure               0.986439
skinthickness               1.003852
insulin                     0.998607
bmi                         1.089609
diabetespedigreefunction    2.732962
age                         1.006341
dtype: float64
</pre></div></div>
</div>
<p><strong>Interpreting the coefficients:</strong></p>
<p>For each feature, the exponentiated coefficient (exp(coef)) represents the change in odds for a one-unit increase in that feature, holding all other features constant.</p>
<div class="line-block">
<div class="line">For example, has the coefficient for ‘pregnancies’ is 0.1604, the odds ratio is exp(0.1604) ≈ 1.174037.</div>
<div class="line">This means that for each one-unit increase in pregnancies, the odds of the outcome occurring (e.g., having diabetes) increase by approximately 17.4%, assuming all other features remain constant.</div>
</div>
<p><strong>Diabetes Prediction: data preparation &amp; model inference</strong></p>
<ul class="simple">
<li><p>Preparing new input data for prediction</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check the number of features in the model</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of features in the model: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of features in the model: 9
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predictive system</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mf">25.8</span><span class="p">,</span> <span class="mf">0.587</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>

<span class="c1"># to numpy array</span>
<span class="n">input_data_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Reshape</span>
<span class="n">input_data_reshape</span> <span class="o">=</span> <span class="n">input_data_array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of input data:&quot;</span><span class="p">,</span> <span class="n">input_data_reshape</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Standardized the data</span>
<span class="n">input_data_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_data_reshape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of standardized input data:&quot;</span><span class="p">,</span> <span class="n">input_data_std</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Shape of input data: (1, 8)
Shape of standardized input data: (1, 8)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add a column of 1s for the intercept</span>
<span class="n">input_data_with_const</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">input_data_std</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of input data with constant:&quot;</span><span class="p">,</span> <span class="n">input_data_with_const</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Shape of input data with constant: (1, 9)
</pre></div></div>
</div>
<ul class="simple">
<li><p>Using the model to make a prediction</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make prediction</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data_with_const</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Predicted probability of diabetes: </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Predicted probability of diabetes: 0.00032363072678058075
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For binary classification, use a threshold (e.g., 0.5)</span>
<span class="k">if</span> <span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The person is diabetic&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The person is not diabetic&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The person is not diabetic
</pre></div></div>
</div>
<ul class="simple">
<li><p>Transform continuous to categorical</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">continuous_to_categorical</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">column</span><span class="p">,</span> <span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform a continuous variable into a categorical variable using binning.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - df: DataFrame containing the data.</span>
<span class="sd">    - column: Name of the column to transform.</span>
<span class="sd">    - bins: List of bin edges.</span>
<span class="sd">    - labels: List of labels for the bins.</span>

<span class="sd">    Returns:</span>
<span class="sd">    - DataFrame with the new categorical column.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="s1">_category&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="n">column</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span> <span class="n">include_lowest</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Tranform &#39;age&#39; to categorical</span>
<span class="n">bins_age</span> <span class="o">=</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>  <span class="c1"># Define bin edges for age</span>
<span class="n">labels_age</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Young&#39;</span><span class="p">,</span> <span class="s1">&#39;Middle-aged&#39;</span><span class="p">,</span> <span class="s1">&#39;Senior&#39;</span><span class="p">]</span>  <span class="c1"># Define labels for age categories</span>
<span class="n">diabetes_df</span> <span class="o">=</span> <span class="n">continuous_to_categorical</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="s1">&#39;age&#39;</span><span class="p">,</span> <span class="n">bins_age</span><span class="p">,</span> <span class="n">labels_age</span><span class="p">)</span>

<span class="c1"># Transform &#39;glucose&#39; to categorical</span>
<span class="n">bins_glucose</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">125</span><span class="p">,</span> <span class="mi">200</span><span class="p">]</span>  <span class="c1"># Define bin edges for glucose</span>
<span class="n">labels_glucose</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Low&#39;</span><span class="p">,</span> <span class="s1">&#39;Normal&#39;</span><span class="p">,</span> <span class="s1">&#39;High&#39;</span><span class="p">]</span>  <span class="c1"># Define labels for glucose categories</span>
<span class="n">diabetes_df</span> <span class="o">=</span> <span class="n">continuous_to_categorical</span><span class="p">(</span><span class="n">diabetes_df</span><span class="p">,</span> <span class="s1">&#39;glucose&#39;</span><span class="p">,</span> <span class="n">bins_glucose</span><span class="p">,</span> <span class="n">labels_glucose</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
</section>
<section id="APPENDIX">
<h2>APPENDIX<a class="headerlink" href="#APPENDIX" title="Permalink to this heading"></a></h2>
<section id="Interpreting-the-coefficients">
<h3>Interpreting the coefficients<a class="headerlink" href="#Interpreting-the-coefficients" title="Permalink to this heading"></a></h3>
<p><strong>Demonstration:</strong> The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>​ represents the <strong>change in the log-odds of</strong> <span class="math notranslate nohighlight">\(Y=1\)</span> for a <strong>one-unit change</strong> in <span class="math notranslate nohighlight">\(X_1\)</span>​ quantitative feature.</p>
<div class="line-block">
<div class="line">Notations:</div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))\]</div>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))\]</div>
<p>We know that:</p>
<div class="math notranslate nohighlight">
\[\text{log(Odds)}(Y=1|X=x+1)=\beta_0 + \beta_1 \times (x+1)\]</div>
<div class="math notranslate nohighlight">
\[\text{log(Odds)}(Y=1|X=x)=\beta_0 + \beta_1 \times x\]</div>
<p>By difference:</p>
<div class="math notranslate nohighlight">
\[\text{log(Odds)}(Y=1|X=x+1) - \text{log(Odds)}(Y=1|X=x) =\beta_0 + \beta_1 \times (x+1) - (\beta_0 + \beta_1 \times x) = \beta_1\]</div>
<div class="math notranslate nohighlight">
\[\text{log}\left(\dfrac{\text{Odds}(Y=1|X=x+1)}{\text{Odds}(Y=1|X=x)}\right) =\beta_1\]</div>
<p><strong>CQFD</strong></p>
<p>Note:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\text{Odds}(Y=1|X=x+1)}{\text{Odds}(Y=1|X=x)} = \exp(\beta_1)\]</div>
</section>
<section id="Model-formulation">
<h3>Model formulation<a class="headerlink" href="#Model-formulation" title="Permalink to this heading"></a></h3>
<p>The prediction <span class="math notranslate nohighlight">\(y_i=1\)</span> of the logistic regression is defined:</p>
<div class="math notranslate nohighlight">
\[\hat{y_i} = P(y_i=1 | x_i; \theta) = \frac{1}{1 + \exp(-\theta^Tx_i)} = h_{\theta}(x_i)\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y_i=1\)</span>, then <span class="math notranslate nohighlight">\(P(y_i|x_i; \theta)=P(y_i=1|x_i; \theta)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(y_i=0\)</span>, then <span class="math notranslate nohighlight">\(P(y_i|x_i; \theta)=P(y_i=0|x_i; \theta) = 1 - P(y_i=1|x_i; \theta)\)</span></p></li>
</ul>
<p>We can write these two equations into a single one:</p>
<div class="math notranslate nohighlight">
\[P(y_i|x_i; \theta)=P(y_i=1|x_i; \theta)^{y_i}\times (1 - P(y_i=1|x_i; \theta))^{1-y_i}\]</div>
<p>With the notations:</p>
<div class="math notranslate nohighlight">
\[P(y_i|x_i; \theta)=h_{\theta}(x_i)^{y_i}\times (1 - h_{\theta}(x_i))^{1-y_i}\]</div>
</section>
<section id="Likelihood-function">
<h3>Likelihood function<a class="headerlink" href="#Likelihood-function" title="Permalink to this heading"></a></h3>
<p>The <strong>likelihood</strong> of the observations <span class="math notranslate nohighlight">\(y_i\)</span> given the inputs <span class="math notranslate nohighlight">\(x_i\)</span> and parameters <span class="math notranslate nohighlight">\(\theta\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \prod_{i=1}^n P(y_i|x_i;\theta) = \prod_{i=1}^n (h_{\theta}(x_i))^{y_i} (1 -h_{\theta}(x_i))^{1-y_i}\]</div>
<p>where the prediction of the logistic regression is defined:</p>
<div class="math notranslate nohighlight">
\[h_{\theta}(x_i) = P(y_i=1 | x_i; \theta) = \frac{1}{1 + \exp(-\theta^Tx_i)}\]</div>
<p>The <strong>log-likelihood</strong> is defined as:</p>
<div class="math notranslate nohighlight">
\[l(\theta) = \log(L(\theta))=\sum_{i=1}^n[y_i \log (h_{\theta}(x_i)) + (1 - y_i) \log(1 - h_{\theta}(x_i))]\]</div>
</section>
<section id="Objective-of-Logistic-Regession">
<h3>Objective of Logistic Regession<a class="headerlink" href="#Objective-of-Logistic-Regession" title="Permalink to this heading"></a></h3>
<p>The goal of learning a <strong>logistic regression model</strong> is to <strong>minimize the cost function</strong> by adjusting the parameters <span class="math notranslate nohighlight">\(\theta\)</span>.The cost function measures the average prediction error across all <span class="math notranslate nohighlight">\(n\)</span> training samples.</p>
</section>
<section id="Cost-function-(general-definition)">
<h3>Cost function (general definition)<a class="headerlink" href="#Cost-function-(general-definition)" title="Permalink to this heading"></a></h3>
<p>The cost function <span class="math notranslate nohighlight">\(J(\theta)\)</span> is defined as the average penalty for prediction errors across the training set. Mathematically, it is expressed as:</p>
<div class="math notranslate nohighlight">
\[J(θ) = -\frac{1}{n} \sum cost(h_\theta(x_i), y_i)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h_\theta(x_i)\)</span> the model’s prediction for sample <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> the observed (true) value.</p></li>
</ul>
<p>Alternatively, it can be written as:</p>
<div class="math notranslate nohighlight">
\[J(θ) = \frac{1}{n} \sum cost(\hat{y_i}, y_i)\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y_i} = h_{\theta}(x_i)\)</span></p></li>
</ul>
</section>
<section id="Cost-function-log-loss">
<h3>Cost function <strong>log loss</strong><a class="headerlink" href="#Cost-function-log-loss" title="Permalink to this heading"></a></h3>
<div class="line-block">
<div class="line">For logistic regression, the cost function is called <strong>log loss</strong> (or logistic loss).</div>
<div class="line"><strong>Log loss</strong> is derived from the log-likelihood <span class="math notranslate nohighlight">\(l(\theta)\)</span> and is defined as:</div>
</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\dfrac{1}{n}l(\theta) = -\frac{1}{n} \sum(y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i)))\]</div>
<section id="How-Log-Loss-penalizes-prediction-errors">
<h4>How Log-Loss penalizes prediction errors<a class="headerlink" href="#How-Log-Loss-penalizes-prediction-errors" title="Permalink to this heading"></a></h4>
<p>The log-loss function penalizes prediction errors based on the estimated probability from the model. It assigns higher penalties when predictions are far from the true labels—specifically:</p>
<ul class="simple">
<li><p>If the label <span class="math notranslate nohighlight">\(y_i = 1\)</span> (positive class), the penalty is <span class="math notranslate nohighlight">\(-log(h_{\theta}(x_i))\)</span>. The closer <span class="math notranslate nohighlight">\(h_{\theta}(x_i)\)</span> is to 0 (far from the true label), the higher the penalty (<span class="math notranslate nohighlight">\(-\log(0^+) \approx + \infty\)</span>).</p></li>
<li><p>If the label <span class="math notranslate nohighlight">\(y_i = 0\)</span> (negative class), the penalty is <span class="math notranslate nohighlight">\(-log(1-h_{\theta}(x_i))\)</span>, the closer <span class="math notranslate nohighlight">\(h_{\theta}(x_i)\)</span> is to 1 (far from the true label), the higher the penalty (<span class="math notranslate nohighlight">\(-\log(1 -1^+) \approx + \infty\)</span>).</p></li>
</ul>
<p>The two cases are combined into a single formula for observation <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[y_i log(h_\theta(x_i)) + (1 - y_i) log(1 - h_\theta(x_i))\]</div>
<p><strong>Key insights:</strong></p>
<ul class="simple">
<li><p><strong>Log loss</strong> evaluates how well the model fits the training data.</p></li>
<li><p>The log loss is the negative average the log likelihood.</p></li>
<li><p><strong>Higher likelihood</strong> leads to <strong>lower log loss</strong> (since <span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{n}l(\theta)\)</span>).</p></li>
<li><p>The log-loss function heavily penalizes confident wrong predictions.</p></li>
</ul>
</section>
</section>
<section id="Optimizing-the-parameters">
<h3>Optimizing the parameters<a class="headerlink" href="#Optimizing-the-parameters" title="Permalink to this heading"></a></h3>
<p>By minimizing the cost function <span class="math notranslate nohighlight">\(J(\theta)\)</span>, we aim to find the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that maximize the likelihood of observing the training data given the model parameters.</p>
<p>To achieve this, we use an iterative optimization method, <strong>gradient descent</strong>, to find the values of <span class="math notranslate nohighlight">\(\theta\)</span> that minimize the cost function over the training set:</p>
<div class="math notranslate nohighlight">
\[\underset{\theta}{minimize}(J(\theta))\]</div>
</section>
<section id="Parameter-estimation-methods">
<h3>Parameter estimation methods<a class="headerlink" href="#Parameter-estimation-methods" title="Permalink to this heading"></a></h3>
<section id="Gradient-Descent">
<h4>Gradient Descent<a class="headerlink" href="#Gradient-Descent" title="Permalink to this heading"></a></h4>
<p>The gradient descent only uses the <strong>gradient</strong> (first derivative) of the cost function to update the parameters <span class="math notranslate nohighlight">\(\theta\)</span> in the opposite direction of the gradient, scaled by a learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>To compute the gradient <span class="math notranslate nohighlight">\(\nabla J(\theta)\)</span> we start by transforming the expression of:</p>
<div class="math notranslate nohighlight">
\[\log(h_\theta(x_i)) = \log\left(\frac{1}{1 + \exp(-\theta^Tx_i)}\right) = -\log(1 + \exp(-\theta^Tx_i))\]</div>
<p>And:</p>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log\left(1 - \frac{1}{1 + \exp(-\theta^Tx_i)}\right)\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log\left(\frac{1 + \exp(-\theta^Tx_i) - 1}{1 + \exp(-\theta^Tx_i)}\right)\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log\left(\frac{\exp(-\theta^Tx_i)}{1 + \exp(-\theta^Tx_i)}\right)\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log(\exp(-\theta^Tx_i)) - \log({1 + \exp(-\theta^Tx_i)})\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)})\]</div>
<p>We integrate these modifications:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i (-\log(1 + \exp(-\theta^Tx_i))) + (1 - y_i) (-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i (-\log(1 + \exp(-\theta^Tx_i))) + (1 - y_i) (-\theta^Tx_i - \log(1 + exp(-\theta^Tx_i)))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i (-\log(1 + \exp(-\theta^Tx_i))) -\theta^Tx_i - \log(1 + \exp(-\theta^Tx_i)) + y_i \theta^Tx_i  + y_i \log(1 + \exp(-\theta^Tx_i))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[- y_i \log(1 + \exp(-\theta^Tx_i)) -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) + y_i \theta^Tx_i  + y_i \log(1 + \exp(-\theta^Tx_i))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[\cancel{- y_i \log(1 + \exp(-\theta^Tx_i))} -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) + y_i \theta^Tx_i  + \cancel{y_i \log(1 + \exp(-\theta^Tx_i))}]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) + y_i \theta^Tx_i  ]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i \theta^Tx_i  -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) ]\]</div>
<p>with:</p>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = - \log(\exp(\theta^T x_i)) - \log(1 + \exp(-\theta^Tx_i))\]</div>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = -(\log(\exp(\theta^T x_i)) + \log(1 + \exp(-\theta^Tx_i)))\]</div>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = -\log[\exp(\theta^T x_i)(1 + \exp(-\theta^Tx_i))]\]</div>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = -\log(\exp(\theta^T x_i) + 1)\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i \theta^Tx_i  -\log(\exp(\theta^T x_i + 1)) ]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i \theta^Tx_i  -\log(1 + \exp(\theta^T x_i)) ]\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j}J(\theta) = -\frac{1}{n} \sum[y_i \frac{\partial}{\partial \theta_j} (\theta^Tx_i)  - \frac{\partial}{\partial \theta_j}\log(1 + \exp(\theta^T x_i)) ]\]</div>
<p>Knowing that:</p>
<div class="math notranslate nohighlight">
\[\theta^Tx_i = \theta_1 {x_i}^{(1)} + \theta_2 {x_i}^{(2)} + \ldots + \theta_k {x_i}^{(k)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} (\theta^Tx_i) = x_i^{(j)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial \theta_j}\left(\log(1 + \exp(\theta^T x_i))\right) \underset{\log(u)^{'} = \dfrac{u^{'}}{u}} = \dfrac{\dfrac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<p>And:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial \theta_j}\left(\log(1 + \exp(\theta^T x_i))\right) \underset{\log(u)^{'} = \dfrac{u^{'}}{u}} = \dfrac{\dfrac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\dfrac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))} {1 + \exp(\theta x_i)} = \dfrac{\dfrac{\partial}{\partial \theta_j}(\exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\dfrac{\partial}{\partial \theta_j}(\exp(\theta^T x_i))} {1 + \exp(\theta x_i)} \underset{\exp(u)^{'} = u^{'}\exp(u)} =  \dfrac{\dfrac{\partial}{\partial \theta_j}(\theta^T x_i) * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\dfrac{\partial}{\partial \theta_j}(\theta^T x_i) * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)} = \frac{x_i^{(j)} * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{x_i^{(j)} * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)} = x_i^{(j)} * h_\theta(x_i)\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial \theta_j}J(\theta) = -\dfrac{1}{n} \sum[y_i x_i^{(j)}  - x_i^{(j)} h_\theta(x_i)]\]</div>
<div class="math notranslate nohighlight">
\[-\dfrac{1}{n} \sum[y_i x_i^{(j)}  - x_i^{(j)} h_\theta(x_i)] = -\dfrac{1}{n} \sum[y_i - h_\theta(x_i) ] x_i^{(j)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{n} \sum[h_\theta(x_i) - y_i ] x_i^{(j)}\]</div>
<p>We know that the expression of the Gradient descent to update the weights is for the weight <span class="math notranslate nohighlight">\(\theta_j\)</span> :</p>
<div class="math notranslate nohighlight">
\[\theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)\]</div>
<div class="math notranslate nohighlight">
\[\theta_j = \theta_j - \frac{\alpha}{n} \sum[h_\theta(x_i) - y_i ] x_i^{(j)}\]</div>
</section>
<section id="The-Newton-Raphson-algorithm">
<h4>The Newton-Raphson algorithm<a class="headerlink" href="#The-Newton-Raphson-algorithm" title="Permalink to this heading"></a></h4>
<p>Others algorithms can be used to find the coefficients of the logistic regression. The Newton-Raphson algorithm is an alternative to the gradient descent.</p>
<p>The Newton-Raphson method uses both the first derivative (gradient) and the second derivative (Hessian) of the cost function. It approximates the cost function as a quadratic form and finds the minimum of this approximation.</p>
<p><strong>Update rule</strong></p>
<div class="math notranslate nohighlight">
\[\theta = \theta - H^{-1} \nabla J(\theta)\]</div>
<p>Logistic regression is a regression model used to predict the probability of a binary event based on one or more predictive variables. In logistic regression, the likelihood function is convex and can be maximized using the Newton-Raphson algorithm.</p>
<p>The Newton-Raphson algorithm is an iterative method for finding the maximum of a function using its first and second derivatives. For logistic regression, the likelihood function is given by:</p>
<p><span class="math notranslate nohighlight">\(L(\theta | X, y) = \prod(P(yi | x_i, \theta)^{yi} (1 - P(y_i | x_i, \theta))^{(1 - y_i)})\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the vector of logistic regression coefficients,</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the matrix of predictive variables,</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the vector of binary response variables, and</p></li>
<li><p><span class="math notranslate nohighlight">\(P(y_i | x_i, \theta)\)</span> is the predicted probability of the binary event for observation <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>To maximize the likelihood function, the Newton-Raphson algorithm updates the coefficient vector <span class="math notranslate nohighlight">\(\theta\)</span> at each iteration using the following formula:</p>
<p><span class="math notranslate nohighlight">\(\theta_{i+1} = \theta_i - H^{-1} . g\)</span></p>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H = \dfrac{\partial^2L}{\partial \theta \partial\theta'}\)</span> is the Hessian matrix of the likelihood function.</p></li>
<li><p><span class="math notranslate nohighlight">\(g = \dfrac{\partial L}{\partial \theta }\)</span> is the gradient vector of the likelihood function, and,</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> is the coefficient vector at iteration <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>The Hessian matrix and gradient vector of the likelihood function are computed using the partial derivatives of the likelihood with respect to the coefficients <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Thus, in logistic regression, the Newton-Raphson algorithm is used to estimate the coefficients by maximizing the likelihood function. This allows the prediction of binary event probabilities based on the predictive variables.</p>
<p><strong>Note</strong>: The iterations stop when the difference between two successive solution vectors becomes negligible.</p>
</section>
</section>
<section id="Convexity">
<h3>Convexity<a class="headerlink" href="#Convexity" title="Permalink to this heading"></a></h3>
<p>Convexity is a crucial property in optimization, as it ensures that any local minimum is also a global minimum. This makes it easier to find the optimal solution using methods like gradient descent.</p>
<p>The Log Loss function is convex due to its logarithmic form, which is always convex for positive values. While convexity guarantees that a stationary point (where the derivative is zero) is a global minimum, it does not guarantee the existence of such a point.</p>
<p>To prove the existence of a minimum, the function must also be bounded below and attain this lower bound. For Log Loss, the function is bounded below by zero (since the logarithm of a positive number is always defined) and reaches this bound when predictions are perfectly accurate (i.e., the predicted probability for the correct class is 1).</p>
<p>Combining convexity with the fact that Log Loss is bounded below and attains its lower bound, we conclude that the function reaches a global minimum when predictions are perfectly correct.</p>
<p>Another way to compute the coefficients</p>
</section>
<section id="Logistic-Regression-from-scratch-with-Gradient-Descent">
<h3>Logistic Regression from scratch with Gradient Descent<a class="headerlink" href="#Logistic-Regression-from-scratch-with-Gradient-Descent" title="Permalink to this heading"></a></h3>
<section id="The-Algorithm-steps">
<h4>The Algorithm steps<a class="headerlink" href="#The-Algorithm-steps" title="Permalink to this heading"></a></h4>
<p>Note: <span class="math notranslate nohighlight">\(\theta = (w,b)\)</span></p>
<p>with <span class="math notranslate nohighlight">\(h_\theta(x) = \frac{1}{1 + \exp(-w x + b)}\)</span></p>
</section>
<section id="Training">
<h4>Training<a class="headerlink" href="#Training" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>Initialize weights as zero</p></li>
<li><p>Initialize bias as zero</p></li>
</ul>
</section>
<section id="Given-a-data-point">
<h4>Given a data point<a class="headerlink" href="#Given-a-data-point" title="Permalink to this heading"></a></h4>
<ul class="simple">
<li><p>Predict result by using <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{1 + \exp(-wx+b)}\)</span></p></li>
<li><p>Calculate the error</p></li>
<li><p>Use Gradient descent to figure out new weights and bias values</p></li>
<li><p>Repeat n times</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Logistic_Regression</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Initiate the constructor</span>
<span class="sd">            INPUT:</span>
<span class="sd">                learning_rate: magnitude of the step</span>
<span class="sd">                n_iter: number of iterations</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Train the model</span>
<span class="sd">        INPUTS:</span>
<span class="sd">            X: the dataset of the features</span>
<span class="sd">            y: the target</span>
<span class="sd">        OUTPUTS:</span>
<span class="sd">            The model</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># initialize the parameters:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Update of the weights with Gradient descent&#39;&#39;&#39;</span>

        <span class="c1"># we compute the prediction (the probability)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)))</span>

        <span class="c1"># update the weights:</span>
        <span class="c1"># w_j = w_j - (alpha / n) * S(p_hat - y_i)xij</span>
        <span class="c1"># b = b - (alpha / n) * S(p_hat - y_i)</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">))</span>
        <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">dw</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">db</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)))</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>
</div>
</div>
</section>
<section id="Testing">
<h4>Testing<a class="headerlink" href="#Testing" title="Permalink to this heading"></a></h4>
<p><strong>Load the dataset</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_true</span><span class="o">==</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
</div>
<p><strong>Fit the model</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">Logistic_Regression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>Assess the model</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy on the training data</span>
<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">training_data_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">training_data_accuracy</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.3544600938967136
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy on the training data</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_data_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
<span class="n">test_data_accuracy</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.42657342657342656
</pre></div></div>
</div>
<p>Given a data point:</p>
<ul class="simple">
<li><p>Put the values from the data point into the equation <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{1 + \exp(-w+b)}\)</span></p></li>
<li><p>Choose the label based on the probability</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">,:]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[1.616e+01, 2.154e+01, 1.062e+02, 8.098e+02, 1.008e-01, 1.284e-01,
        1.043e-01, 5.613e-02, 2.160e-01, 5.891e-02, 4.332e-01, 1.265e+00,
        2.844e+00, 4.368e+01, 4.877e-03, 1.952e-02, 2.219e-02, 9.231e-03,
        1.535e-02, 2.373e-03, 1.947e+01, 3.168e+01, 1.297e+02, 1.175e+03,
        1.395e-01, 3.055e-01, 2.992e-01, 1.312e-01, 3.480e-01, 7.619e-02],
       [1.195e+01, 1.496e+01, 7.723e+01, 4.267e+02, 1.158e-01, 1.206e-01,
        1.171e-02, 1.787e-02, 2.459e-01, 6.581e-02, 3.610e-01, 1.050e+00,
        2.455e+00, 2.665e+01, 5.800e-03, 2.417e-02, 7.816e-03, 1.052e-02,
        2.734e-02, 3.114e-03, 1.281e+01, 1.772e+01, 8.309e+01, 4.962e+02,
        1.293e-01, 1.885e-01, 3.122e-02, 4.766e-02, 3.124e-01, 7.590e-02],
       [1.340e+01, 2.052e+01, 8.864e+01, 5.567e+02, 1.106e-01, 1.469e-01,
        1.445e-01, 8.172e-02, 2.116e-01, 7.325e-02, 3.906e-01, 9.306e-01,
        3.093e+00, 3.367e+01, 5.414e-03, 2.265e-02, 3.452e-02, 1.334e-02,
        1.705e-02, 4.005e-03, 1.641e+01, 2.966e+01, 1.133e+02, 8.444e+02,
        1.574e-01, 3.856e-01, 5.106e-01, 2.051e-01, 3.585e-01, 1.109e-01],
       [1.625e+01, 1.951e+01, 1.098e+02, 8.158e+02, 1.026e-01, 1.893e-01,
        2.236e-01, 9.194e-02, 2.151e-01, 6.578e-02, 3.147e-01, 9.857e-01,
        3.070e+00, 3.312e+01, 9.197e-03, 5.470e-02, 8.079e-02, 2.215e-02,
        2.773e-02, 6.355e-03, 1.739e+01, 2.305e+01, 1.221e+02, 9.397e+02,
        1.377e-01, 4.462e-01, 5.897e-01, 1.775e-01, 3.318e-01, 9.136e-02],
       [7.691e+00, 2.544e+01, 4.834e+01, 1.704e+02, 8.668e-02, 1.199e-01,
        9.252e-02, 1.364e-02, 2.037e-01, 7.751e-02, 2.196e-01, 1.479e+00,
        1.445e+00, 1.173e+01, 1.547e-02, 6.457e-02, 9.252e-02, 1.364e-02,
        2.105e-02, 7.551e-03, 8.678e+00, 3.189e+01, 5.449e+01, 2.236e+02,
        1.596e-01, 3.064e-01, 3.393e-01, 5.000e-02, 2.790e-01, 1.066e-01]])
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">Logistic_Regression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.3986013986013986
</pre></div></div>
</div>
<p>The inputs:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X=\begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1,k} \\ x_{2,1} &amp; \ldots &amp; x_{2,k} \\ \ldots &amp; x_{i,j} &amp; \ldots \\ x_{n,1} &amp; \ldots &amp; x_{n,k} \end{pmatrix}, w=\begin{pmatrix} w_1 \\ w_2 \\ \ldots \\ w_k \end{pmatrix}, b = \text{constant}\end{split}\]</div>
<p>The linear model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X.w + b = \begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1,k} \\ x_{2,1} &amp; \ldots &amp; x_{2,k} \\ \ldots &amp; x_{i,j} &amp; \ldots \\ x_{n,1} &amp; \ldots &amp; x_{n,k} \end{pmatrix}.\begin{pmatrix} w_1 \\ w_2 \\ \ldots \\ w_k \end{pmatrix} + b = \begin{pmatrix} x_{1,1}w_1 + &amp; \ldots &amp; + x_{1,k}w_k + b \\ x_{2,1}w_1 + &amp; \ldots &amp; + x_{2,k}w_k + b \\ \ldots &amp; \ldots &amp;   \ldots \\ x_{n,1}w_1 + &amp; \ldots &amp; + x_{n,k}w_k + b \end{pmatrix}\end{split}\]</div>
<p>The model prediction (output) is given by:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}(X.w+b) = \frac{1}{1 + \exp(-X.w+b)}= \hat{p} = h_\omega(X)\]</div>
<p>The updates of the weights and bias are given by:</p>
<div class="math notranslate nohighlight">
\[\omega_j = \omega_j - \frac{\alpha}{n} \sum[h_\omega(x_i) - y_i ] x_{i,j}\]</div>
<div class="math notranslate nohighlight">
\[b = b - \frac{\alpha}{n} \sum[h_\omega(x_i) - y_i ]\]</div>
<p>For <span class="math notranslate nohighlight">\(\omega\)</span> using linear algebra formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\omega = X^t.(\hat{p} - y) = \begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1,n} \\ x_{2,1} &amp; \ldots &amp; x_{2,n} \\ \ldots &amp; x_{i,j} &amp; \ldots \\ x_{k,1} &amp; \ldots &amp; x_{k,n} \end{pmatrix}.\begin{pmatrix} \hat{p_1} - y_1 \\ \hat{p_2} - y_2 \\ \ldots \\ \hat{p_n} - y_n \end{pmatrix}\end{split}\]</div>
<p>For <span class="math notranslate nohighlight">\(b\)</span> using linear algebra formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}b = \sum(\hat{p} - y) = \sum\begin{pmatrix} \hat{p_1} - y_1 \\ \hat{p_2} - y_2 \\ \ldots \\ \hat{p_n} - y_n \end{pmatrix}\end{split}\]</div>
<p>The weights and bias are given by:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}(X.w+b) = \frac{1}{1 + \exp(-X.w+b)}= \hat{p}\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The Logistic Regression from scratch</span>
</pre></div>
</div>
</div>
</section>
</section>
</section>
<section id="Coefficients-significativity">
<h2>Coefficients significativity<a class="headerlink" href="#Coefficients-significativity" title="Permalink to this heading"></a></h2>
<p>The Wald statistic allows to test the coefficients significativity <span class="math notranslate nohighlight">\(\hat{w_j}\)</span>. Wald statistic is given by:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>:math:`(\frac{\hat{w_j}}{\sigma(\hat{w_j})})^2`
</pre></div>
</div>
<p>Under $H_0 : {<span class="math">\hat{w_j}</span> = 0 } <span class="math">\Longrightarrow `:nbsphinx-math:</span>frac{hat{w_j}}{sigma(hat{w_j})}` $ ~ <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span></p>
<p>The added-value of the variable <span class="math notranslate nohighlight">\(X_j\)</span> is only real if the Wald statistic &gt; 4 <span class="math notranslate nohighlight">\((3.84 = 1.96^2)\)</span></p>
<p><span class="math notranslate nohighlight">\(Wald &gt; 4\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff (\frac{\hat{w_j}}{\sigma(\hat{w_j})})^2 &gt; 4\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff \frac{\hat{w_j}}{\sigma(\hat{w_j})} &gt; 2\)</span></p>
<p>$:nbsphinx-math:<cite>iff `:nbsphinx-math:</cite>hat{w_j}` &gt; 2:nbsphinx-math:<cite>sigma`(:nbsphinx-math:</cite>hat{w_j}`) $</p>
<p><span class="math notranslate nohighlight">\(\iff \hat{w_j} - 2\sigma(\hat{w_j}) &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff \hat{w_j}\)</span> se trouve à plus de 2 écarts-type de 0</p>
<p>$:nbsphinx-math:<cite>iff `$ l’intervalle de confiance de :math:</cite>hat{w_j}` ne contient pas 0 à 95%</p>
<p>CQFD</p>
</section>
<section id="Model-quality-mesure-(Deviance)">
<h2>Model quality mesure (Deviance)<a class="headerlink" href="#Model-quality-mesure-(Deviance)" title="Permalink to this heading"></a></h2>
<p>Cf. S.Tufféry p.315</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(n:\)</span> number of observations</div>
<div class="line"><span class="math notranslate nohighlight">\(k:\)</span> number of features</div>
</div>
<p><span class="math notranslate nohighlight">\(L(\omega_k)\)</span> Likelihood of the “modèle ajusté”</p>
<p><span class="math notranslate nohighlight">\(L(\omega_0)\)</span> Likelihood of the “modèle réduit à la constante”</p>
<p><span class="math notranslate nohighlight">\(L(\omega_{max})\)</span> Likelihood of the “modèle saturé”. The one the model will compare.</p>
<p>The Deviance formula:</p>
<p><span class="math notranslate nohighlight">\(D(\omega_k) = -2[log(L(\omega_k)) - log(L(\omega_{max}))]\)</span> <span class="math notranslate nohighlight">\(^{(*)}\)</span></p>
<p>As the target is 0 or 1 <span class="math notranslate nohighlight">\(\Longrightarrow L(\omega_{max})=1 \Longrightarrow log(L(\omega_{max}))=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\Longrightarrow D(\omega_k) = -2[log(L(\omega_k))]\)</span></p>
<p>(*) <span class="math notranslate nohighlight">\(D(\omega_k) = (\frac{log(L(\omega_k))}{log(L(\omega_{max}))}^2)\)</span></p>
<p>The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance.</p>
<p>The Deviance is equivalent to the SCE for the linear regression.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Supervised learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
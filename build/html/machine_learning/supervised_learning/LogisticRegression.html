

<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The logistic regression &mdash; website Machine Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=e031e9a9"></script>
      <script src="../../_static/doctools.js?v=888ff710"></script>
      <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Supervised learning" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            website Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../probabilities/index.html">Probabilities</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Machine Leanring</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/index.html">Unsupervised learning</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Supervised learning</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">The logistic regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Table-of-Contents"><strong>Table of Contents</strong></a></li>
<li class="toctree-l4"><a class="reference internal" href="#Introduction-to-Logistic-Regression">Introduction to Logistic Regression</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-model">The model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-linear-LOGIT-model">The linear LOGIT model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Key-Assumptions-for-Generalizability-of-the-logit-model">Key Assumptions for Generalizability of the logit model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Coefficients-interpretation">Coefficients interpretation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#APPENDIX">APPENDIX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Coefficients-significativity">Coefficients significativity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Model-quality-mesure-(Deviance)">Model quality mesure (Deviance)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">website Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Machine Leanring</a></li>
          <li class="breadcrumb-item"><a href="index.html">Supervised learning</a></li>
      <li class="breadcrumb-item active">The logistic regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/machine_learning/supervised_learning/LogisticRegression.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="The-logistic-regression">
<h1>The logistic regression<a class="headerlink" href="#The-logistic-regression" title="Permalink to this heading"></a></h1>
<section id="Table-of-Contents">
<h2><strong>Table of Contents</strong><a class="headerlink" href="#Table-of-Contents" title="Permalink to this heading"></a></h2>
<ol class="arabic">
<li><p><a class="reference internal" href="#Introduction-to-Logistic-Regression"><span class="std std-ref">Introduction to Logistic Regression</span></a></p></li>
<li><p>Theory and Fundamentals</p>
<ul class="simple">
<li><p>Model and Assumptions</p></li>
<li><p>Coefficient Interpretation</p></li>
<li><p>Cost Function and Optimization</p></li>
</ul>
</li>
<li><p><cite>Implementation with ``statsmodels`</cite> &lt;#implementation-with-statsmodels&gt;`__</p>
<ul class="simple">
<li><p>Data Preparation</p></li>
<li><p>Training and Interpretation</p></li>
</ul>
</li>
<li><p>Implementation “From Scratch”</p>
<ul class="simple">
<li><p>Gradient Descent Algorithm</p></li>
<li><p>Testing and Validation</p></li>
</ul>
</li>
<li><p>Application: Diabetes Prediction</p>
<ul class="simple">
<li><p>Data Preparation</p></li>
<li><p>Prediction and Interpretation</p></li>
</ul>
</li>
<li><p>Appendices</p>
<ul class="simple">
<li><p>Mathematical Demonstrations</p></li>
<li><p>Wald Statistic and Deviance</p></li>
</ul>
</li>
<li><p class="rubric" id="references">References</p>
</li>
</ol>
</section>
<section id="Introduction-to-Logistic-Regression">
<h2>Introduction to Logistic Regression<a class="headerlink" href="#Introduction-to-Logistic-Regression" title="Permalink to this heading"></a></h2>
<p>The Logistic regression applies to cases where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> is a random qualitative variable with 2 categories (a binary variable by convention, <span class="math notranslate nohighlight">\(Y = 0\)</span> if the event does not occur, and <span class="math notranslate nohighlight">\(Y = 1\)</span> if it does),</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1,\ldots,X_k\)</span> are non-random qualitative or quantitative variables (<span class="math notranslate nohighlight">\(K\)</span> explanatory variables in total).</p></li>
<li><p><span class="math notranslate nohighlight">\((Y, X_1,\ldots,X_k)\)</span> represent the population variables, from which a sample of <span class="math notranslate nohighlight">\(n\)</span> individuals <span class="math notranslate nohighlight">\((i)\)</span> is drawn, and <span class="math notranslate nohighlight">\((y, x_i)\)</span> is the vector of observed realizations of <span class="math notranslate nohighlight">\((Y_i, X_i)\)</span> for each individual in the sample.</p></li>
</ul>
<p>Unlike simple linear regression, logistic regression estimates <strong>the probability</strong> of an event occurring, rather than predicting a specific numerical value.</p>
</section>
<section id="The-model">
<h2>The model<a class="headerlink" href="#The-model" title="Permalink to this heading"></a></h2>
<p>The variable <span class="math notranslate nohighlight">\(Y_i\)</span> follow a Bernoulli distribution with parameter <span class="math notranslate nohighlight">\(p_i\)</span> representing the probability that <span class="math notranslate nohighlight">\(Y_i=1\)</span>.</p>
<div class="math notranslate nohighlight">
\[Y_i \sim B(p_i)\]</div>
<div class="math notranslate nohighlight">
\[P(Y_i=1) = p_i \quad, \quad P(Y_i = 0) = 1 - p_i\]</div>
<p>which is equivalent to:</p>
<div class="math notranslate nohighlight">
\[P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \quad \text{for k} \in \{0, 1\}\]</div>
</section>
<section id="The-linear-LOGIT-model">
<h2>The linear LOGIT model<a class="headerlink" href="#The-linear-LOGIT-model" title="Permalink to this heading"></a></h2>
<p>To ensure that the expected value of <span class="math notranslate nohighlight">\(Y, E(Y)\)</span>, only takes values between 0 and 1, we use the logistic function:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{\text{exp(x)}}{1 + \text{exp(x)}} = p\]</div>
<p>or similarly:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{1 + \text{exp(-x)}} = p\]</div>
<p>This guarantees that <span class="math notranslate nohighlight">\(0 &lt; f(x) &lt; 1\)</span>, so <span class="math notranslate nohighlight">\(E[Y]\)</span> can represent a valid probability.</p>
<p>The logit function is used to transform a probability <span class="math notranslate nohighlight">\(p\)</span> into an <strong>unrestricted real value</strong>:</p>
<p><span class="math notranslate nohighlight">\(\quad \text{Notations:} \quad X = (1,X_1, \ldots, X_k) \quad \text{and} \quad \beta = (\beta_0,\beta_1, \ldots, \beta_k)\)</span></p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \text{log}(\dfrac{p}{1 - p})\]</div>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \beta .X\]</div>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]</div>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{p}{1-p} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]</div>
<div class="math notranslate nohighlight">
\[p = \frac{1}{1 + \exp(-\beta .X)}\]</div>
<p>Demonstration:</p>
<div class="math notranslate nohighlight">
\[p(x) = \dfrac{1}{1 + \exp(-\beta x)}\]</div>
<div class="math notranslate nohighlight">
\[\underset{inverse}   \iff \dfrac{1}{p} = 1 + \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1}{p} - 1 = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1}{p} - \dfrac{p}{p} = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1-p}{p} = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \log(\dfrac{1-p}{p}) = -\beta x\]</div>
<div class="math notranslate nohighlight">
\[\iff \log(\dfrac{p}{1-p}) = \beta x\]</div>
<p>To simplify the writing we have put <span class="math notranslate nohighlight">\(p\)</span> rather than <span class="math notranslate nohighlight">\(p(x)\)</span></p>
</section>
<section id="Key-Assumptions-for-Generalizability-of-the-logit-model">
<h2>Key Assumptions for Generalizability of the logit model<a class="headerlink" href="#Key-Assumptions-for-Generalizability-of-the-logit-model" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Linearity of Log-Odds:</strong> The relationship between each continuous predictor and the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span> is linear. If this assumption is violated (e.g., non-linear effects), the interpretation of <span class="math notranslate nohighlight">\(\beta_1\)</span>​ may not hold.</p></li>
<li><p><strong>No Multicollinearity:</strong> Predictors should not be highly correlated, as this can distort the interpretation of individual coefficients.</p></li>
<li><p><strong>Additivity:</strong> The effect of each predictor on the log-odds is additive. There should be no significant interaction effects unless explicitly modeled.</p></li>
<li><p><strong>Independence of Observations:</strong> The model assumes that observations are independent of each other.</p></li>
</ul>
</section>
<section id="Coefficients-interpretation">
<h2>Coefficients interpretation<a class="headerlink" href="#Coefficients-interpretation" title="Permalink to this heading"></a></h2>
<p><strong>The Odds</strong></p>
<p>The odds are defined by:</p>
<div class="math notranslate nohighlight">
\[\text{Odds} = \dfrac{p}{1-p}\]</div>
<p><span class="math notranslate nohighlight">\(\text{Where} \quad p = P(target=1|X)\)</span></p>
<blockquote>
<div><p><em>If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are ‘3 to 1’:</em> <span class="math notranslate nohighlight">\(\text{Odds} = \dfrac{3/4}{1/4}=3\)</span></p>
</div></blockquote>
<ul>
<li><p><strong>Notation:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=0)=\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}\]</div>
</li>
</ul>
<section id="The-Odds-Ratio">
<h3><strong>The Odds Ratio</strong><a class="headerlink" href="#The-Odds-Ratio" title="Permalink to this heading"></a></h3>
<p>The odds ratio comparing the <strong>probability of :math:`target=1`</strong> between individuals with value <span class="math notranslate nohighlight">\(X\)</span> and those without it.</p>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{\text{Odds}(Y=1|X=1)}{\text{Odds}(Y=1|X=0)}\]</div>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}\]</div>
<p>We know that logit is given by:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \text{log}(\dfrac{p}{1-p}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_k x_{ik}\]</div>
</section>
<section id="Interpreting-the-Intercept">
<h3><strong>Interpreting the Intercept</strong><a class="headerlink" href="#Interpreting-the-Intercept" title="Permalink to this heading"></a></h3>
<div class="line-block">
<div class="line">The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>​ represents the <strong>log-odds of the outcome :math:`Y=1` when all predictors are equal to zero</strong>.</div>
<div class="line"><span class="math notranslate nohighlight">\(\beta_0\)</span>​ defines the <strong>baseline probability</strong> of the outcome when all predictors are zero.</div>
</div>
<div class="line-block">
<div class="line">⚠️ <strong>Caveat</strong>:</div>
<div class="line">This interpretation of <span class="math notranslate nohighlight">\(\beta_0\)</span> is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, <span class="math notranslate nohighlight">\(\beta_0\)</span>​ is primarily a mathematical component of the model and is rarely interpreted in isolation.</div>
</div>
<ul class="simple">
<li><p><strong>Odds for the baseline group:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1∣X_1=0)=\exp⁡(\beta_0)\]</div>
<ul>
<li><p><strong>Probability for the baseline group:</strong></p>
<div class="math notranslate nohighlight">
\[P(Y=1∣X_1=0)=\dfrac{\exp⁡(\beta_0)}{1 + \exp(\beta_0)}\]</div>
</li>
</ul>
<blockquote>
<div><p>If <span class="math notranslate nohighlight">\(X_1\)</span>​ is “smoking status” (<span class="math notranslate nohighlight">\(0\)</span> = non-smoker, <span class="math notranslate nohighlight">\(1\)</span> = smoker), then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span>​ gives the <strong>log-odds</strong> of the outcome for non-smokers</p></li>
<li><p><span class="math notranslate nohighlight">\(\exp(\beta_0)\)</span> gives the <strong>odds</strong> of the outcome for non-smokers.</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(\beta_0 = -1\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\exp(\beta_0) = \exp(-1) \approx 0.37\]</div>
<div class="math notranslate nohighlight">
\[P(Y=1∣X_1=0)= \dfrac{0.37}{1 + 0.37} \approx 0.27\]</div>
<div class="line-block">
<div class="line">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.</div>
<div class="line">It is the observed proportion of lung cancer for non-smokers.</div>
</div>
</div></blockquote>
</section>
<section id="Interpreting-the-Slope">
<h3><strong>Interpreting the Slope</strong><a class="headerlink" href="#Interpreting-the-Slope" title="Permalink to this heading"></a></h3>
<div class="line-block">
<div class="line">In a model with multiple predictors, each <span class="math notranslate nohighlight">\(\beta_i\)</span>​ (and its corresponding odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_i)\)</span> represents the effect of that predictor on the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span>, holding all other predictors constant.</div>
<div class="line">This is the key assumption of multivariable regression: ceteris paribus (all else being equal).</div>
</div>
<p>The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>​ represents the <strong>change in the log-odds of</strong> <span class="math notranslate nohighlight">\(Y=1\)</span> for a <strong>one-unit change</strong> in <span class="math notranslate nohighlight">\(X_1\)</span>​. The odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ quantifies how the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> change with <span class="math notranslate nohighlight">\(X_1\)</span>​.</p>
<p><strong>General Formula for Odds Ratio</strong></p>
<div class="line-block">
<div class="line">For any type of predictor <span class="math notranslate nohighlight">\(X_1\)</span>, the odds ratio for a one-unit increase is:</div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \frac{\text{Odds}(Y=1 | X_1 = x+1)}{\text{Odds}(Y=1 | X_1 = x)} = \exp(\beta_1)\]</div>
</div></blockquote>
<div class="line-block">
<div class="line">📌 <strong>Note:</strong></div>
<div class="line"><span class="math notranslate nohighlight">\(\exp(\beta_1​)\)</span> compares the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_1=0\)</span>, controlling for all other variables in the model (all others features constant).</div>
</div>
<ul class="simple">
<li><p>Case: <span class="math notranslate nohighlight">\(X_1\)</span> is Binary</p></li>
</ul>
<p>For a binary predictor <span class="math notranslate nohighlight">\(X_1\)</span>​ (e.g., <span class="math notranslate nohighlight">\(0\)</span> = non-smoker, <span class="math notranslate nohighlight">\(1\)</span> = smoker), the odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ compares the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between the two groups.</p>
<ul class="simple">
<li><p><strong>Logistic regression equation:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\log\left(\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\right) = \beta_0 + \beta_1 1_{\{X_1 = 1\}}​\]</div>
<ul>
<li><p><strong>Odds ratio:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \exp(\beta_1)\]</div>
</li>
</ul>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1) = 1\)</span>: No effect of the feature <span class="math notranslate nohighlight">\(X_1\)</span>​ on the odds of <span class="math notranslate nohighlight">\(Y=1\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1)&gt;1\)</span>: The odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are higher when <span class="math notranslate nohighlight">\(X_1​=1\)</span>. The feature <span class="math notranslate nohighlight">\(X_1\)</span>​ is <strong>positively associated</strong> with the outcome.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1) &lt; 1\)</span>: The odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are lower when <span class="math notranslate nohighlight">\(X_1​=1\)</span>. The feature <span class="math notranslate nohighlight">\(X_1\)</span>​ is <strong>negatively associated</strong> with the outcome.</p></li>
</ul>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>Example:</strong></div>
<div class="line">If <span class="math notranslate nohighlight">\(\beta_1 = 0.7 \rightarrow \exp(\beta_1) \approx 2.01\)</span>. The odds of lung cancer for smokers <span class="math notranslate nohighlight">\((X_1=1)\)</span> are twice as high as for non-smokers <span class="math notranslate nohighlight">\((X_1=0)\)</span>.</div>
</div>
</div></blockquote>
<ul class="simple">
<li><p>Case: <span class="math notranslate nohighlight">\(X_1\)</span> is Categorical</p></li>
</ul>
<p>For a categorical predictor <span class="math notranslate nohighlight">\(X_1\)</span> with more than two levels (e.g., color = red, green, blue), you use <strong>dummy variables</strong>.</p>
<ul class="simple">
<li><p><strong>The logistic regression model becomes:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{green}1_{\{X_1 = \text{green}\}} + \beta_{blue}1_{\{X_1 = \text{blue}\}}\]</div>
<ul class="simple">
<li><p><strong>Reference Category (“red”):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=0\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=0\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0\]</div>
<p><strong>Interpretation:</strong> This means <span class="math notranslate nohighlight">\(\beta_0\)</span>​ represents the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for the reference category (“red”).</p>
<ul class="simple">
<li><p><strong>Category (“green”):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=1\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=0\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{green}\]</div>
<ul class="simple">
<li><p><strong>Category (“blue”):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=0\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=1\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{blue}\]</div>
<p>The <strong>odds ratio for “blue” relative to the reference “red”</strong> is:</p>
<div class="math notranslate nohighlight">
\[\exp(\beta_{blue}) = \dfrac{\text{Odds}(Y=1 | \text{blue})}{\text{Odds}(Y=1 | \text{red})}\]</div>
<p>The same way, <span class="math notranslate nohighlight">\(\exp(\beta_{\text{green}})\)</span>​ compares the odds for “green” vs. the “red” reference.</p>
<blockquote>
<div><p><strong>Interpretation:</strong></p>
<p>If <span class="math notranslate nohighlight">\(\exp(\beta_{\text{green}})​=1.5\)</span>, the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are <span class="math notranslate nohighlight">\(1.5\)</span> times higher for “green” compared to “red”.</p>
</div></blockquote>
<ul>
<li><p class="rubric" id="case-x-1-is-quantitative">Case: <span class="math notranslate nohighlight">\(X_1\)</span> is Quantitative</p>
</li>
</ul>
<p>For a continuous predictor <span class="math notranslate nohighlight">\(X_1\)</span> (e.g., age, blood pressure), the odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ represents the multiplicative change in the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>​.</p>
<ul class="simple">
<li><p><strong>Logistic regression equation:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\log\left(\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\right) = \beta_0 + \beta_1 X_1​\]</div>
<ul class="simple">
<li><p><strong>Odds ratio for a one-unit increase:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \frac{\text{Odds}(Y=1 | X_1 = x+1)}{\text{Odds}(Y=1 | X_1 = x)} = \exp(\beta_1)​\]</div>
<p><strong>In short</strong>: <span class="math notranslate nohighlight">\(\beta_1\)</span>​ captures the <strong>constant log-odds</strong> change per unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>, so <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>​ is the <strong>odds ratio</strong> for that one-unit change.</p>
<p>This holds regardless of the starting value of <span class="math notranslate nohighlight">\(X_1\)</span>​ because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression).</p>
<blockquote>
<div><p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\beta_1=0.095 \rightarrow \exp(\beta_1)=1.1\)</span>, the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> increase by 10% for each one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>​.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_1\)</span>​ is “years of smoking” and <span class="math notranslate nohighlight">\(\beta_1 = 0.7 \rightarrow  \exp(\beta_1) \approx 2.01\)</span>. For each additional year of smoking, the odds of lung cancer double. .</p></li>
</ul>
</div></blockquote>
<p><strong>Summary</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Type of <span class="math notranslate nohighlight">\(X_1\)</span></p></th>
<th class="head"><p>Interpretation of <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary</p></td>
<td><p>Compares odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_1=0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Categorical</p></td>
<td><p>Compares odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a given category relative to the reference.</p></td>
</tr>
<tr class="row-even"><td><p>Quantitative</p></td>
<td><p>Multiplicative change in odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span></p></td>
</tr>
</tbody>
</table>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>

<span class="n">rep</span> <span class="o">=</span> <span class="s1">&#39;/Users/davidtbo/Library/Mobile Documents/com~apple~CloudDocs/data/external&#39;</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rep</span><span class="p">,</span> <span class="s1">&#39;diabetes.csv&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="nb">str</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">df</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">str</span><span class="o">.</span><span class="n">strip</span><span class="p">()]</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>pregnancies</th>
      <th>glucose</th>
      <th>bloodpressure</th>
      <th>skinthickness</th>
      <th>insulin</th>
      <th>bmi</th>
      <th>diabetespedigreefunction</th>
      <th>age</th>
      <th>outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data preparation</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;outcome&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># separate the features of the target</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;outcome&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;outcome&#39;</span><span class="p">]</span>

<span class="c1"># Data standardization</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>
<span class="n">standardized_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># Reassign the names of the columns of origin</span>
<span class="n">standardized_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">standardized_data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># Display the head of the standardized DataFrame</span>
<span class="nb">print</span><span class="p">(</span><span class="n">standardized_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span><span class="o">.</span><span class="n">to_string</span><span class="p">())</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
   pregnancies   glucose  bloodpressure  skinthickness   insulin       bmi  diabetespedigreefunction       age
0     0.639947  0.848324       0.149641       0.907270 -0.692891  0.204013                  0.468492  1.425995
1    -0.844885 -1.123396      -0.160546       0.530902 -0.692891 -0.684422                 -0.365061 -0.190672
2     1.233880  1.943724      -0.263941      -1.288212 -0.692891 -1.103255                  0.604397 -0.105584
3    -0.844885 -0.998208      -0.160546       0.154533  0.123302 -0.494043                 -0.920763 -1.041549
4    -1.141852  0.504055      -1.504687       0.907270  0.765836  1.409746                  5.484909 -0.020496
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># train_test_split</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Logistic-Regression-with-scipy">
<h3>Logistic Regression with scipy<a class="headerlink" href="#Logistic-Regression-with-scipy" title="Permalink to this heading"></a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Add a constant for statsmodels</span>
<span class="n">X_train_sm</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># Logistic Regression with L2 regularization</span>
<span class="n">logit_model</span> <span class="o">=</span> <span class="n">sm</span><span class="o">.</span><span class="n">Logit</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">X_train_sm</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">logit_model</span><span class="o">.</span><span class="n">fit_regularized</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Display the summary</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Optimization terminated successfully    (Exit mode 0)
            Current function value: 0.46842232325133
            Iterations: 79
            Function evaluations: 95
            Gradient evaluations: 79
                           Logit Regression Results
==============================================================================
Dep. Variable:                outcome   No. Observations:                  614
Model:                          Logit   Df Residuals:                      605
Method:                           MLE   Df Model:                            8
Date:                Sat, 18 Oct 2025   Pseudo R-squ.:                  0.2875
Time:                        11:14:02   Log-Likelihood:                -286.64
converged:                       True   LL-Null:                       -402.31
Covariance Type:            nonrobust   LLR p-value:                 1.532e-45
============================================================================================
                               coef    std err          z      P&gt;|z|      [0.025      0.975]
--------------------------------------------------------------------------------------------
const                       -8.4317      0.802    -10.513      0.000     -10.004      -6.860
pregnancies                  0.1604      0.037      4.321      0.000       0.088       0.233
glucose                      0.0375      0.004      8.812      0.000       0.029       0.046
bloodpressure               -0.0137      0.006     -2.338      0.019      -0.025      -0.002
skinthickness                0.0038      0.008      0.499      0.618      -0.011       0.019
insulin                     -0.0014      0.001     -1.421      0.155      -0.003       0.001
bmi                          0.0858      0.017      5.054      0.000       0.053       0.119
diabetespedigreefunction     1.0054      0.326      3.087      0.002       0.367       1.644
age                          0.0063      0.011      0.575      0.565      -0.015       0.028
============================================================================================
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">odds_ratios</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">odds_ratios</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
const                       0.000218
pregnancies                 1.174037
glucose                     1.038217
bloodpressure               0.986439
skinthickness               1.003852
insulin                     0.998607
bmi                         1.089609
diabetespedigreefunction    2.732962
age                         1.006341
dtype: float64
</pre></div></div>
</div>
<p><strong>Interpreting the coefficients:</strong></p>
<p>For each feature, the exponentiated coefficient (exp(coef)) represents the change in odds for a one-unit increase in that feature, holding all other features constant.</p>
<div class="line-block">
<div class="line">For example, has the coefficient for ‘pregnancies’ is 0.1604, the odds ratio is exp(0.1604) ≈ 1.174037.</div>
<div class="line">This means that for each one-unit increase in pregnancies, the odds of the outcome occurring (e.g., having diabetes) increase by approximately 17.4%, assuming all other features remain constant.</div>
</div>
<p><strong>Diabetes Prediction: data preparation &amp; model inference</strong></p>
<ul class="simple">
<li><p>Preparing new input data for prediction</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># check the number of features in the model</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Number of features in the model: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">params</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of features in the model: 9
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predictive system</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mf">25.8</span><span class="p">,</span> <span class="mf">0.587</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>

<span class="c1"># to numpy array</span>
<span class="n">input_data_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Reshape</span>
<span class="n">input_data_reshape</span> <span class="o">=</span> <span class="n">input_data_array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of input data:&quot;</span><span class="p">,</span> <span class="n">input_data_reshape</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Standardized the data</span>
<span class="n">input_data_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_data_reshape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of standardized input data:&quot;</span><span class="p">,</span> <span class="n">input_data_std</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Shape of input data: (1, 8)
Shape of standardized input data: (1, 8)
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Add a column of 1s for the intercept</span>
<span class="n">input_data_with_const</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">input_data_std</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Shape of input data with constant:&quot;</span><span class="p">,</span> <span class="n">input_data_with_const</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Shape of input data with constant: (1, 9)
</pre></div></div>
</div>
<ul class="simple">
<li><p>Using the model to make a prediction</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make prediction</span>
<span class="n">pred</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data_with_const</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Predicted probability of diabetes: </span><span class="si">{</span><span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Predicted probability of diabetes: 0.00032363072678058075
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># For binary classification, use a threshold (e.g., 0.5)</span>
<span class="k">if</span> <span class="n">pred</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The person is diabetic&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The person is not diabetic&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The person is not diabetic
</pre></div></div>
</div>
</section>
</section>
</section>
<section id="APPENDIX">
<h1>APPENDIX<a class="headerlink" href="#APPENDIX" title="Permalink to this heading"></a></h1>
<p><strong>Demonstration:</strong> The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>​ represents the <strong>change in the log-odds of</strong> <span class="math notranslate nohighlight">\(Y=1\)</span> for a <strong>one-unit change</strong> in <span class="math notranslate nohighlight">\(X_1\)</span>​ quantitative feature.</p>
<div class="line-block">
<div class="line">Notations:</div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))\]</div>
</div></blockquote>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))\]</div>
<p>We know that:</p>
<div class="math notranslate nohighlight">
\[\text{log(Odds)}(Y=1|X=x+1)=\beta_0 + \beta_1 \times (x+1)\]</div>
<div class="math notranslate nohighlight">
\[\text{log(Odds)}(Y=1|X=x)=\beta_0 + \beta_1 \times x\]</div>
<p>By difference:</p>
<div class="math notranslate nohighlight">
\[\text{log(Odds)}(Y=1|X=x+1) - \text{log(Odds)}(Y=1|X=x) =\beta_0 + \beta_1 \times (x+1) - (\beta_0 + \beta_1 \times x) = \beta_1\]</div>
<div class="math notranslate nohighlight">
\[\text{log}\left(\dfrac{\text{Odds}(Y=1|X=x+1)}{\text{Odds}(Y=1|X=x)}\right) =\beta_1\]</div>
<p><strong>CQFD</strong></p>
<p>Note:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\text{Odds}(Y=1|X=x+1)}{\text{Odds}(Y=1|X=x)} = \exp(\beta_1)\]</div>
<p>The prediction <span class="math notranslate nohighlight">\(y_i=1\)</span> of the logistic regression is defined:</p>
<div class="math notranslate nohighlight">
\[\hat{y_i} = P(y_i=1 | x_i; \theta) = \frac{1}{1 + \exp(-\theta^Tx_i)} = h_{\theta}(x_i)\]</div>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(y_i=1\)</span>, then <span class="math notranslate nohighlight">\(P(y_i|x_i; \theta)=P(y_i=1|x_i; \theta)\)</span></p></li>
<li><p>If <span class="math notranslate nohighlight">\(y_i=0\)</span>, then <span class="math notranslate nohighlight">\(P(y_i|x_i; \theta)=P(y_i=0|x_i; \theta) = 1 - P(y_i=1|x_i; \theta)\)</span></p></li>
</ul>
<p>We can write these two equations into a single one:</p>
<div class="math notranslate nohighlight">
\[P(y_i|x_i; \theta)=P(y_i=1|x_i; \theta)^{y_i}\times (1 - P(y_i=1|x_i; \theta))^{1-y_i}\]</div>
<p>With the notations:</p>
<div class="math notranslate nohighlight">
\[P(y_i|x_i; \theta)=h_{\theta}(x_i)^{y_i}\times (1 - h_{\theta}(x_i))^{1-y_i}\]</div>
<p>The <strong>likelihood</strong> of the observations <span class="math notranslate nohighlight">\(y_i\)</span> given the inputs <span class="math notranslate nohighlight">\(x_i\)</span> and parameters <span class="math notranslate nohighlight">\(\theta\)</span> is defined as:</p>
<div class="math notranslate nohighlight">
\[L(\theta) = \prod_{i=1}^n P(y_i|x_i;\theta) = \prod_{i=1}^n (h_{\theta}(x_i))^{y_i} (1 -h_{\theta}(x_i))^{1-y_i}\]</div>
<p>where the prediction of the logistic regression is defined:</p>
<div class="math notranslate nohighlight">
\[h_{\theta}(x_i) = P(y_i=1 | x_i; \theta) = \frac{1}{1 + \exp(-\theta^Tx_i)}\]</div>
<p>The <strong>log-likelihood</strong> is defined as:</p>
<div class="math notranslate nohighlight">
\[l(\theta) = \log(L(\theta))=\sum_{i=1}^n[y_i \log (h_{\theta}(x_i)) + (1 - y_i) \log(1 - h_{\theta}(x_i))]\]</div>
<p>The goal of learning a <strong>logistic regression model</strong> is to <strong>minimize the cost function</strong> by adjusting the parameters <span class="math notranslate nohighlight">\(\theta\)</span>.The cost function measures the average prediction error across all <span class="math notranslate nohighlight">\(n\)</span> training samples.</p>
<p>The cost function <span class="math notranslate nohighlight">\(J(\theta)\)</span> is defined as the average penalty for prediction errors across the training set. Mathematically, it is expressed as:</p>
<div class="math notranslate nohighlight">
\[J(θ) = -\frac{1}{n} \sum cost(h_\theta(x_i), y_i)\]</div>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(h_\theta(x_i)\)</span> the model’s prediction for sample <span class="math notranslate nohighlight">\(x_i\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> the observed (true) value.</p></li>
</ul>
<p>Alternatively, it can be written as:</p>
<div class="math notranslate nohighlight">
\[J(θ) = \frac{1}{n} \sum cost(\hat{y_i}, y_i)\]</div>
<p>Where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\hat{y_i} = h_{\theta}(x_i)\)</span></p></li>
</ul>
<div class="line-block">
<div class="line">For logistic regression, the cost function is called <strong>log loss</strong> (or logistic loss).</div>
<div class="line"><strong>Log loss</strong> is derived from the log-likelihood <span class="math notranslate nohighlight">\(l(\theta)\)</span> and is defined as:</div>
</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\dfrac{1}{n}l(\theta) = -\frac{1}{n} \sum(y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i)))\]</div>
<p>The log-loss function penalizes prediction errors based on the estimated probability from the model. It assigns higher penalties when predictions are far from the true labels—specifically:</p>
<ul class="simple">
<li><p>If the label <span class="math notranslate nohighlight">\(y_i = 1\)</span> (positive class), the penalty is <span class="math notranslate nohighlight">\(-log(h_{\theta}(x_i))\)</span>. The closer <span class="math notranslate nohighlight">\(h_{\theta}(x_i)\)</span> is to 0 (far from the true label), the higher the penalty (<span class="math notranslate nohighlight">\(-\log(0^+) \approx + \infty\)</span>).</p></li>
<li><p>If the label <span class="math notranslate nohighlight">\(y_i = 0\)</span> (negative class), the penalty is <span class="math notranslate nohighlight">\(-log(1-h_{\theta}(x_i))\)</span>, the closer <span class="math notranslate nohighlight">\(h_{\theta}(x_i)\)</span> is to 1 (far from the true label), the higher the penalty (<span class="math notranslate nohighlight">\(-\log(1 -1^+) \approx + \infty\)</span>).</p></li>
</ul>
<p>The two cases are combined into a single formula for observation <span class="math notranslate nohighlight">\(i\)</span>:</p>
<div class="math notranslate nohighlight">
\[y_i log(h_\theta(x_i)) + (1 - y_i) log(1 - h_\theta(x_i))\]</div>
<p><strong>Key insights:</strong></p>
<ul class="simple">
<li><p><strong>Log loss</strong> evaluates how well the model fits the training data.</p></li>
<li><p>The log loss is the negative average the log likelihood.</p></li>
<li><p><strong>Higher likelihood</strong> leads to <strong>lower log loss</strong> (since <span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{n}l(\theta)\)</span>).</p></li>
<li><p>The log-loss function heavily penalizes confident wrong predictions.</p></li>
</ul>
<p>By minimizing the cost function <span class="math notranslate nohighlight">\(J(\theta)\)</span>, we aim to find the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that maximize the likelihood of observing the training data given the model parameters.</p>
<p>To achieve this, we use an iterative optimization method, <strong>gradient descent</strong>, to find the values of <span class="math notranslate nohighlight">\(\theta\)</span> that minimize the cost function over the training set:</p>
<div class="math notranslate nohighlight">
\[\underset{\theta}{minimize}(J(\theta))\]</div>
<p>The gradient descent only uses the <strong>gradient</strong> (first derivative) of the cost function to update the parameters <span class="math notranslate nohighlight">\(\theta\)</span> in the opposite direction of the gradient, scaled by a learning rate <span class="math notranslate nohighlight">\(\alpha\)</span>.</p>
<p>To compute the gradient <span class="math notranslate nohighlight">\(\nabla J(\theta)\)</span> we start by transforming the expression of:</p>
<div class="math notranslate nohighlight">
\[\log(h_\theta(x_i)) = \log\left(\frac{1}{1 + \exp(-\theta^Tx_i)}\right) = -\log(1 + \exp(-\theta^Tx_i))\]</div>
<p>And:</p>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log\left(1 - \frac{1}{1 + \exp(-\theta^Tx_i)}\right)\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log\left(\frac{1 + \exp(-\theta^Tx_i) - 1}{1 + \exp(-\theta^Tx_i)}\right)\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log\left(\frac{\exp(-\theta^Tx_i)}{1 + \exp(-\theta^Tx_i)}\right)\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = \log(\exp(-\theta^Tx_i)) - \log({1 + \exp(-\theta^Tx_i)})\]</div>
<div class="math notranslate nohighlight">
\[\log(1 - h_\theta(x_i)) = -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)})\]</div>
<p>We integrate these modifications:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i (-\log(1 + \exp(-\theta^Tx_i))) + (1 - y_i) (-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i (-\log(1 + \exp(-\theta^Tx_i))) + (1 - y_i) (-\theta^Tx_i - \log(1 + exp(-\theta^Tx_i)))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i (-\log(1 + \exp(-\theta^Tx_i))) -\theta^Tx_i - \log(1 + \exp(-\theta^Tx_i)) + y_i \theta^Tx_i  + y_i \log(1 + \exp(-\theta^Tx_i))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[- y_i \log(1 + \exp(-\theta^Tx_i)) -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) + y_i \theta^Tx_i  + y_i \log(1 + \exp(-\theta^Tx_i))]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[\cancel{- y_i \log(1 + \exp(-\theta^Tx_i))} -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) + y_i \theta^Tx_i  + \cancel{y_i \log(1 + \exp(-\theta^Tx_i))}]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) + y_i \theta^Tx_i  ]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i \theta^Tx_i  -\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) ]\]</div>
<p>with:</p>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = - \log(\exp(\theta^T x_i)) - \log(1 + \exp(-\theta^Tx_i))\]</div>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = -(\log(\exp(\theta^T x_i)) + \log(1 + \exp(-\theta^Tx_i)))\]</div>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = -\log[\exp(\theta^T x_i)(1 + \exp(-\theta^Tx_i))]\]</div>
<div class="math notranslate nohighlight">
\[-\theta^Tx_i - \log({1 + \exp(-\theta^Tx_i)}) = -\log(\exp(\theta^T x_i) + 1)\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i \theta^Tx_i  -\log(\exp(\theta^T x_i + 1)) ]\]</div>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n} \sum[y_i \theta^Tx_i  -\log(1 + \exp(\theta^T x_i)) ]\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j}J(\theta) = -\frac{1}{n} \sum[y_i \frac{\partial}{\partial \theta_j} (\theta^Tx_i)  - \frac{\partial}{\partial \theta_j}\log(1 + \exp(\theta^T x_i)) ]\]</div>
<p>Knowing that:</p>
<div class="math notranslate nohighlight">
\[\theta^Tx_i = \theta_1 {x_i}^{(1)} + \theta_2 {x_i}^{(2)} + \ldots + \theta_k {x_i}^{(k)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j} (\theta^Tx_i) = x_i^{(j)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial \theta_j}\left(\log(1 + \exp(\theta^T x_i))\right) \underset{\log(u)^{'} = \dfrac{u^{'}}{u}} = \dfrac{\dfrac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<p>And:</p>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial \theta_j}\left(\log(1 + \exp(\theta^T x_i))\right) \underset{\log(u)^{'} = \dfrac{u^{'}}{u}} = \dfrac{\dfrac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\dfrac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))} {1 + \exp(\theta x_i)} = \dfrac{\dfrac{\partial}{\partial \theta_j}(\exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\dfrac{\partial}{\partial \theta_j}(\exp(\theta^T x_i))} {1 + \exp(\theta x_i)} \underset{\exp(u)^{'} = u^{'}\exp(u)} =  \dfrac{\dfrac{\partial}{\partial \theta_j}(\theta^T x_i) * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\dfrac{\partial}{\partial \theta_j}(\theta^T x_i) * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)} = \frac{x_i^{(j)} * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)}\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{x_i^{(j)} * (\exp(\theta^T x_i))} {1 + \exp(\theta x_i)} = x_i^{(j)} * h_\theta(x_i)\]</div>
<div class="math notranslate nohighlight">
\[\dfrac{\partial}{\partial \theta_j}J(\theta) = -\dfrac{1}{n} \sum[y_i x_i^{(j)}  - x_i^{(j)} h_\theta(x_i)]\]</div>
<div class="math notranslate nohighlight">
\[-\dfrac{1}{n} \sum[y_i x_i^{(j)}  - x_i^{(j)} h_\theta(x_i)] = -\dfrac{1}{n} \sum[y_i - h_\theta(x_i) ] x_i^{(j)}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial}{\partial \theta_j}J(\theta) = \frac{1}{n} \sum[h_\theta(x_i) - y_i ] x_i^{(j)}\]</div>
<p>We know that the expression of the Gradient descent to update the weights is for the weight <span class="math notranslate nohighlight">\(\theta_j\)</span> :</p>
<div class="math notranslate nohighlight">
\[\theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)\]</div>
<div class="math notranslate nohighlight">
\[\theta_j = \theta_j - \frac{\alpha}{n} \sum[h_\theta(x_i) - y_i ] x_i^{(j)}\]</div>
<p>Others algorithms can be used to find the coefficients of the logistic regression. The Newton-Raphson algorithm is an alternative to the gradient descent.</p>
<p>The Newton-Raphson method uses both the first derivative (gradient) and the second derivative (Hessian) of the cost function. It approximates the cost function as a quadratic form and finds the minimum of this approximation.</p>
<p><strong>Update rule</strong></p>
<div class="math notranslate nohighlight">
\[\theta = \theta - H^{-1} \nabla J(\theta)\]</div>
<p>Logistic regression is a regression model used to predict the probability of a binary event based on one or more predictive variables. In logistic regression, the likelihood function is convex and can be maximized using the Newton-Raphson algorithm.</p>
<p>The Newton-Raphson algorithm is an iterative method for finding the maximum of a function using its first and second derivatives. For logistic regression, the likelihood function is given by:</p>
<p><span class="math notranslate nohighlight">\(L(\theta | X, y) = \prod(P(yi | x_i, \theta)^{yi} (1 - P(y_i | x_i, \theta))^{(1 - y_i)})\)</span></p>
<p>where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\theta\)</span> is the vector of logistic regression coefficients,</p></li>
<li><p><span class="math notranslate nohighlight">\(X\)</span> is the matrix of predictive variables,</p></li>
<li><p><span class="math notranslate nohighlight">\(y\)</span> is the vector of binary response variables, and</p></li>
<li><p><span class="math notranslate nohighlight">\(P(y_i | x_i, \theta)\)</span> is the predicted probability of the binary event for observation <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>To maximize the likelihood function, the Newton-Raphson algorithm updates the coefficient vector <span class="math notranslate nohighlight">\(\theta\)</span> at each iteration using the following formula:</p>
<p><span class="math notranslate nohighlight">\(\theta_{i+1} = \theta_i - H^{-1} . g\)</span></p>
<p>where</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H = \dfrac{\partial^2L}{\partial \theta \partial\theta'}\)</span> is the Hessian matrix of the likelihood function.</p></li>
<li><p><span class="math notranslate nohighlight">\(g = \dfrac{\partial L}{\partial \theta }\)</span> is the gradient vector of the likelihood function, and,</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> is the coefficient vector at iteration <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>The Hessian matrix and gradient vector of the likelihood function are computed using the partial derivatives of the likelihood with respect to the coefficients <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>Thus, in logistic regression, the Newton-Raphson algorithm is used to estimate the coefficients by maximizing the likelihood function. This allows the prediction of binary event probabilities based on the predictive variables.</p>
<p><strong>Note</strong>: The iterations stop when the difference between two successive solution vectors becomes negligible.</p>
<p>Convexity is a crucial property in optimization, as it ensures that any local minimum is also a global minimum. This makes it easier to find the optimal solution using methods like gradient descent.</p>
<p>The Log Loss function is convex due to its logarithmic form, which is always convex for positive values. While convexity guarantees that a stationary point (where the derivative is zero) is a global minimum, it does not guarantee the existence of such a point.</p>
<p>To prove the existence of a minimum, the function must also be bounded below and attain this lower bound. For Log Loss, the function is bounded below by zero (since the logarithm of a positive number is always defined) and reaches this bound when predictions are perfectly accurate (i.e., the predicted probability for the correct class is 1).</p>
<p>Combining convexity with the fact that Log Loss is bounded below and attains its lower bound, we conclude that the function reaches a global minimum when predictions are perfectly correct.</p>
<p>Another way to compute the coefficients</p>
<p>Note: <span class="math notranslate nohighlight">\(\theta = (w,b)\)</span></p>
<p>with <span class="math notranslate nohighlight">\(h_\theta(x) = \frac{1}{1 + \exp(-w x + b)}\)</span></p>
<ul class="simple">
<li><p>Initialize weights as zero</p></li>
<li><p>Initialize bias as zero</p></li>
</ul>
<ul class="simple">
<li><p>Predict result by using <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{1 + \exp(-wx+b)}\)</span></p></li>
<li><p>Calculate the error</p></li>
<li><p>Use Gradient descent to figure out new weights and bias values</p></li>
<li><p>Repeat n times</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Logistic_Regression</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Initiate the constructor</span>
<span class="sd">            INPUT:</span>
<span class="sd">                learning_rate: magnitude of the step</span>
<span class="sd">                n_iter: number of iterations</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Train the model</span>
<span class="sd">        INPUTS:</span>
<span class="sd">            X: the dataset of the features</span>
<span class="sd">            y: the target</span>
<span class="sd">        OUTPUTS:</span>
<span class="sd">            The model</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># initialize the parameters:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Update of the weights with Gradient descent&#39;&#39;&#39;</span>

        <span class="c1"># we compute the prediction (the probability)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)))</span>

        <span class="c1"># update the weights:</span>
        <span class="c1"># w_j = w_j - (alpha / n) * S(p_hat - y_i)xij</span>
        <span class="c1"># b = b - (alpha / n) * S(p_hat - y_i)</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">))</span>
        <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">dw</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">db</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)))</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>
</div>
</div>
<p><strong>Load the dataset</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_true</span><span class="o">==</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
</div>
<p><strong>Fit the model</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">Logistic_Regression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><strong>Assess the model</strong></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy on the training data</span>
<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">training_data_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">training_data_accuracy</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.3544600938967136
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy on the training data</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_data_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
<span class="n">test_data_accuracy</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.42657342657342656
</pre></div></div>
</div>
<p>Given a data point:</p>
<ul class="simple">
<li><p>Put the values from the data point into the equation <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{1 + \exp(-w+b)}\)</span></p></li>
<li><p>Choose the label based on the probability</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">[:</span><span class="mi">5</span><span class="p">,:]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([[1.616e+01, 2.154e+01, 1.062e+02, 8.098e+02, 1.008e-01, 1.284e-01,
        1.043e-01, 5.613e-02, 2.160e-01, 5.891e-02, 4.332e-01, 1.265e+00,
        2.844e+00, 4.368e+01, 4.877e-03, 1.952e-02, 2.219e-02, 9.231e-03,
        1.535e-02, 2.373e-03, 1.947e+01, 3.168e+01, 1.297e+02, 1.175e+03,
        1.395e-01, 3.055e-01, 2.992e-01, 1.312e-01, 3.480e-01, 7.619e-02],
       [1.195e+01, 1.496e+01, 7.723e+01, 4.267e+02, 1.158e-01, 1.206e-01,
        1.171e-02, 1.787e-02, 2.459e-01, 6.581e-02, 3.610e-01, 1.050e+00,
        2.455e+00, 2.665e+01, 5.800e-03, 2.417e-02, 7.816e-03, 1.052e-02,
        2.734e-02, 3.114e-03, 1.281e+01, 1.772e+01, 8.309e+01, 4.962e+02,
        1.293e-01, 1.885e-01, 3.122e-02, 4.766e-02, 3.124e-01, 7.590e-02],
       [1.340e+01, 2.052e+01, 8.864e+01, 5.567e+02, 1.106e-01, 1.469e-01,
        1.445e-01, 8.172e-02, 2.116e-01, 7.325e-02, 3.906e-01, 9.306e-01,
        3.093e+00, 3.367e+01, 5.414e-03, 2.265e-02, 3.452e-02, 1.334e-02,
        1.705e-02, 4.005e-03, 1.641e+01, 2.966e+01, 1.133e+02, 8.444e+02,
        1.574e-01, 3.856e-01, 5.106e-01, 2.051e-01, 3.585e-01, 1.109e-01],
       [1.625e+01, 1.951e+01, 1.098e+02, 8.158e+02, 1.026e-01, 1.893e-01,
        2.236e-01, 9.194e-02, 2.151e-01, 6.578e-02, 3.147e-01, 9.857e-01,
        3.070e+00, 3.312e+01, 9.197e-03, 5.470e-02, 8.079e-02, 2.215e-02,
        2.773e-02, 6.355e-03, 1.739e+01, 2.305e+01, 1.221e+02, 9.397e+02,
        1.377e-01, 4.462e-01, 5.897e-01, 1.775e-01, 3.318e-01, 9.136e-02],
       [7.691e+00, 2.544e+01, 4.834e+01, 1.704e+02, 8.668e-02, 1.199e-01,
        9.252e-02, 1.364e-02, 2.037e-01, 7.751e-02, 2.196e-01, 1.479e+00,
        1.445e+00, 1.173e+01, 1.547e-02, 6.457e-02, 9.252e-02, 1.364e-02,
        2.105e-02, 7.551e-03, 8.678e+00, 3.189e+01, 5.449e+01, 2.236e+02,
        1.596e-01, 3.064e-01, 3.393e-01, 5.000e-02, 2.790e-01, 1.066e-01]])
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">Logistic_Regression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.3986013986013986
</pre></div></div>
</div>
<p>The inputs:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X=\begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1,k} \\ x_{2,1} &amp; \ldots &amp; x_{2,k} \\ \ldots &amp; x_{i,j} &amp; \ldots \\ x_{n,1} &amp; \ldots &amp; x_{n,k} \end{pmatrix}, w=\begin{pmatrix} w_1 \\ w_2 \\ \ldots \\ w_k \end{pmatrix}, b = \text{constant}\end{split}\]</div>
<p>The linear model:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X.w + b = \begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1,k} \\ x_{2,1} &amp; \ldots &amp; x_{2,k} \\ \ldots &amp; x_{i,j} &amp; \ldots \\ x_{n,1} &amp; \ldots &amp; x_{n,k} \end{pmatrix}.\begin{pmatrix} w_1 \\ w_2 \\ \ldots \\ w_k \end{pmatrix} + b = \begin{pmatrix} x_{1,1}w_1 + &amp; \ldots &amp; + x_{1,k}w_k + b \\ x_{2,1}w_1 + &amp; \ldots &amp; + x_{2,k}w_k + b \\ \ldots &amp; \ldots &amp;   \ldots \\ x_{n,1}w_1 + &amp; \ldots &amp; + x_{n,k}w_k + b \end{pmatrix}\end{split}\]</div>
<p>The model prediction (output) is given by:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}(X.w+b) = \frac{1}{1 + \exp(-X.w+b)}= \hat{p} = h_\omega(X)\]</div>
<p>The updates of the weights and bias are given by:</p>
<div class="math notranslate nohighlight">
\[\omega_j = \omega_j - \frac{\alpha}{n} \sum[h_\omega(x_i) - y_i ] x_{i,j}\]</div>
<div class="math notranslate nohighlight">
\[b = b - \frac{\alpha}{n} \sum[h_\omega(x_i) - y_i ]\]</div>
<p>For <span class="math notranslate nohighlight">\(\omega\)</span> using linear algebra formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\omega = X^t.(\hat{p} - y) = \begin{pmatrix} x_{1,1} &amp; \ldots &amp; x_{1,n} \\ x_{2,1} &amp; \ldots &amp; x_{2,n} \\ \ldots &amp; x_{i,j} &amp; \ldots \\ x_{k,1} &amp; \ldots &amp; x_{k,n} \end{pmatrix}.\begin{pmatrix} \hat{p_1} - y_1 \\ \hat{p_2} - y_2 \\ \ldots \\ \hat{p_n} - y_n \end{pmatrix}\end{split}\]</div>
<p>For <span class="math notranslate nohighlight">\(b\)</span> using linear algebra formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}b = \sum(\hat{p} - y) = \sum\begin{pmatrix} \hat{p_1} - y_1 \\ \hat{p_2} - y_2 \\ \ldots \\ \hat{p_n} - y_n \end{pmatrix}\end{split}\]</div>
<p>The weights and bias are given by:</p>
<div class="math notranslate nohighlight">
\[\text{sigmoid}(X.w+b) = \frac{1}{1 + \exp(-X.w+b)}= \hat{p}\]</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># The Logistic Regression from scratch</span>
</pre></div>
</div>
</div>
<section id="Coefficients-significativity">
<h2>Coefficients significativity<a class="headerlink" href="#Coefficients-significativity" title="Permalink to this heading"></a></h2>
<p>The Wald statistic allows to test the coefficients significativity <span class="math notranslate nohighlight">\(\hat{w_j}\)</span>. Wald statistic is given by:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>:math:`(\frac{\hat{w_j}}{\sigma(\hat{w_j})})^2`
</pre></div>
</div>
<p>Under $H_0 : {<span class="math">\hat{w_j}</span> = 0 } <span class="math">\Longrightarrow `:nbsphinx-math:</span>frac{hat{w_j}}{sigma(hat{w_j})}` $ ~ <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span></p>
<p>The added-value of the variable <span class="math notranslate nohighlight">\(X_j\)</span> is only real if the Wald statistic &gt; 4 <span class="math notranslate nohighlight">\((3.84 = 1.96^2)\)</span></p>
<p><span class="math notranslate nohighlight">\(Wald &gt; 4\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff (\frac{\hat{w_j}}{\sigma(\hat{w_j})})^2 &gt; 4\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff \frac{\hat{w_j}}{\sigma(\hat{w_j})} &gt; 2\)</span></p>
<p>$:nbsphinx-math:<cite>iff `:nbsphinx-math:</cite>hat{w_j}` &gt; 2:nbsphinx-math:<cite>sigma`(:nbsphinx-math:</cite>hat{w_j}`) $</p>
<p><span class="math notranslate nohighlight">\(\iff \hat{w_j} - 2\sigma(\hat{w_j}) &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff \hat{w_j}\)</span> se trouve à plus de 2 écarts-type de 0</p>
<p>$:nbsphinx-math:<cite>iff `$ l’intervalle de confiance de :math:</cite>hat{w_j}` ne contient pas 0 à 95%</p>
<p>CQFD</p>
</section>
<section id="Model-quality-mesure-(Deviance)">
<h2>Model quality mesure (Deviance)<a class="headerlink" href="#Model-quality-mesure-(Deviance)" title="Permalink to this heading"></a></h2>
<p>Cf. S.Tufféry p.315</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(n:\)</span> number of observations</div>
<div class="line"><span class="math notranslate nohighlight">\(k:\)</span> number of features</div>
</div>
<p><span class="math notranslate nohighlight">\(L(\omega_k)\)</span> Likelihood of the “modèle ajusté”</p>
<p><span class="math notranslate nohighlight">\(L(\omega_0)\)</span> Likelihood of the “modèle réduit à la constante”</p>
<p><span class="math notranslate nohighlight">\(L(\omega_{max})\)</span> Likelihood of the “modèle saturé”. The one the model will compare.</p>
<p>The Deviance formula:</p>
<p><span class="math notranslate nohighlight">\(D(\omega_k) = -2[log(L(\omega_k)) - log(L(\omega_{max}))]\)</span> <span class="math notranslate nohighlight">\(^{(*)}\)</span></p>
<p>As the target is 0 or 1 <span class="math notranslate nohighlight">\(\Longrightarrow L(\omega_{max})=1 \Longrightarrow log(L(\omega_{max}))=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\Longrightarrow D(\omega_k) = -2[log(L(\omega_k))]\)</span></p>
<p>(*) <span class="math notranslate nohighlight">\(D(\omega_k) = (\frac{log(L(\omega_k))}{log(L(\omega_{max}))}^2)\)</span></p>
<p>The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance.</p>
<p>The Deviance is equivalent to the SCE for the linear regression.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Supervised learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>


<!DOCTYPE html>
<html class="writer-html5" lang="en">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The logistic regression &mdash; website Machine Learning 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/nbsphinx-code-cells.css?v=2aa19091" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js?v=e031e9a9"></script>
      <script src="../../_static/doctools.js?v=888ff710"></script>
      <script src="../../_static/sphinx_highlight.js?v=4825356b"></script>
      <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
      <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="prev" title="Supervised learning" href="index.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            website Machine Learning
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../probabilities/index.html">Probabilities</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Machine Leanring</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../unsupervised_learning/index.html">Unsupervised learning</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Supervised learning</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">The logistic regression</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-model">The model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#The-linear-LOGIT-model">The linear LOGIT model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Key-Assumptions-for-Generalizability-of-the-logit-model">Key Assumptions for Generalizability of the logit model</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Coefficients-interpretation">Coefficients interpretation</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#The-cost-function-to-minimize">The cost function to minimize</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Convexit√©">Convexit√©</a></li>
<li class="toctree-l4"><a class="reference internal" href="#How-the-Log-Loss-was-determined-?">How the Log-Loss was determined ?</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Computation-of-the-gradient-of-the-cost-function">Computation of the gradient of the cost function</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#The-Algorithm-steps">The Algorithm steps</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Training">Training</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Given-a-data-point">Given a data point</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Testing">Testing</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#APPENDIX">APPENDIX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#The-Newton-Raphson-algorithm">The Newton-Raphson algorithm</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Coefficients-significativity">Coefficients significativity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Model-quality-mesure-(Deviance)">Model quality mesure (Deviance)</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">website Machine Learning</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../index.html">Machine Leanring</a></li>
          <li class="breadcrumb-item"><a href="index.html">Supervised learning</a></li>
      <li class="breadcrumb-item active">The logistic regression</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/machine_learning/supervised_learning/LogisticRegression.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="The-logistic-regression">
<h1>The logistic regression<a class="headerlink" href="#The-logistic-regression" title="Permalink to this heading">ÔÉÅ</a></h1>
<section id="Introduction">
<h2>Introduction<a class="headerlink" href="#Introduction" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The Logistic regression applies to cases where:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(Y\)</span> is a random qualitative variable with 2 categories (a binary variable by convention, <span class="math notranslate nohighlight">\(Y = 0\)</span> if the event does not occur, and <span class="math notranslate nohighlight">\(Y = 1\)</span> if it does),</p></li>
<li><p><span class="math notranslate nohighlight">\(X_1,\ldots,X_k\)</span> are non-random qualitative or quantitative variables (<span class="math notranslate nohighlight">\(K\)</span> explanatory variables in total).</p></li>
<li><p><span class="math notranslate nohighlight">\((Y, X_1,\ldots,X_k)\)</span> represent the population variables, from which a sample of <span class="math notranslate nohighlight">\(n\)</span> individuals <span class="math notranslate nohighlight">\((i)\)</span> is drawn, and <span class="math notranslate nohighlight">\((y, x_i)\)</span> is the vector of observed realizations of <span class="math notranslate nohighlight">\((Y_i, X_i)\)</span> for each individual in the sample.</p></li>
</ul>
<p>Unlike simple linear regression, logistic regression estimates <strong>the probability</strong> of an event occurring, rather than predicting a specific numerical value.</p>
</section>
<section id="The-model">
<h2>The model<a class="headerlink" href="#The-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The variable <span class="math notranslate nohighlight">\(Y_i\)</span> follow a Bernoulli distribution with parameter <span class="math notranslate nohighlight">\(p_i\)</span> representing the probability that <span class="math notranslate nohighlight">\(Y_i=1\)</span>.</p>
<div class="math notranslate nohighlight">
\[Y_i \sim B(p_i)\]</div>
<div class="math notranslate nohighlight">
\[P(Y_i=1) = p_i \quad, \quad P(Y_i = 0) = 1 - p_i\]</div>
<p>which is equivalent to:</p>
<div class="math notranslate nohighlight">
\[P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \quad \text{for k} \in \{0, 1\}\]</div>
</section>
<section id="The-linear-LOGIT-model">
<h2>The linear LOGIT model<a class="headerlink" href="#The-linear-LOGIT-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>To ensure that the expected value of <span class="math notranslate nohighlight">\(Y, E(Y)\)</span>, only takes values between 0 and 1, we use the logistic function:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{\text{exp(x)}}{1 + \text{exp(x)}} = p\]</div>
<p>or similarly:</p>
<div class="math notranslate nohighlight">
\[f(x) = \dfrac{1}{1 + \text{exp(-x)}} = p\]</div>
<p>This guarantees that <span class="math notranslate nohighlight">\(0 &lt; f(x) &lt; 1\)</span>, so <span class="math notranslate nohighlight">\(E[Y]\)</span> can represent a valid probability.</p>
<p>The logit function is used to transform a probability <span class="math notranslate nohighlight">\(p\)</span> into an <strong>unrestricted real value</strong>:</p>
<p><span class="math notranslate nohighlight">\(\quad \text{Notations:} \quad X = (1,X_1, \ldots, X_k) \quad \text{and} \quad \beta = (\beta_0,\beta_1, \ldots, \beta_k)\)</span></p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \text{log}(\dfrac{p}{1 - p})\]</div>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \beta .X\]</div>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]</div>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{p}{1-p} \right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\]</div>
<div class="math notranslate nohighlight">
\[p = \frac{1}{1 + \exp(-\beta .X)}\]</div>
<p>Demonstration:</p>
<div class="math notranslate nohighlight">
\[p(x) = \dfrac{1}{1 + \exp(-\beta x)}\]</div>
<div class="math notranslate nohighlight">
\[\underset{inverse}   \iff \dfrac{1}{p} = 1 + \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1}{p} - 1 = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1}{p} - \dfrac{p}{p} = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \dfrac{1-p}{p} = \exp(-\beta x)\]</div>
<div class="math notranslate nohighlight">
\[\iff \log(\dfrac{1-p}{p}) = -\beta x\]</div>
<div class="math notranslate nohighlight">
\[\iff \log(\dfrac{p}{1-p}) = \beta x\]</div>
<p>To simplify the writing we have put <span class="math notranslate nohighlight">\(p\)</span> rather than <span class="math notranslate nohighlight">\(p(x)\)</span></p>
</section>
<section id="Key-Assumptions-for-Generalizability-of-the-logit-model">
<h2>Key Assumptions for Generalizability of the logit model<a class="headerlink" href="#Key-Assumptions-for-Generalizability-of-the-logit-model" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p><strong>Linearity of Log-Odds:</strong> The relationship between each continuous predictor and the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span> is linear. If this assumption is violated (e.g., non-linear effects), the interpretation of <span class="math notranslate nohighlight">\(\beta_1\)</span>‚Äã may not hold.</p></li>
<li><p><strong>No Multicollinearity:</strong> Predictors should not be highly correlated, as this can distort the interpretation of individual coefficients.</p></li>
<li><p><strong>Additivity:</strong> The effect of each predictor on the log-odds is additive. There should be no significant interaction effects unless explicitly modeled.</p></li>
<li><p><strong>Independence of Observations:</strong> The model assumes that observations are independent of each other.</p></li>
</ul>
</section>
<section id="Coefficients-interpretation">
<h2>Coefficients interpretation<a class="headerlink" href="#Coefficients-interpretation" title="Permalink to this heading">ÔÉÅ</a></h2>
<p><strong>The Odds</strong></p>
<p>The odds are defined by:</p>
<div class="math notranslate nohighlight">
\[\text{Odds} = \dfrac{p}{1-p}\]</div>
<p><span class="math notranslate nohighlight">\(\text{Where} \quad p = P(target=1|X)\)</span></p>
<blockquote>
<div><p><em>If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are ‚Äò3 to 1‚Äô:</em> <span class="math notranslate nohighlight">\(\text{Odds} = \dfrac{3/4}{1/4}=3\)</span></p>
</div></blockquote>
<ul>
<li><p><strong>Notation:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1|X=0)=\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}\]</div>
</li>
</ul>
<section id="The-Odds-Ratio">
<h3><strong>The Odds Ratio</strong><a class="headerlink" href="#The-Odds-Ratio" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>The odds ratio comparing the <strong>probability of :math:`target=1`</strong> between individuals with value <span class="math notranslate nohighlight">\(X\)</span> and those without it.</p>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{\text{Odds}(Y=1|X=1)}{\text{Odds}(Y=1|X=0)}\]</div>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}\]</div>
<p>We know that logit is given by:</p>
<div class="math notranslate nohighlight">
\[\text{logit}(p) = \text{log}(\dfrac{p}{1-p}) = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \ldots + \beta_k x_{ik}\]</div>
</section>
<section id="Interpreting-the-Intercept">
<h3><strong>Interpreting the Intercept</strong><a class="headerlink" href="#Interpreting-the-Intercept" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="line-block">
<div class="line">The intercept <span class="math notranslate nohighlight">\(\beta_0\)</span>‚Äã represents the <strong>log-odds of the outcome :math:`Y=1` when all predictors are equal to zero</strong>.</div>
<div class="line"><span class="math notranslate nohighlight">\(\beta_0\)</span>‚Äã defines the <strong>baseline probability</strong> of the outcome when all predictors are zero.</div>
</div>
<p>‚ö†Ô∏è <strong>Caveat</strong>: This interpretation of <span class="math notranslate nohighlight">\(\beta_0\)</span> is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, <span class="math notranslate nohighlight">\(\beta_0\)</span>‚Äã is primarily a mathematical component of the model and is rarely interpreted in isolation.</p>
<ul class="simple">
<li><p><strong>Odds for the baseline group:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Odds}(Y=1‚à£X_1=0)=\exp‚Å°(\beta_0)\]</div>
<ul>
<li><p><strong>Probability for the baseline group:</strong></p>
<div class="math notranslate nohighlight">
\[P(Y=1‚à£X_1=0)=\dfrac{\exp‚Å°(\beta_0)}{1 + \exp(\beta_0)}\]</div>
</li>
</ul>
<blockquote>
<div><p>If <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã is ‚Äúsmoking status‚Äù (<span class="math notranslate nohighlight">\(0\)</span> = non-smoker, <span class="math notranslate nohighlight">\(1\)</span> = smoker), then</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\beta_0\)</span>‚Äã gives the <strong>log-odds</strong> of the outcome for non-smokers</p></li>
<li><p><span class="math notranslate nohighlight">\(\exp(\beta_0)\)</span> gives the <strong>odds</strong> of the outcome for non-smokers.</p></li>
</ul>
<p>If <span class="math notranslate nohighlight">\(\beta_0 = -1\)</span>, then:</p>
<div class="math notranslate nohighlight">
\[\exp(\beta_0) = \exp(-1) \approx 0.37\]</div>
<div class="math notranslate nohighlight">
\[P(Y=1‚à£X_1=0)= \dfrac{0.37}{1 + 0.37} \approx 0.27\]</div>
<div class="line-block">
<div class="line">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.</div>
<div class="line">It is the observed proportion of lung cancer for non-smokers.</div>
</div>
</div></blockquote>
</section>
<section id="Interpreting-the-Slope">
<h3><strong>Interpreting the Slope</strong><a class="headerlink" href="#Interpreting-the-Slope" title="Permalink to this heading">ÔÉÅ</a></h3>
<div class="line-block">
<div class="line">In a model with multiple predictors, each <span class="math notranslate nohighlight">\(\beta_i\)</span>‚Äã (and its corresponding odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_i)\)</span> represents the effect of that predictor on the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span>, holding all other predictors constant.</div>
<div class="line">This is the key assumption of multivariable regression: ceteris paribus (all else being equal).</div>
</div>
<p>The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>‚Äã represents the <strong>change in the log-odds of</strong> <span class="math notranslate nohighlight">\(Y=1\)</span> for a <strong>one-unit change</strong> in <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã. The odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>‚Äã quantifies how the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> change with <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã.</p>
<p><strong>General Formula for Odds Ratio</strong></p>
<div class="line-block">
<div class="line">For any type of predictor <span class="math notranslate nohighlight">\(X_1\)</span>, the odds ratio for a one-unit increase is:</div>
<div class="line"><br /></div>
</div>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \frac{\text{Odds}(Y=1 | X_1 = x+1)}{\text{Odds}(Y=1 | X_1 = x)} = \exp(\beta_1)\]</div>
</div></blockquote>
<p>üìå <strong>:math:`exp(beta_1‚Äã)` compares the odds of :math:`Y=1` between :math:`X_1=1` and :math:`X_1=0`, controlling for all other variables in the model (all others features constant).</strong></p>
<ul class="simple">
<li><p><strong>Case: :math:`X_1` is Binary</strong></p></li>
</ul>
<p>For a binary predictor <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã (e.g., <span class="math notranslate nohighlight">\(0\)</span> = non-smoker, <span class="math notranslate nohighlight">\(1\)</span> = smoker), the odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>‚Äã compares the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between the two groups.</p>
<ul class="simple">
<li><p><strong>Logistic regression equation:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\log\left(\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\right) = \beta_0 + \beta_1 1_{\{X_1 = 1\}}‚Äã\]</div>
<ul>
<li><p><strong>Odds ratio:</strong></p>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \exp(\beta_1)\]</div>
</li>
</ul>
<p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1) = 1\)</span>: No effect of the feature <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã on the odds of <span class="math notranslate nohighlight">\(Y=1\)</span>.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1)&gt;1\)</span>: The odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are higher when <span class="math notranslate nohighlight">\(X_1‚Äã=1\)</span>. The feature <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã is <strong>positively associated</strong> with the outcome.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(\exp(\beta_1) &lt; 1\)</span>: The odds of <span class="math notranslate nohighlight">\(Y=1\)</span> are lower when <span class="math notranslate nohighlight">\(X_1‚Äã=1\)</span>. The feature <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã is <strong>negatively associated</strong> with the outcome.</p></li>
</ul>
<blockquote>
<div><div class="line-block">
<div class="line"><strong>Example:</strong></div>
<div class="line"><strong>If :math:`beta_1 = 0.7 rightarrow exp(beta_1) approx 2.01`. The odds of lung cancer for smokers :math:`(X_1=1)` are twice as high as for non-smokers :math:`(X_1=0)`.</strong></div>
</div>
</div></blockquote>
<ul>
<li><p class="rubric" id="case-x-1-is-categorical"><strong>Case: :math:`X_1` is Categorical</strong></p>
</li>
</ul>
<p>For a categorical predictor <span class="math notranslate nohighlight">\(X_1\)</span> with more than two levels (e.g., color = red, green, blue), you use <strong>dummy variables</strong>.</p>
<ul class="simple">
<li><p><strong>The logistic regression model becomes:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{green}1_{\{X_1 = \text{green}\}} + \beta_{blue}1_{\{X_1 = \text{blue}\}}\]</div>
<ul class="simple">
<li><p><strong>Reference Category (‚Äúred‚Äù):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=0\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=0\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0\]</div>
<p><strong>Interpretation:</strong> This means <span class="math notranslate nohighlight">\(\beta_0\)</span>‚Äã represents the log-odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for the reference category (‚Äúred‚Äù).</p>
<ul class="simple">
<li><p><strong>Category (‚Äúgreen‚Äù):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=1\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=0\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{green}\]</div>
<ul class="simple">
<li><p><strong>Category (‚Äúblue‚Äù):</strong> When <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{green}\}}=0\)</span> and <span class="math notranslate nohighlight">\(1_{\{X_1 = \text{blue}\}}=1\)</span>, the log-odds are:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{log}\left( \dfrac{P(Y=1)}{1 - P(Y=1)} \right) = \beta_0 + \beta_{blue}\]</div>
<p>The <strong>odds ratio for ‚Äúblue‚Äù relative to the reference ‚Äúred‚Äù</strong> is:</p>
<div class="math notranslate nohighlight">
\[\exp(\beta_{blue}) = \dfrac{\text{Odds}(Y=1 | \text{blue})}{\text{Odds}(Y=1 | \text{red})}\]</div>
<p>The same way, <span class="math notranslate nohighlight">\(\exp(\beta_{\text{green}})\)</span>‚Äã compares the odds for ‚Äúgreen‚Äù vs. the ‚Äúred‚Äù reference.</p>
<blockquote>
<div><p><strong>Interpretation:</strong></p>
<p><strong>If :math:`exp(beta_{text{green}})‚Äã=1.5`, the odds of :math:`Y=1` are :math:`1.5` times higher for ‚Äúgreen‚Äù compared to ‚Äúred‚Äù.</strong></p>
</div></blockquote>
<ul>
<li><p class="rubric" id="case-x-1-is-quantitative"><strong>Case: :math:`X_1` is Quantitative</strong></p>
</li>
</ul>
<p>For a continuous predictor <span class="math notranslate nohighlight">\(X_1\)</span> (e.g., age, blood pressure), the odds ratio <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>‚Äã represents the multiplicative change in the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã.</p>
<ul class="simple">
<li><p><strong>Logistic regression equation:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\log\left(\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\right) = \beta_0 + \beta_1 X_1‚Äã\]</div>
<ul class="simple">
<li><p><strong>Odds ratio for a one-unit increase:</strong></p></li>
</ul>
<div class="math notranslate nohighlight">
\[\text{Odds Ratio} = \frac{\text{Odds}(Y=1 | X_1 = x+1)}{\text{Odds}(Y=1 | X_1 = x)} = \exp(\beta_1)‚Äã\]</div>
<p><strong>In short</strong>: <span class="math notranslate nohighlight">\(\beta_1\)</span>‚Äã captures the <strong>constant log-odds</strong> change per unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>, so <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span>‚Äã is the <strong>odds ratio</strong> for that one-unit change.</p>
<p>This holds regardless of the starting value of <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression).</p>
<blockquote>
<div><p><strong>Interpretation:</strong></p>
<ul class="simple">
<li><p>If <span class="math notranslate nohighlight">\(\beta_1=0.095 \rightarrow \exp(\beta_1)=1.1\)</span>, the odds of <span class="math notranslate nohighlight">\(Y=1\)</span> increase by 10% for each one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã.</p></li>
<li><p>If <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã is ‚Äúyears of smoking‚Äù and <span class="math notranslate nohighlight">\(\beta_1 = 0.7 \rightarrow  \exp(\beta_1) \approx 2.01\)</span>. For each additional year of smoking, the odds of lung cancer double. .</p></li>
</ul>
</div></blockquote>
<p><strong>Summary</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Type of <span class="math notranslate nohighlight">\(X_1\)</span></p></th>
<th class="head"><p>Interpretation of <span class="math notranslate nohighlight">\(\exp(\beta_1)\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Binary</p></td>
<td><p>Compares odds of <span class="math notranslate nohighlight">\(Y=1\)</span> between <span class="math notranslate nohighlight">\(X_1=1\)</span> and <span class="math notranslate nohighlight">\(X_1=0\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Categorical</p></td>
<td><p>Compares odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a given category relative to the reference.</p></td>
</tr>
<tr class="row-even"><td><p>Quantitative</p></td>
<td><p>Multiplicative change in odds of <span class="math notranslate nohighlight">\(Y=1\)</span> for a one-unit increase in <span class="math notranslate nohighlight">\(X_1\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="The-cost-function-to-minimize">
<h1>The cost function to minimize<a class="headerlink" href="#The-cost-function-to-minimize" title="Permalink to this heading">ÔÉÅ</a></h1>
<div class="line-block">
<div class="line">La fonction de co√ªt de la r√©gression logistique, √©galement appel√©e fonction de perte logistique ou <strong>log loss</strong>, est une mesure de l‚Äôerreur de pr√©diction d‚Äôun mod√®le de r√©gression logistique.</div>
<div class="line">Elle permet d‚Äô√©valuer la qualit√© de l‚Äôajustement du mod√®le aux donn√©es d‚Äôentra√Ænement et est utilis√©e pour optimiser les param√®tres du mod√®le pendant l‚Äôapprentissage.</div>
</div>
<p>La fonction de co√ªt de la r√©gression logistique est d√©finie comme suit :</p>
<p><span class="math notranslate nohighlight">\(J(Œ∏) = -1/n * \sum(Cost(h_\theta(x_i), y_i)\)</span></p>
<p><span class="math notranslate nohighlight">\(J(Œ∏) = -1/n * \sum(y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i)))\)</span></p>
<p>o√π :</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(n\)</span> est le nombre d‚Äôexemples dans l‚Äôensemble d‚Äôentra√Ænement;</p></li>
<li><p><span class="math notranslate nohighlight">\(y_i\)</span> est la valeur r√©elle de la variable d√©pendante (0 ou 1) pour l‚Äôexemple <span class="math notranslate nohighlight">\(i\)</span>;</p></li>
<li><p><span class="math notranslate nohighlight">\(h_\theta(x_i)\)</span> est la pr√©diction du mod√®le de r√©gression logistique pour l‚Äôexemple <span class="math notranslate nohighlight">\(i\)</span>, en fonction des param√®tres <span class="math notranslate nohighlight">\(\theta\)</span> et des variables explicatives <span class="math notranslate nohighlight">\(x_i\)</span>;</p></li>
</ul>
<p><span class="math notranslate nohighlight">\(log\)</span> d√©signe le logarithme naturel.</p>
<section id="Convexit√©">
<h2>Convexit√©<a class="headerlink" href="#Convexit√©" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>La convexit√© est une propri√©t√© importante en optimisation, car elle garantit qu‚Äôun minimum local est √©galement un minimum global. Cela signifie qu‚Äôil est plus facile de trouver la solution optimale lors de l‚Äôutilisation de m√©thodes d‚Äôoptimisation telles que la descente de gradient.</p>
<p>Dans le cas de la fonction Log Loss, sa convexit√© provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives.</p>
<p>La convexit√© est une propri√©t√© importante en optimisation, car elle garantit qu‚Äôun minimum local est √©galement un minimum global. Cela signifie qu‚Äôil est plus facile de trouver la solution optimale lors de l‚Äôutilisation de m√©thodes d‚Äôoptimisation telles que la descente de gradient.</p>
<p>Dans le cas de la fonction Log Loss, sa convexit√© provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives.</p>
<p>La convexit√© d‚Äôune fonction garantit qu‚Äôun point stationnaire (c‚Äôest-√†-dire un point o√π la d√©riv√©e est nulle) est un minimum global, mais elle ne garantit pas √† elle seule l‚Äôexistence d‚Äôun tel point.</p>
<p>Pour prouver l‚Äôexistence d‚Äôun extremum, il faut √©galement montrer que la fonction est born√©e inf√©rieurement et qu‚Äôelle atteint cette borne inf√©rieure. En d‚Äôautres termes, il faut montrer qu‚Äôil existe une valeur minimale que la fonction peut atteindre et qu‚Äôil existe au moins un point o√π la fonction prend cette valeur.</p>
<p>Dans le cas de la fonction Log Loss, on peut montrer qu‚Äôelle est born√©e inf√©rieurement par z√©ro (puisque le logarithme d‚Äôun nombre positif est toujours positif) et qu‚Äôelle atteint cette borne inf√©rieure lorsque les pr√©dictions sont parfaitement correctes (c‚Äôest-√†-dire lorsque la probabilit√© pr√©dite pour la classe correcte est √©gale √† 1).</p>
<p>En combinant la convexit√© de la fonction Log Loss avec le fait qu‚Äôelle est born√©e inf√©rieurement et qu‚Äôelle atteint sa borne inf√©rieure, on peut conclure que la fonction atteint un minimum global lorsque les pr√©dictions sont parfaitement correctes.</p>
</section>
<section id="How-the-Log-Loss-was-determined-?">
<h2>How the Log-Loss was determined ?<a class="headerlink" href="#How-the-Log-Loss-was-determined-?" title="Permalink to this heading">ÔÉÅ</a></h2>
<div class="line-block">
<div class="line">The log-loss p√©nalise les erreurs de pr√©diction en fonction de la probabilit√© estim√©e par le mod√®le.</div>
<div class="line">La fonction de co√ªt attribue une p√©nalit√© plus √©lev√©e aux exemples pour lesquels la pr√©diction est loin de la valeur r√©elle, c‚Äôest-√†-dire lorsque la probabilit√© estim√©e est proche de 0 ou de 1 alors que la valeur r√©elle est respectivement 1 ou 0.</div>
</div>
<ul class="simple">
<li><p>Si la valeur r√©elle <span class="math notranslate nohighlight">\(y_i\)</span> est √©gale √† 1 (c‚Äôest-√†-dire que l‚Äôexemple appartient √† la classe positive), la p√©nalit√© est √©gale √† <span class="math notranslate nohighlight">\(-log(h_\theta(x_i))\)</span>, o√π <span class="math notranslate nohighlight">\(h_\theta(x_i)\)</span> est la probabilit√© estim√©e par le mod√®le que l‚Äôexemple appartienne √† la classe positive. Plus la probabilit√© estim√©e <span class="math notranslate nohighlight">\((h_\theta(x_i))\)</span> est proche de 0 (et donc s‚Äô√©loigne de l‚Äôobservation‚Äô), plus la p√©nalit√© est √©lev√©e (<span class="math notranslate nohighlight">\(-\log(0^+) \approx +\infty\)</span>).</p></li>
<li><p>Si la valeur r√©elle <span class="math notranslate nohighlight">\(y_i\)</span> est √©gale √† 0 (c‚Äôest-√†-dire que l‚Äôexemple appartient √† la classe n√©gative), la p√©nalit√© est √©gale √† <span class="math notranslate nohighlight">\(-log(1 - h_\theta(x_i))\)</span>, o√π <span class="math notranslate nohighlight">\(h_\theta(x_i)\)</span> est la probabilit√© estim√©e par le mod√®le que l‚Äôexemple appartienne √† la classe positive. Plus la probabilit√© estim√©e est proche de 1, plus la p√©nalit√© est √©lev√©e (<span class="math notranslate nohighlight">\(-\log(1 - 1^-) \approx +\infty\)</span>).</p></li>
</ul>
<p>On r√©sume les deux conditions ‚Äúsi‚Äù pr√©c√©dentes en une seule formule : <span class="math notranslate nohighlight">\(y_i * log(h_\theta(x_i)) + (1 - y_i) * log(1 - h_\theta(x_i))\)</span> pour l‚Äôobservation <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>L‚Äôobjectif de l‚Äôapprentissage d‚Äôun mod√®le de r√©gression logistique est de minimiser la fonction de co√ªt en ajustant les param√®tres :math:`theta`.</strong></p>
<p>Pour ce faire, on utilisera une m√©thode d‚Äôoptimisation it√©rative, <strong>la descente de gradient</strong>, pour trouver les valeurs de <span class="math notranslate nohighlight">\(\theta\)</span> qui minimisent la fonction de co√ªt sur l‚Äôensemble d‚Äôentra√Ænement.</p>
<p>La fonction de co√ªt de la r√©gression logistique est alors d√©finie comme la moyenne des p√©nalit√©s sur tous les exemples de l‚Äôensemble d‚Äôentra√Ænement.</p>
<p>En minimisant la fonction de co√ªt, on cherche √† trouver les param√®tres <span class="math notranslate nohighlight">\(\theta\)</span> qui permettent de minimiser la somme des p√©nalit√©s <span class="math notranslate nohighlight">\(\underset{\theta} min(J(\theta))\)</span> sur tous les exemples, c‚Äôest-√†-dire de maximiser la probabilit√© d‚Äôobserver les donn√©es d‚Äôentra√Ænement en fonction des param√®tres du mod√®le.</p>
</section>
<section id="Computation-of-the-gradient-of-the-cost-function">
<h2>Computation of the gradient of the cost function<a class="headerlink" href="#Computation-of-the-gradient-of-the-cost-function" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The gradient of <span class="math notranslate nohighlight">\(J(\theta) = \frac{\partial}{\partial \theta}J(\theta)\)</span></p>
<p>We start by transforming the expression of <span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{n} \sum(y_i \log(h_\theta(x_i)) + (1 - y_i) \log(1 - h_\theta(x_i)))\)</span></p>
<p>$ <span class="math">\log`(h\_:nbsphinx-math:</span>theta`(x_i)) = <span class="math">\log`(:nbsphinx-math:</span>frac{1}{1 + exp(-theta^Tx_i)}`) = -<span class="math">\log`(1 + :nbsphinx-math:</span>exp`(-:nbsphinx-math:<a href="#id1"><span class="problematic" id="id2">`</span></a>theta`^Tx_i))$</p>
<p>and</p>
<p>$ <span class="math">\log`(1 - h\_:nbsphinx-math:</span>theta`(x_i)) = <span class="math">\log`(1 - :nbsphinx-math:</span>frac{1}{1 + exp(-theta^Tx_i)}`) = <span class="math">\log`(:nbsphinx-math:</span>frac{1 + exp(-theta^Tx_i) - 1}{1 + exp(-theta^Tx_i)}`) = <span class="math">\log`(:nbsphinx-math:</span>frac{exp(-theta^Tx_i)}{1 + exp(-theta^Tx_i)}`) = <span class="math">\log`(:nbsphinx-math:</span>exp`(-<span class="math">\theta`^Tx_i)) - :nbsphinx-math:</span>log`({1 + <span class="math">\exp`(-:nbsphinx-math:</span>theta`^Tx_i)}) =
-<span class="math">\theta`^Tx_i - :nbsphinx-math:</span>log`({1 + <span class="math">\exp`(-:nbsphinx-math:</span>theta`^Tx_i)})$</p>
<p>$ <span class="math">\Rightarrow `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id3"><span class="problematic" id="id4">`</span></a>sum[y_i (-log(1 + exp(-theta^Tx_i))) + (1 - y_i) (-theta^Tx_i - log({1 + exp(-theta^Tx_i)}))]`$</p>
<p>$ <span class="math">\iff `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id5"><span class="problematic" id="id6">`</span></a>sum[y_i (-log(1 + exp(-theta^Tx_i))) + (1 - y_i) (-theta^Tx_i - log(1 + exp(-theta^Tx_i)))]`$</p>
<p>$ <span class="math">\iff `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id7"><span class="problematic" id="id8">`</span></a>sum[y_i (-log(1 + exp(-theta^Tx_i))) -theta^Tx_i - log(1 + exp(-theta^Tx_i)) + y_i theta^Tx_i  + y_i log(1 + exp(-theta^Tx_i))]`$</p>
<p>$ <span class="math">\iff `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id9"><span class="problematic" id="id10">`</span></a>sum[- y_i log(1 + exp(-theta^Tx_i)) -theta^Tx_i - log({1 + exp(-theta^Tx_i)}) + y_i theta^Tx_i  + y_i log(1 + exp(-theta^Tx_i))]`$</p>
<p>$ <span class="math">\iff `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id11"><span class="problematic" id="id12">`</span></a>sum[cancel{- y_i log(1 + exp(-theta^Tx_i))} -theta^Tx_i - log({1 + exp(-theta^Tx_i)}) + y_i theta^Tx_i  + cancel{y_i log(1 + exp(-theta^Tx_i))}]`$</p>
<p>$ <span class="math">\iff `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id13"><span class="problematic" id="id14">`</span></a>sum[-theta^Tx_i - log({1 + exp(-theta^Tx_i)}) + y_i theta^Tx_i  ]`$</p>
<p>$ <span class="math">\iff `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id15"><span class="problematic" id="id16">`</span></a>sum[y_i theta^Tx_i  -theta^Tx_i - log({1 + exp(-theta^Tx_i)}) ]`$</p>
<p>with:</p>
<p>$ -<span class="math">\theta`^Tx_i - :nbsphinx-math:</span>log`({1 + <span class="math">\exp`(-:nbsphinx-math:</span>theta`^Tx_i)}) = - <span class="math">\log`(:nbsphinx-math:</span>exp`(<span class="math">\theta`^T x_i)) - :nbsphinx-math:</span>log`(1 + <span class="math">\exp`(-:nbsphinx-math:</span>theta`^Tx_i)) = -(<span class="math">\log`(:nbsphinx-math:</span>exp`(<span class="math">\theta`^T x_i)) + :nbsphinx-math:</span>log`(1 + <span class="math">\exp`(-:nbsphinx-math:</span>theta`^Tx_i))) $</p>
<p>$= -<span class="math">\log[\exp(\theta^T x_i)(1 + \exp(-\theta^Tx_i))] `= -:nbsphinx-math:</span>log`(<span class="math">\exp`(:nbsphinx-math:</span>theta`^T x_i + 1)) $</p>
<p>$ <span class="math">\Rightarrow `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id17"><span class="problematic" id="id18">`</span></a>sum[y_i theta^Tx_i  -log(exp(theta^T x_i + 1)) ]`$</p>
<p>$ <span class="math">\Rightarrow `J(:nbsphinx-math:</span>theta`) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id19"><span class="problematic" id="id20">`</span></a>sum[y_i theta^Tx_i  -log(1 + exp(theta^T x_i)) ]`$</p>
<p>$ <span class="math">\Rightarrow  `:nbsphinx-math:</span>frac{partial}{partial theta_j}`J(<span class="math">\theta</span>) = -<span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id21"><span class="problematic" id="id22">`</span></a>sum[y_i frac{partial}{partial theta_j} (theta^Tx_i)  - frac{partial}{partial theta_j}log(1 + exp(theta^T x_i)) ]`$</p>
<p>Knowing that: <span class="math notranslate nohighlight">\(\theta^Tx_i = \theta_1 {x_i}^{(1)} + \theta_2 {x_i}^{(2)} + \ldots + \theta_k {x_i}^{(k)}\)</span></p>
<p>$ <span class="math">\Rightarrow   `:nbsphinx-math:</span>frac{partial}{partial theta_j}` (:nbsphinx-math:<a href="#id23"><span class="problematic" id="id24">`</span></a>theta`^Tx_i) = x_i^{(j)} $</p>
<p>And:</p>
<p>$ <span class="math">\frac{\partial}{\partial \theta_j}</span><span class="math">\log`(1 + :nbsphinx-math:</span>exp`(<span class="math">\theta`^T x_i)) :nbsphinx-math:</span>underset{log(u)^{‚Äô} = frac{u^{‚Äò}}{u}}` = <span class="math">\frac{\frac{\partial}{\partial \theta_j}(1 + \exp(\theta^T x_i))}</span> {1 + <span class="math">\exp`(:nbsphinx-math:</span>theta <cite>x_i)} = :nbsphinx-math:</cite>frac{frac{partial}{partial theta_j}(exp(theta^T x_i))}` {1 + <span class="math">\exp`(:nbsphinx-math:</span>theta <cite>x_i)}
:nbsphinx-math:</cite>underset{exp(u)^{‚Äô} = u^{‚Äò}exp(u)}` = <span class="math">\frac{\frac{\partial}{\partial \theta_j}(\theta^T x_i) * (\exp(\theta^T x_i))}</span> {1 + <span class="math">\exp`(:nbsphinx-math:</span>theta <a href="#id25"><span class="problematic" id="id26">`</span></a>x_i)} $</p>
<p>$= <span class="math">\frac{x_i^{(j)} * (\exp(\theta^T x_i))}</span> {1 + <span class="math">\exp`(:nbsphinx-math:</span>theta <cite>x_i)} = x_i^{(j)} h_:nbsphinx-math:</cite>theta`(x_i) $</p>
<p>$ <span class="math">\Rightarrow  `:nbsphinx-math:</span>frac{partial}{partial theta_j}`J(<span class="math">\theta</span>) = -<span class="math">\frac{1}{n}</span> <span class="math">\sum[y_i x_i^{(j)}  - x_i^{(j)} h_\theta(x_i) ] `= -:nbsphinx-math:</span>frac{1}{n}` :nbsphinx-math:<a href="#id27"><span class="problematic" id="id28">`</span></a>sum[y_i - <a href="#id41"><span class="problematic" id="id42">h_</span></a>theta(x_i) ] <a href="#id29"><span class="problematic" id="id30">`</span></a>x_i^{(j)}$</p>
<p>$ <span class="math">\iff  `:nbsphinx-math:</span>frac{partial}{partial theta_j}`J(<span class="math">\theta</span>) = <span class="math">\frac{1}{n}</span> :nbsphinx-math:<a href="#id31"><span class="problematic" id="id32">`</span></a>sum[<a href="#id43"><span class="problematic" id="id44">h_</span></a>theta(x_i) - y_i ] <a href="#id33"><span class="problematic" id="id34">`</span></a>x_i^{(j)}$</p>
<p>Sachant que l‚Äôexpression de la descente de Gradient pour mettre √† jour les pond√©rations est pour le poids <span class="math notranslate nohighlight">\(\theta_j\)</span> :</p>
<p><span class="math notranslate nohighlight">\(\theta_j = \theta_j - \alpha \frac{\partial}{\partial \theta_j}J(\theta)\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\(\alpha\)</span> est le learning rate, on obtient:</p>
<p><span class="math notranslate nohighlight">\(\theta_j = \theta_j - \frac{\alpha}{n} \sum[h_\theta(x_i) - y_i ] x_i^{(j)}\)</span></p>
</section>
</section>
<section id="The-Algorithm-steps">
<h1>The Algorithm steps<a class="headerlink" href="#The-Algorithm-steps" title="Permalink to this heading">ÔÉÅ</a></h1>
<p>Note: <span class="math notranslate nohighlight">\(\theta = (w,b)\)</span></p>
<p>with <span class="math notranslate nohighlight">\(h_\theta(x) = \frac{1}{1 + \exp(-w x + b)}\)</span></p>
<section id="Training">
<h2>Training<a class="headerlink" href="#Training" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Initialize weights as zero</p></li>
<li><p>Initialize bias as zero</p></li>
</ul>
</section>
<section id="Given-a-data-point">
<h2>Given a data point<a class="headerlink" href="#Given-a-data-point" title="Permalink to this heading">ÔÉÅ</a></h2>
<ul class="simple">
<li><p>Predict result by using <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{1 + \exp(-wx+b)}\)</span></p></li>
<li><p>Calculate the error</p></li>
<li><p>Use Gradient descent to figure out new weights and bias values</p></li>
<li><p>Repeat n times</p></li>
</ul>
</section>
<section id="Testing">
<h2>Testing<a class="headerlink" href="#Testing" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Given a data point:</p>
<ul class="simple">
<li><p>Put the values from the data point into the equation <span class="math notranslate nohighlight">\(\hat{y} = \frac{1}{1 + \exp(-w+b)}\)</span></p></li>
<li><p>Choose the label based on the probability</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Logistic Regression from scratch</span>
</pre></div>
</div>
</div>
<p>The inputs:</p>
<p>:math:<a href="#id35"><span class="problematic" id="id36">`</span></a>X=
begin{pmatrix}</p>
<blockquote>
<div><p>x_{1,1} &amp; ldots &amp; x_{1,k} \
x_{2,1} &amp; ldots &amp; x_{2,k} \
ldots &amp; x_{i,j} &amp; ldots \
x_{n,1} &amp; ldots &amp; x_{n,k}</p>
</div></blockquote>
<p>end{pmatrix}` , $ w=</p>
<p>$ , <span class="math notranslate nohighlight">\(b = constant\)</span></p>
<p>The linear model:</p>
<p><span class="math notranslate nohighlight">\(X.w + b\)</span></p>
<p><span class="math notranslate nohighlight">\(=\)</span> $</p>
<p>$ . $</p>
<p>$</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(b\)</span></p></li>
</ul>
<p>:math:<a href="#id37"><span class="problematic" id="id38">`</span></a>=
begin{pmatrix}</p>
<blockquote>
<div><p>x_{1,1}w_1 + &amp; ldots &amp; + x_{1,k}w_k + b \
x_{2,1}w_1 + &amp; ldots &amp; + x_{2,k}w_k + b \
ldots     &amp;   ldots &amp;   ldots \
x_{n,1}w_1 + &amp; ldots &amp; + x_{n,k}w_k + b</p>
</div></blockquote>
<p>end{pmatrix}`</p>
<p>The model prediction (output) is given by:</p>
<p><span class="math notranslate nohighlight">\(sigmoid(X.w+b) = \frac{1}{1 + \exp(-X.w+b)}= \hat{p} = h_\omega(X)\)</span></p>
<p>The updates of the weights and bias are given by:</p>
<p><span class="math notranslate nohighlight">\(\omega_j = \omega_j - \frac{\alpha}{n} \sum[h_\omega(x_i) - y_i ] x_{i,j}\)</span></p>
<p><span class="math notranslate nohighlight">\(b = b - \frac{\alpha}{n} \sum[h_\omega(x_i) - y_i ]\)</span></p>
<p>For <span class="math notranslate nohighlight">\(\omega\)</span> using linear algebra formula:</p>
<p><span class="math notranslate nohighlight">\(\omega = X^t.(\hat{p} - y)\)</span></p>
<p><span class="math notranslate nohighlight">\(=\)</span> $</p>
<p>$ . $</p>
<p>$</p>
<p>For <span class="math notranslate nohighlight">\(b\)</span> using linear algebra formula:</p>
<p><span class="math notranslate nohighlight">\(b = \sum(\hat{p} - y)\)</span></p>
<p>:math:<a href="#id39"><span class="problematic" id="id40">`</span></a>= sum
begin{pmatrix}</p>
<blockquote>
<div><p>hat{p_1} - y_1 \
hat{p_2} - y_2 \
ldots \
hat{p_n} - y_n</p>
</div></blockquote>
<p>end{pmatrix}`</p>
<p>The weights and bias are given by:</p>
<p><span class="math notranslate nohighlight">\(sigmoid(X.w+b) = \frac{1}{1 + \exp(-X.w+b)}= \hat{p}\)</span></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#¬†The Logistic Regression from scratch</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[95]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[128]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Logistic_Regression</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Initiate the constructor</span>
<span class="sd">            INPUT:</span>
<span class="sd">                learning_rate: magnitude of the step</span>
<span class="sd">                n_iter: number of iterations</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span> <span class="o">=</span> <span class="n">n_iter</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Train the model</span>
<span class="sd">        INPUTS:</span>
<span class="sd">            X: the dataset of the features</span>
<span class="sd">            y: the target</span>
<span class="sd">        OUTPUTS:</span>
<span class="sd">            The model</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1"># initialize the parameters:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">X</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">y</span> <span class="o">=</span> <span class="n">y</span>

        <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">update_weights</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;Update of the weights with Gradient descent&#39;&#39;&#39;</span>

        <span class="c1"># we compute the prediction (the probability)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)))</span>

        <span class="c1"># update the weights:</span>
        <span class="c1"># w_j = w_j - (alpha / n) * S(p_hat - y_i)xij</span>
        <span class="c1"># b = b - (alpha / n) * S(p_hat - y_i)</span>
        <span class="n">dw</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">))</span>
        <span class="n">db</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_samples</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">dw</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="o">*</span><span class="n">db</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span> <span class="o">-</span> <span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span><span class="p">)))</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mf">0.5</span> <span class="p">,</span> <span class="mi">1</span> <span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">y_pred</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[129]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[130]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>

<span class="n">rep</span> <span class="o">=</span> <span class="s1">&#39;/Users/davidtbo/Documents/Data_Science/99_Data&#39;</span>

<span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">rep</span><span class="p">,</span> <span class="s1">&#39;diabetes.csv&#39;</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[131]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[131]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Pregnancies</th>
      <th>Glucose</th>
      <th>BloodPressure</th>
      <th>SkinThickness</th>
      <th>Insulin</th>
      <th>BMI</th>
      <th>DiabetesPedigreeFunction</th>
      <th>Age</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6</td>
      <td>148</td>
      <td>72</td>
      <td>35</td>
      <td>0</td>
      <td>33.6</td>
      <td>0.627</td>
      <td>50</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>85</td>
      <td>66</td>
      <td>29</td>
      <td>0</td>
      <td>26.6</td>
      <td>0.351</td>
      <td>31</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8</td>
      <td>183</td>
      <td>64</td>
      <td>0</td>
      <td>0</td>
      <td>23.3</td>
      <td>0.672</td>
      <td>32</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>89</td>
      <td>66</td>
      <td>23</td>
      <td>94</td>
      <td>28.1</td>
      <td>0.167</td>
      <td>21</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>137</td>
      <td>40</td>
      <td>35</td>
      <td>168</td>
      <td>43.1</td>
      <td>2.288</td>
      <td>33</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[132]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">features</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Outcome&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[133]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Standardize the data</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>

<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="n">standardized_data</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">features</span><span class="p">)</span>

<span class="n">features</span> <span class="o">=</span> <span class="n">standardized_data</span>
<span class="n">target</span><span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Outcome&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[134]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[135]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#¬†Testing the algorithm</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[136]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span> <span class="o">=</span> <span class="n">Logistic_Regression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[137]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[138]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy</span>

<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[139]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy on the training data</span>
<span class="n">y_train_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">training_data_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_train_pred</span><span class="p">)</span>
<span class="n">training_data_accuracy</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[139]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.754071661237785
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[140]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Accuracy on the training data</span>
<span class="n">y_test_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">test_data_accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_test_pred</span><span class="p">)</span>
<span class="n">test_data_accuracy</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[140]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.7272727272727273
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[143]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predictive system</span>

<span class="n">input_data</span> <span class="o">=</span> <span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">166</span><span class="p">,</span> <span class="mi">72</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">175</span><span class="p">,</span> <span class="mf">25.8</span><span class="p">,</span> <span class="mf">0.587</span><span class="p">,</span> <span class="mi">51</span><span class="p">)</span>

<span class="c1"># to numpy array</span>
<span class="n">input_data_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="c1"># Reshape</span>
<span class="n">input_data_reshape</span> <span class="o">=</span> <span class="n">input_data_array</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Standardized the data</span>
<span class="n">input_data_std</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">input_data_reshape</span><span class="p">)</span>

<span class="n">pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">input_data_std</span><span class="p">)</span>

<span class="k">if</span> <span class="n">pred</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The person is diabetic&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The person is not diabetic&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
The person is diabetic
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area stderr docutils container">
<div class="highlight"><pre>
/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names
  warnings.warn(
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[120]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[106]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[107]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[108]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_true</span><span class="o">==</span><span class="n">y_pred</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">y_true</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">accuracy</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[109]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">clf</span> <span class="o">=</span> <span class="n">Logistic_Regression</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[110]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[111]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[111]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[112]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[112]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
0.3986013986013986
</pre></div></div>
</div>
</section>
</section>
<section id="APPENDIX">
<h1>APPENDIX<a class="headerlink" href="#APPENDIX" title="Permalink to this heading">ÔÉÅ</a></h1>
<p><strong>Demonstration:</strong> The coefficient <span class="math notranslate nohighlight">\(\beta_1\)</span>‚Äã represents the <strong>change in the log-odds of</strong> <span class="math notranslate nohighlight">\(Y=1\)</span> for a <strong>one-unit change</strong> in <span class="math notranslate nohighlight">\(X_1\)</span>‚Äã quantitative feature.</p>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))\)</span></p></li>
</ul>
<p>We know that:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{log(Odds)}(Y=1|X=x+1)=\beta_0 + \beta_1 \times (x+1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{log(Odds)}(Y=1|X=x)=\beta_0 + \beta_1 \times x\)</span></p></li>
</ul>
<p>By difference:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{log(Odds)}(Y=1|X=x+1) - \text{log(Odds)}(Y=1|X=x) =\beta_0 + \beta_1 \times (x+1) - (\beta_0 + \beta_1 \times x) = \beta_1\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{log}\left(\dfrac{\text{Odds}(Y=1|X=x+1)}{\text{Odds}(Y=1|X=x)}\right) =\beta_1\)</span></p></li>
</ul>
<p><strong>CQFD</strong></p>
<p>Note:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\dfrac{\text{Odds}(Y=1|X=x+1)}{\text{Odds}(Y=1|X=x)} = \exp(\beta_1)\)</span></p></li>
</ul>
<section id="The-Newton-Raphson-algorithm">
<h2>The Newton-Raphson algorithm<a class="headerlink" href="#The-Newton-Raphson-algorithm" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Another way to compute the coefficients</p>
<div class="line-block">
<div class="line">L‚Äôalgorithme de la m√©thode de Newton-Raphson est utilis√© pour trouver les coefficients de la r√©gression logistique en maximisant la fonction de vraisemblance.</div>
<div class="line">La r√©gression logistique est un mod√®le de r√©gression utilis√© pour pr√©dire la probabilit√© d‚Äôun √©v√©nement binaire en fonction d‚Äôune ou plusieurs variables pr√©dictives.</div>
</div>
<div class="line-block">
<div class="line">Dans le cas de la r√©gression logistique, la fonction de vraisemblance est une fonction convexe et peut √™tre maximis√©e √† l‚Äôaide de l‚Äôalgorithme de Newton-Raphson.</div>
<div class="line">L‚Äôalgorithme de Newton-Raphson est une m√©thode it√©rative pour trouver le maximum d‚Äôune fonction en utilisant la d√©riv√©e et la d√©riv√©e seconde de la fonction.</div>
</div>
<p>Dans le cas de la r√©gression logistique, la fonction de vraisemblance est donn√©e par:</p>
<p><span class="math notranslate nohighlight">\(L(\omega | X, y) = prod(p(yi | xi, beta)^{yi} * (1 - p(yi | xi, beta))^{(1 - yi)})\)</span></p>
<p>o√π <span class="math notranslate nohighlight">\(\omega\)</span> est le vecteur de coefficients de la r√©gression logistique, <span class="math notranslate nohighlight">\(X\)</span> est la matrice de variables pr√©dictives, <span class="math notranslate nohighlight">\(y\)</span> est le vecteur de variables r√©ponses binaires et <span class="math notranslate nohighlight">\(p(yi | xi, \omega)\)</span> est la probabilit√© pr√©dite de l‚Äô√©v√©nement binaire pour l‚Äôobservation <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p>Pour maximiser la fonction de vraisemblance, nous pouvons utiliser l‚Äôalgorithme de Newton-Raphson. √Ä chaque it√©ration, l‚Äôalgorithme met √† jour le vecteur de coefficients beta en utilisant la formule suivante:</p>
<p><span class="math notranslate nohighlight">\(\omega_{i+1} = \omega_i - H^{-1} * g\)</span></p>
<p>o√π</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H = \frac{\partial^2L}{\partial \omega \partial\omega'}\)</span> est la matrice hessienne de la fonction de vraisemblance,</p></li>
<li><p><span class="math notranslate nohighlight">\(g = \frac{\partial L}{\partial \omega }\)</span> est le vecteur gradient de la fonction de vraisemblance et</p></li>
<li><p><span class="math notranslate nohighlight">\(\omega_i\)</span> est le vecteur de coefficients √† l‚Äôit√©ration <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
<p>La matrice hessienne et le vecteur gradient de la fonction de vraisemblance peuvent √™tre calcul√©s √† l‚Äôaide des d√©riv√©es partielles de la fonction de vraisemblance par rapport aux coefficients beta.</p>
<p>Donc, dans le cas de la r√©gression logistique, l‚Äôalgorithme de la m√©thode de Newton-Raphson est utilis√© pour trouver les coefficients de la r√©gression logistique en maximisant la fonction de vraisemblance.</p>
<p>Cela permet d‚Äôestimer les probabilit√©s de l‚Äô√©v√©nement binaire en fonction des variables pr√©dictives.</p>
<p>NB: les it√©rations sont interrompues lorsque la diff√©rence entre deux vecteurs de solution successifs est n√©gligeable.</p>
</section>
<section id="Coefficients-significativity">
<h2>Coefficients significativity<a class="headerlink" href="#Coefficients-significativity" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>The Wald statistic allows to test the coefficients significativity <span class="math notranslate nohighlight">\(\hat{w_j}\)</span>. Wald statistic is given by:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>:math:`(\frac{\hat{w_j}}{\sigma(\hat{w_j})})^2`
</pre></div>
</div>
<p>Under $H_0 : {<span class="math">\hat{w_j}</span> = 0 } <span class="math">\Longrightarrow `:nbsphinx-math:</span>frac{hat{w_j}}{sigma(hat{w_j})}` $ ~ <span class="math notranslate nohighlight">\(\mathcal{N}(0, 1)\)</span></p>
<p>The added-value of the variable <span class="math notranslate nohighlight">\(X_j\)</span> is only real if the Wald statistic &gt; 4 <span class="math notranslate nohighlight">\((3.84 = 1.96^2)\)</span></p>
<p><span class="math notranslate nohighlight">\(Wald &gt; 4\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff (\frac{\hat{w_j}}{\sigma(\hat{w_j})})^2 &gt; 4\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff \frac{\hat{w_j}}{\sigma(\hat{w_j})} &gt; 2\)</span></p>
<p>$:nbsphinx-math:<cite>iff `:nbsphinx-math:</cite>hat{w_j}` &gt; 2:nbsphinx-math:<cite>sigma`(:nbsphinx-math:</cite>hat{w_j}`) $</p>
<p><span class="math notranslate nohighlight">\(\iff \hat{w_j} - 2\sigma(\hat{w_j}) &gt; 0\)</span></p>
<p><span class="math notranslate nohighlight">\(\iff \hat{w_j}\)</span> se trouve √† plus de 2 √©carts-type de 0</p>
<p>$:nbsphinx-math:<cite>iff `$ l‚Äôintervalle de confiance de :math:</cite>hat{w_j}` ne contient pas 0 √† 95%</p>
<p>CQFD</p>
</section>
<section id="Model-quality-mesure-(Deviance)">
<h2>Model quality mesure (Deviance)<a class="headerlink" href="#Model-quality-mesure-(Deviance)" title="Permalink to this heading">ÔÉÅ</a></h2>
<p>Cf. S.Tuff√©ry p.315</p>
<div class="line-block">
<div class="line"><span class="math notranslate nohighlight">\(n:\)</span> number of observations</div>
<div class="line"><span class="math notranslate nohighlight">\(k:\)</span> number of features</div>
</div>
<p><span class="math notranslate nohighlight">\(L(\omega_k)\)</span> Likelihood of the ‚Äúmod√®le ajust√©‚Äù</p>
<p><span class="math notranslate nohighlight">\(L(\omega_0)\)</span> Likelihood of the ‚Äúmod√®le r√©duit √† la constante‚Äù</p>
<p><span class="math notranslate nohighlight">\(L(\omega_{max})\)</span> Likelihood of the ‚Äúmod√®le satur√©‚Äù. The one the model will compare.</p>
<p>The Deviance formula:</p>
<p><span class="math notranslate nohighlight">\(D(\omega_k) = -2[log(L(\omega_k)) - log(L(\omega_{max}))]\)</span> <span class="math notranslate nohighlight">\(^{(*)}\)</span></p>
<p>As the target is 0 or 1 <span class="math notranslate nohighlight">\(\Longrightarrow L(\omega_{max})=1 \Longrightarrow log(L(\omega_{max}))=0\)</span></p>
<p><span class="math notranslate nohighlight">\(\Longrightarrow D(\omega_k) = -2[log(L(\omega_k))]\)</span></p>
<p>(*) <span class="math notranslate nohighlight">\(D(\omega_k) = (\frac{log(L(\omega_k))}{log(L(\omega_{max}))}^2)\)</span></p>
<p>The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance.</p>
<p>The Deviance is equivalent to the SCE for the linear regression.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="index.html" class="btn btn-neutral float-left" title="Supervised learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
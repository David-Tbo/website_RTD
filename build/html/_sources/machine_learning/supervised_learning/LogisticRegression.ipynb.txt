{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic regression applies to cases where:\n",
    "\n",
    "* $Y$ is a random qualitative variable with 2 categories (a binary variable by convention, $Y = 0$ if the event does not occur, and $Y = 1$ if it does),\n",
    "* $X_1,\\ldots,X_k$ are non-random qualitative or quantitative variables ($K$ explanatory variables in total).\n",
    "\n",
    "* $(Y, X_1,\\ldots,X_k)$ represent the population variables, from which a sample of $n$ individuals $(i)$ is drawn, and $(y, x_i)$ is the vector of observed realizations of $(Y_i, X_i)$ for each individual in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike simple linear regression, logistic regression estimates **the probability** of an event occurring, rather than predicting a specific numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $Y_i$ follow a Bernoulli distribution with parameter $p_i$ representing the probability that $Y_i=1$.    \n",
    "\n",
    "$$Y_i \\sim B(p_i)$$\n",
    "\n",
    "\n",
    "$$P(Y_i=1) = p_i \\quad, \\quad P(Y_i = 0) = 1 - p_i$$\n",
    "\n",
    "which is equivalent to: \n",
    "\n",
    "$$P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \\quad \\text{for k} \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear LOGIT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the expected value of $Y, E(Y)$, only takes values between 0 and 1, we use the logistic function:  \n",
    "\n",
    "$$f(x) = \\dfrac{\\text{exp(x)}}{1 + \\text{exp(x)}} = p$$\n",
    "\n",
    "or similarly:  \n",
    "\n",
    "$$f(x) = \\dfrac{1}{1 + \\text{exp(-x)}} = p$$\n",
    "\n",
    "This guarantees that $0 < f(x) < 1$, so $E[Y]$ can represent a valid probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit function is used to transform a probability $p$ into an **unrestricted real value**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad \\text{Notations:} \\quad X = (1,X_1, \\ldots, X_k) \\quad \\text{and} \\quad \\beta = (\\beta_0,\\beta_1, \\ldots, \\beta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1 - p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta .X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{log}\\left( \\dfrac{p}{1-p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p = \\frac{1}{1 + \\exp(-\\beta .X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions for Generalizability of the logit model\n",
    "\n",
    "* **Linearity of Log-Odds:** The relationship between each continuous predictor and the log-odds of $Y=1$ is linear. If this assumption is violated (e.g., non-linear effects), the interpretation of $\\beta_1$‚Äã may not hold.  \n",
    "* **No Multicollinearity:** Predictors should not be highly correlated, as this can distort the interpretation of individual coefficients.  \n",
    "* **Additivity:** The effect of each predictor on the log-odds is additive. There should be no significant interaction effects unless explicitly modeled.  \n",
    "* **Independence of Observations:** The model assumes that observations are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Odds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds are defined by:  \n",
    "\n",
    "$$\\text{Odds} = \\dfrac{p}{1-p}$$\n",
    "\n",
    "\n",
    "$\\text{Where} \\quad p = P(target=1|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are '3 to 1':_ $\\text{Odds} = \\dfrac{3/4}{1/4}=3$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Notation:**\n",
    "$$\\text{Odds}(Y=1|X=0)=\\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Odds Ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio comparing the **probability of $target=1$** between individuals with value $X$ and those without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{\\text{Odds}(Y=1|X=1)}{\\text{Odds}(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \\dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that logit is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1-p}) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting the Intercept**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept $\\beta_0$‚Äã represents the **log-odds of the outcome $Y=1$ when all predictors are equal to zero**.  \n",
    "$\\beta_0$‚Äã defines the **baseline probability** of the outcome when all predictors are zero.  \n",
    "\n",
    "‚ö†Ô∏è **Caveat**:  \n",
    "This interpretation of $\\beta_0$ is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, $\\beta_0$‚Äã is primarily a mathematical component of the model and is rarely interpreted in isolation.  \n",
    "  \n",
    "  \n",
    "* **Odds for the baseline group:**  \n",
    "\n",
    "$$\\text{Odds}(Y=1‚à£X_1=0)=\\exp‚Å°(\\beta_0)$$\n",
    "\n",
    "* **Probability for the baseline group:**\n",
    "$$P(Y=1‚à£X_1=0)=\\dfrac{\\exp‚Å°(\\beta_0)}{1 + \\exp(\\beta_0)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If $X_1$‚Äã is \"smoking status\" ($0$ = non-smoker, $1$ = smoker), then \n",
    ">* $\\beta_0$‚Äã gives the **log-odds** of the outcome for non-smokers  \n",
    ">* $\\exp(\\beta_0)$ gives the **odds** of the outcome for non-smokers.  \n",
    ">\n",
    ">If $\\beta_0 = -1$, then: $$\\exp(\\beta_0) = \\exp(-1) \\approx 0.37$$\n",
    ">\n",
    ">$$P(Y=1‚à£X_1=0)= \\dfrac{0.37}{1 + 0.37} \\approx 0.27$$\n",
    ">\n",
    ">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.  \n",
    ">It is the observed proportion of lung cancer for non-smokers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting the Slope**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a model with multiple predictors, each $\\beta_i$‚Äã (and its corresponding odds ratio $\\exp(\\beta_i)$ represents the effect of that predictor on the log-odds of $Y=1$, holding all other predictors constant.  \n",
    "This is the key assumption of multivariable regression: ceteris paribus (all else being equal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient $\\beta_1$‚Äã represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$‚Äã. The odds ratio $\\exp(\\beta_1)$‚Äã quantifies how the odds of $Y=1$ change with $X_1$‚Äã.\n",
    "\n",
    "**General Formula for Odds Ratio**\n",
    "\n",
    "For any type of predictor $X_1$, the odds ratio for a one-unit increase is:  \n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)$$\n",
    "\n",
    "üìå **Note:**  \n",
    "$\\exp(\\beta_1‚Äã)$ compares the odds of $Y=1$ between $X_1=1$ and $X_1=0$, controlling for all other variables in the model (all others features constant).\n",
    "\n",
    "* Case: $X_1$ is Binary\n",
    "\n",
    "For a binary predictor $X_1$‚Äã (e.g., $0$ = non-smoker, $1$ = smoker), the odds ratio $\\exp(\\beta_1)$‚Äã compares the odds of $Y=1$ between the two groups.\n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 1_{\\{X_1 = 1\\}}‚Äã$$\n",
    "\n",
    "* **Odds ratio:**\n",
    "$$\\text{Odds Ratio} = \\dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \\dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \\exp(\\beta_1)$$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* If $\\exp(\\beta_1) = 1$: No effect of the feature $X_1$‚Äã on the odds of $Y=1$.\n",
    "* If $\\exp(\\beta_1)>1$: The odds of $Y=1$ are higher when $X_1‚Äã=1$. The feature $X_1$‚Äã is **positively associated** with the outcome.\n",
    "* If $\\exp(\\beta_1) < 1$: The odds of $Y=1$ are lower when $X_1‚Äã=1$. The feature $X_1$‚Äã is **negatively associated** with the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Example:**  \n",
    ">If $\\beta_1 = 0.7 \\rightarrow \\exp(\\beta_1) \\approx 2.01$. The odds of lung cancer for smokers $(X_1=1)$ are twice as high as for non-smokers $(X_1=0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Case: $X_1$ is Categorical\n",
    "\n",
    "For a categorical predictor $X_1$ with more than two levels (e.g., color = red, green, blue), you use **dummy variables**. \n",
    "\n",
    "* **The logistic regression model becomes:**\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}1_{\\{X_1 = \\text{green}\\}} + \\beta_{blue}1_{\\{X_1 = \\text{blue}\\}}$$\n",
    "\n",
    "* **Reference Category (\"red\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0$$\n",
    "\n",
    "**Interpretation:** This means $\\beta_0$‚Äã represents the log-odds of $Y=1$ for the reference category (\"red\").\n",
    "\n",
    "* **Category (\"green\"):** When $1_{\\{X_1 = \\text{green}\\}}=1$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}$$\n",
    "\n",
    "* **Category (\"blue\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=1$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{blue}$$\n",
    "\n",
    "The **odds ratio for \"blue\" relative to the reference \"red\"** is:\n",
    "$$\\exp(\\beta_{blue}) = \\dfrac{\\text{Odds}(Y=1 | \\text{blue})}{\\text{Odds}(Y=1 | \\text{red})}$$\n",
    "\n",
    "The same way, $\\exp(\\beta_{\\text{green}})$‚Äã compares the odds for \"green\" vs. the \"red\" reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Interpretation:**\n",
    ">\n",
    ">If $\\exp(\\beta_{\\text{green}})‚Äã=1.5$, the odds of $Y=1$ are $1.5$ times higher for \"green\" compared to \"red\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Case: $X_1$ is Quantitative\n",
    "\n",
    "For a continuous predictor $X_1$ (e.g., age, blood pressure), the odds ratio $\\exp(\\beta_1)$‚Äã represents the multiplicative change in the odds of $Y=1$ for a one-unit increase in $X_1$‚Äã.  \n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 X_1‚Äã$$\n",
    "\n",
    "* **Odds ratio for a one-unit increase:**\n",
    "\n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)‚Äã$$\n",
    "\n",
    "**In short**: $\\beta_1$‚Äã captures the **constant log-odds** change per unit increase in $X_1$, so $\\exp(\\beta_1)$‚Äã is the **odds ratio** for that one-unit change.\n",
    "\n",
    "This holds regardless of the starting value of $X_1$‚Äã because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Interpretation:**\n",
    ">\n",
    ">* If $\\beta_1=0.095 \\rightarrow \\exp(\\beta_1)=1.1$, the odds of $Y=1$ increase by 10% for each one-unit increase in $X_1$‚Äã.\n",
    ">* If $X_1$‚Äã is \"years of smoking\" and $\\beta_1 = 0.7 \\rightarrow  \\exp(\\beta_1) \\approx 2.01$. For each additional year of smoking, the odds of lung cancer double.\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "| Type of $X_1$         | Interpretation of $\\exp(\\beta_1)$                                       |\n",
    "|-----------------------|-------------------------------------------------------------------------|\n",
    "| Binary                | Compares odds of $Y=1$ between $X_1=1$ and $X_1=0$                      |\n",
    "| Categorical           | Compares odds of $Y=1$ for a given category relative to the reference.  |\n",
    "| Quantitative          | Multiplicative change in odds of $Y=1$ for a one-unit increase in $X_1$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cost function to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de co√ªt de la r√©gression logistique, √©galement appel√©e fonction de perte logistique ou **log loss**, est une mesure de l'erreur de pr√©diction d'un mod√®le de r√©gression logistique.  \n",
    "Elle permet d'√©valuer la qualit√© de l'ajustement du mod√®le aux donn√©es d'entra√Ænement et est utilis√©e pour optimiser les param√®tres du mod√®le pendant l'apprentissage.\n",
    "\n",
    "La fonction de co√ªt de la r√©gression logistique est d√©finie comme suit :\n",
    "\n",
    "$J(Œ∏) = -1/n * \\sum(Cost(h_\\theta(x_i), y_i)$\n",
    "\n",
    "$J(Œ∏) = -1/n * \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$\n",
    "\n",
    "o√π :\n",
    "\n",
    "- $n$ est le nombre d'exemples dans l'ensemble d'entra√Ænement;  \n",
    "- $y_i$ est la valeur r√©elle de la variable d√©pendante (0 ou 1) pour l'exemple $i$;  \n",
    "- $h_\\theta(x_i)$ est la pr√©diction du mod√®le de r√©gression logistique pour l'exemple $i$, en fonction des param√®tres $\\theta$ et des variables explicatives $x_i$;  \n",
    "\n",
    "$log$ d√©signe le logarithme naturel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convexit√©\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexit√© est une propri√©t√© importante en optimisation, car elle garantit qu'un minimum local est √©galement un minimum global. Cela signifie qu'il est plus facile de trouver la solution optimale lors de l'utilisation de m√©thodes d'optimisation telles que la descente de gradient.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, sa convexit√© provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexit√© est une propri√©t√© importante en optimisation, car elle garantit qu'un minimum local est √©galement un minimum global. Cela signifie qu'il est plus facile de trouver la solution optimale lors de l'utilisation de m√©thodes d'optimisation telles que la descente de gradient.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, sa convexit√© provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexit√© d'une fonction garantit qu'un point stationnaire (c'est-√†-dire un point o√π la d√©riv√©e est nulle) est un minimum global, mais elle ne garantit pas √† elle seule l'existence d'un tel point.\n",
    "\n",
    "Pour prouver l'existence d'un extremum, il faut √©galement montrer que la fonction est born√©e inf√©rieurement et qu'elle atteint cette borne inf√©rieure. En d'autres termes, il faut montrer qu'il existe une valeur minimale que la fonction peut atteindre et qu'il existe au moins un point o√π la fonction prend cette valeur.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, on peut montrer qu'elle est born√©e inf√©rieurement par z√©ro (puisque le logarithme d'un nombre positif est toujours positif) et qu'elle atteint cette borne inf√©rieure lorsque les pr√©dictions sont parfaitement correctes (c'est-√†-dire lorsque la probabilit√© pr√©dite pour la classe correcte est √©gale √† 1).\n",
    "\n",
    "En combinant la convexit√© de la fonction Log Loss avec le fait qu'elle est born√©e inf√©rieurement et qu'elle atteint sa borne inf√©rieure, on peut conclure que la fonction atteint un minimum global lorsque les pr√©dictions sont parfaitement correctes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Log-Loss was determined ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss p√©nalise les erreurs de pr√©diction en fonction de la probabilit√© estim√©e par le mod√®le.  \n",
    "La fonction de co√ªt attribue une p√©nalit√© plus √©lev√©e aux exemples pour lesquels la pr√©diction est loin de la valeur r√©elle, c'est-√†-dire lorsque la probabilit√© estim√©e est proche de 0 ou de 1 alors que la valeur r√©elle est respectivement 1 ou 0.\n",
    "\n",
    "- Si la valeur r√©elle $y_i$ est √©gale √† 1 (c'est-√†-dire que l'exemple appartient √† la classe positive), la p√©nalit√© est √©gale √† $-log(h_\\theta(x_i))$, o√π $h_\\theta(x_i)$ est la probabilit√© estim√©e par le mod√®le que l'exemple appartienne √† la classe positive. Plus la probabilit√© estim√©e $(h_\\theta(x_i))$ est proche de 0 (et donc s'√©loigne de l'observation'), plus la p√©nalit√© est √©lev√©e ($-\\log(0^+) \\approx +\\infty$).  \n",
    " \n",
    "- Si la valeur r√©elle $y_i$ est √©gale √† 0 (c'est-√†-dire que l'exemple appartient √† la classe n√©gative), la p√©nalit√© est √©gale √† $-log(1 - h_\\theta(x_i))$, o√π $h_\\theta(x_i)$ est la probabilit√© estim√©e par le mod√®le que l'exemple appartienne √† la classe positive. Plus la probabilit√© estim√©e est proche de 1, plus la p√©nalit√© est √©lev√©e ($-\\log(1 - 1^-) \\approx +\\infty$).  \n",
    "\n",
    "On r√©sume les deux conditions \"si\" pr√©c√©dentes en une seule formule : $y_i * log(h_\\theta(x_i)) + (1 - y_i) * log(1 - h_\\theta(x_i))$ pour l'observation $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'objectif de l'apprentissage d'un mod√®le de r√©gression logistique est de minimiser la fonction de co√ªt en ajustant les param√®tres $\\theta$.**\n",
    "\n",
    "Pour ce faire, on utilisera une m√©thode d'optimisation it√©rative, **la descente de gradient**, pour trouver les valeurs de $\\theta$ qui minimisent la fonction de co√ªt sur l'ensemble d'entra√Ænement.\n",
    "\n",
    "La fonction de co√ªt de la r√©gression logistique est alors d√©finie comme la moyenne des p√©nalit√©s sur tous les exemples de l'ensemble d'entra√Ænement.\n",
    "\n",
    "En minimisant la fonction de co√ªt, on cherche √† trouver les param√®tres $\\theta$ qui permettent de minimiser la somme des p√©nalit√©s $\\underset{\\theta} min(J(\\theta))$  sur tous les exemples, c'est-√†-dire de maximiser la probabilit√© d'observer les donn√©es d'entra√Ænement en fonction des param√®tres du mod√®le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $J(\\theta) = \\frac{\\partial}{\\partial \\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by transforming the expression of  $J(\\theta) = -\\frac{1}{n} \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$\n",
    "\n",
    "$\\log(h_\\theta(x_i)) = \\log(\\frac{1}{1 + \\exp(-\\theta^Tx_i)}) = -\\log(1 + \\exp(-\\theta^Tx_i))$  \n",
    "\n",
    "and \n",
    "\n",
    "$\\log(1 - h_\\theta(x_i)) = \\log(1 - \\frac{1}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\frac{1 + \\exp(-\\theta^Tx_i) - 1}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\frac{\\exp(-\\theta^Tx_i)}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\exp(-\\theta^Tx_i)) - \\log({1 + \\exp(-\\theta^Tx_i)}) = -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log(1 + exp(-\\theta^Tx_i)))]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) -\\theta^Tx_i - \\log(1 + \\exp(-\\theta^Tx_i)) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[- y_i \\log(1 + \\exp(-\\theta^Tx_i)) -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[\\cancel{- y_i \\log(1 + \\exp(-\\theta^Tx_i))} -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + \\cancel{y_i \\log(1 + \\exp(-\\theta^Tx_i))}]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  ]$\n",
    "\n",
    "$\\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with:\n",
    "\n",
    "$-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = - \\log(\\exp(\\theta^T x_i)) - \\log(1 + \\exp(-\\theta^Tx_i)) = -(\\log(\\exp(\\theta^T x_i)) + \\log(1 + \\exp(-\\theta^Tx_i)))$  \n",
    "\n",
    "$= -\\log[\\exp(\\theta^T x_i)(1 + \\exp(-\\theta^Tx_i))] = -\\log(\\exp(\\theta^T x_i + 1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(\\exp(\\theta^T x_i + 1)) ]$\n",
    "\n",
    "$\\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(1 + \\exp(\\theta^T x_i)) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i)  - \\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that: $\\theta^Tx_i = \\theta_1 {x_i}^{(1)} + \\theta_2 {x_i}^{(2)} + \\ldots + \\theta_k {x_i}^{(k)}$\n",
    "\n",
    "$\\Rightarrow \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i) = x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:  \n",
    "\n",
    "$\\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) \\underset{\\log(u)^{'} = \\frac{u^{'}}{u}} = \\frac{\\frac{\\partial}{\\partial \\theta_j}(1 + \\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = \\frac{\\frac{\\partial}{\\partial \\theta_j}(\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} \\underset{\\exp(u)^{'} = u^{'}\\exp(u)} =  \\frac{\\frac{\\partial}{\\partial \\theta_j}(\\theta^T x_i) * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$  \n",
    "\n",
    "$=  \\frac{x_i^{(j)} * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = x_i^{(j)} h_\\theta(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Rightarrow  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i x_i^{(j)}  - x_i^{(j)} h_\\theta(x_i) ] = -\\frac{1}{n} \\sum[y_i - h_\\theta(x_i) ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\iff  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sachant que l'expression de la descente de Gradient pour mettre √† jour les pond√©rations est pour le poids $\\theta_j$ :\n",
    "\n",
    "$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "o√π $\\alpha$ est le learning rate, on obtient:\n",
    "\n",
    "$\\theta_j = \\theta_j - \\frac{\\alpha}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Algorithm steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $\\theta = (w,b)$   \n",
    "\n",
    "with $h_\\theta(x) = \\frac{1}{1 + \\exp(-w x + b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize weights as zero\n",
    "- Initialize bias as zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict result by using $\\hat{y} = \\frac{1}{1 + \\exp(-wx+b)}$\n",
    "- Calculate the error\n",
    "- Use Gradient descent to figure out new weights and bias values\n",
    "- Repeat n times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data point:  \n",
    "- Put the values from the data point into the equation $\\hat{y} = \\frac{1}{1 + \\exp(-w+b)}$\n",
    "- Choose the label based on the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs:\n",
    "\n",
    "$$X=\\begin{pmatrix} x_{1,1} & \\ldots & x_{1,k} \\\\ x_{2,1} & \\ldots & x_{2,k} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{n,1} & \\ldots & x_{n,k} \\end{pmatrix}, w=\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_k \\end{pmatrix}, b = \\text{constant}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model:\n",
    "\n",
    "$$X.w + b = \\begin{pmatrix} x_{1,1} & \\ldots & x_{1,k} \\\\ x_{2,1} & \\ldots & x_{2,k} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{n,1} & \\ldots & x_{n,k} \\end{pmatrix}.\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_k \\end{pmatrix} + b = \\begin{pmatrix} x_{1,1}w_1 + & \\ldots & + x_{1,k}w_k + b \\\\ x_{2,1}w_1 + & \\ldots & + x_{2,k}w_k + b \\\\ \\ldots & \\ldots &   \\ldots \\\\ x_{n,1}w_1 + & \\ldots & + x_{n,k}w_k + b \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction (output) is given by:\n",
    "\n",
    "$$\\text{sigmoid}(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p} = h_\\omega(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updates of the weights and bias are given by:\n",
    "\n",
    "$$\\omega_j = \\omega_j - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ] x_{i,j}$$\n",
    "\n",
    "$$b = b - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\omega$ using linear algebra formula:\n",
    "\n",
    "$$\\omega = X^t.(\\hat{p} - y) = \\begin{pmatrix} x_{1,1} & \\ldots & x_{1,n} \\\\ x_{2,1} & \\ldots & x_{2,n} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{k,1} & \\ldots & x_{k,n} \\end{pmatrix}.\\begin{pmatrix} \\hat{p_1} - y_1 \\\\ \\hat{p_2} - y_2 \\\\ \\ldots \\\\ \\hat{p_n} - y_n \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $b$ using linear algebra formula:\n",
    "\n",
    "$$b = \\sum(\\hat{p} - y) = \\sum\\begin{pmatrix} \\hat{p_1} - y_1 \\\\ \\hat{p_2} - y_2 \\\\ \\ldots \\\\ \\hat{p_n} - y_n \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and bias are given by:\n",
    "\n",
    "$$\\text{sigmoid}(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†The Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
    "        '''Initiate the constructor\n",
    "            INPUT:\n",
    "                learning_rate: magnitude of the step\n",
    "                n_iter: number of iterations\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Train the model\n",
    "        INPUTS:\n",
    "            X: the dataset of the features\n",
    "            y: the target\n",
    "        OUTPUTS:\n",
    "            The model\n",
    "        '''\n",
    "\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # initialize the parameters:\n",
    "        self.weights = np.zeros(self.n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            return self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        '''Update of the weights with Gradient descent'''\n",
    "\n",
    "        # we compute the prediction (the probability)\n",
    "        y_pred = 1 / (1 + np.exp( - (np.dot(self.X, self.weights) + self.bias)))\n",
    "\n",
    "        # update the weights:\n",
    "        # w_j = w_j - (alpha / n) * S(p_hat - y_i)xij\n",
    "        # b = b - (alpha / n) * S(p_hat - y_i)\n",
    "        dw = (1 / self.n_samples) * np.dot(self.X.T, (y_pred - self.y))\n",
    "        db = (1 / self.n_samples) * np.sum(y_pred - self.y)\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate*dw\n",
    "        self.bias = self.bias - self.learning_rate*db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = 1 / (1 + np.exp( - (X.dot(self.weights) + self.bias)))\n",
    "        y_pred = np.where(y_pred > 0.5 , 1 , 0)\n",
    "        return y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "rep = '/Users/davidtbo/Documents/Data_Science/99_Data'\n",
    "\n",
    "filename = os.path.join(rep, 'diabetes.csv')\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(features)\n",
    "\n",
    "standardized_data = scaler.transform(features)\n",
    "\n",
    "features = standardized_data\n",
    "target= df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#¬†Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Logistic_Regression(learning_rate=0.01, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754071661237785"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "training_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person is diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predictive system\n",
    "\n",
    "input_data = (5, 166, 72, 19, 175, 25.8, 0.587, 51)\n",
    "\n",
    "# to numpy array\n",
    "input_data_array = np.asarray(input_data)\n",
    "\n",
    "# Reshape\n",
    "input_data_reshape = input_data_array.reshape(1, -1)\n",
    "\n",
    "# Standardized the data\n",
    "input_data_std = scaler.transform(input_data_reshape)\n",
    "\n",
    "pred = classifier.predict(input_data_std)\n",
    "\n",
    "if pred:\n",
    "    print('The person is diabetic')\n",
    "else:\n",
    "    print('The person is not diabetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_breast_cancer()\n",
    "X, y = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = sum(y_true==y_pred)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Logistic_Regression(learning_rate=0.0001)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3986013986013986"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration:** The coefficient $\\beta_1$‚Äã represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$‚Äã quantitative feature.  \n",
    "\n",
    "Notations:  \n",
    "* $\\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))$ \n",
    "* $\\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))$   \n",
    "\n",
    "We know that:  \n",
    "\n",
    "* $\\text{log(Odds)}(Y=1|X=x+1)=\\beta_0 + \\beta_1 \\times (x+1)$\n",
    "* $\\text{log(Odds)}(Y=1|X=x)=\\beta_0 + \\beta_1 \\times x$   \n",
    "\n",
    "By difference:  \n",
    "* $\\text{log(Odds)}(Y=1|X=x+1) - \\text{log(Odds)}(Y=1|X=x) =\\beta_0 + \\beta_1 \\times (x+1) - (\\beta_0 + \\beta_1 \\times x) = \\beta_1$  \n",
    "\n",
    "* $\\text{log}\\left(\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)}\\right) =\\beta_1$  \n",
    "\n",
    "**CQFD**  \n",
    "\n",
    "Note:  \n",
    "\n",
    "* $\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)} = \\exp(\\beta_1)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Newton-Raphson algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compute the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de la m√©thode de Newton-Raphson est utilis√© pour trouver les coefficients de la r√©gression logistique en maximisant la fonction de vraisemblance.  \n",
    "La r√©gression logistique est un mod√®le de r√©gression utilis√© pour pr√©dire la probabilit√© d'un √©v√©nement binaire en fonction d'une ou plusieurs variables pr√©dictives.  \n",
    "\n",
    "Dans le cas de la r√©gression logistique, la fonction de vraisemblance est une fonction convexe et peut √™tre maximis√©e √† l'aide de l'algorithme de Newton-Raphson.   \n",
    "L'algorithme de Newton-Raphson est une m√©thode it√©rative pour trouver le maximum d'une fonction en utilisant la d√©riv√©e et la d√©riv√©e seconde de la fonction.\n",
    "\n",
    "Dans le cas de la r√©gression logistique, la fonction de vraisemblance est donn√©e par:\n",
    "\n",
    "$L(\\omega | X, y) = prod(p(yi | xi, beta)^{yi} * (1 - p(yi | xi, beta))^{(1 - yi)})$\n",
    "\n",
    "o√π $\\omega$ est le vecteur de coefficients de la r√©gression logistique, $X$ est la matrice de variables pr√©dictives, $y$ est le vecteur de variables r√©ponses binaires et $p(yi | xi, \\omega)$ est la probabilit√© pr√©dite de l'√©v√©nement binaire pour l'observation $i$.\n",
    "\n",
    "Pour maximiser la fonction de vraisemblance, nous pouvons utiliser l'algorithme de Newton-Raphson. √Ä chaque it√©ration, l'algorithme met √† jour le vecteur de coefficients beta en utilisant la formule suivante:\n",
    "\n",
    "\n",
    "$\\omega_{i+1} = \\omega_i - H^{-1} * g$  \n",
    "\n",
    "o√π  \n",
    "\n",
    "- $H = \\frac{\\partial^2L}{\\partial \\omega \\partial\\omega'}$ est la matrice hessienne de la fonction de vraisemblance,  \n",
    "\n",
    "- $g = \\frac{\\partial L}{\\partial \\omega }$ est le vecteur gradient de la fonction de vraisemblance et  \n",
    "\n",
    "- $\\omega_i$ est le vecteur de coefficients √† l'it√©ration $i$.\n",
    "\n",
    "La matrice hessienne et le vecteur gradient de la fonction de vraisemblance peuvent √™tre calcul√©s √† l'aide des d√©riv√©es partielles de la fonction de vraisemblance par rapport aux coefficients beta.\n",
    "\n",
    "Donc, dans le cas de la r√©gression logistique, l'algorithme de la m√©thode de Newton-Raphson est utilis√© pour trouver les coefficients de la r√©gression logistique en maximisant la fonction de vraisemblance.  \n",
    "\n",
    "Cela permet d'estimer les probabilit√©s de l'√©v√©nement binaire en fonction des variables pr√©dictives.  \n",
    "\n",
    "NB: les it√©rations sont interrompues lorsque la diff√©rence entre deux vecteurs de solution successifs est n√©gligeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients significativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wald statistic allows to test the coefficients significativity $\\hat{w_j}$. Wald statistic is given by::    \n",
    "\n",
    "$(\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2$  \n",
    "\n",
    "Under $H_0 : \\{\\hat{w_j} = 0 \\} \\Longrightarrow \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} $ ~ $\\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added-value of the variable $X_j$ is only real if the Wald statistic > 4 $(3.84 = 1.96^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Wald > 4$    \n",
    "\n",
    "$\\iff (\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2 > 4$  \n",
    "\n",
    "$\\iff \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} > 2$  \n",
    "\n",
    "$\\iff \\hat{w_j} > 2\\sigma(\\hat{w_j}) $  \n",
    "\n",
    "$\\iff \\hat{w_j} - 2\\sigma(\\hat{w_j}) > 0$  \n",
    "\n",
    "$\\iff \\hat{w_j}$ se trouve √† plus de 2 √©carts-type de 0  \n",
    "\n",
    "$\\iff $ l'intervalle de confiance de $\\hat{w_j}$ ne contient pas 0 √† 95%  \n",
    "\n",
    "CQFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality mesure (Deviance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cf. S.Tuff√©ry p.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n:$ number of observations  \n",
    "$k:$ number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\omega_k)$ Likelihood of the \"mod√®le ajust√©\"  \n",
    "\n",
    "$L(\\omega_0)$ Likelihood of the \"mod√®le r√©duit √† la constante\"  \n",
    "\n",
    "$L(\\omega_{max})$ Likelihood of the \"mod√®le satur√©\". The one the model will compare.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance formula:  \n",
    "\n",
    "$D(\\omega_k) = -2[log(L(\\omega_k)) - log(L(\\omega_{max}))]$  $^{(*)}$\n",
    "\n",
    "As the target is 0 or 1 $\\Longrightarrow L(\\omega_{max})=1 \\Longrightarrow log(L(\\omega_{max}))=0$  \n",
    "\n",
    "$\\Longrightarrow D(\\omega_k) = -2[log(L(\\omega_k))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) $D(\\omega_k) = (\\frac{log(L(\\omega_k))}{log(L(\\omega_{max}))}^2)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance is equivalent to the SCE for the linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

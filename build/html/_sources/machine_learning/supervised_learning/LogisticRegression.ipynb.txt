{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic regression applies to cases where:\n",
    "\n",
    "* $Y$ is a random qualitative variable with 2 categories (a binary variable by convention, $Y = 0$ if the event does not occur, and $Y = 1$ if it does),\n",
    "* $X_1,\\ldots,X_k$ are non-random qualitative or quantitative variables ($K$ explanatory variables in total).\n",
    "\n",
    "* $(Y, X_1,\\ldots,X_k)$ represent the population variables, from which a sample of $n$ individuals $(i)$ is drawn, and $(y, x_i)$ is the vector of observed realizations of $(Y_i, X_i)$ for each individual in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike simple linear regression, logistic regression estimates **the probability** of an event occurring, rather than predicting a specific numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $Y_i$ follow a Bernoulli distribution with parameter $p_i$ representing the probability that $Y_i=1$.    \n",
    "\n",
    "$$Y_i \\sim B(p_i)$$\n",
    "\n",
    "\n",
    "$$P(Y_i=1) = p_i \\quad, \\quad P(Y_i = 0) = 1 - p_i$$\n",
    "\n",
    "which is equivalent to: \n",
    "\n",
    "$$P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \\quad \\text{for k} \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear LOGIT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the expected value of $Y, E(Y)$, only takes values between 0 and 1, we use the logistic function:  \n",
    "\n",
    "$$f(x) = \\dfrac{\\text{exp(x)}}{1 + \\text{exp(x)}} = p$$\n",
    "\n",
    "or similarly:  \n",
    "\n",
    "$$f(x) = \\dfrac{1}{1 + \\text{exp(-x)}} = p$$\n",
    "\n",
    "This guarantees that $0 < f(x) < 1$, so $E[Y]$ can represent a valid probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit function is used to transform a probability $p$ into an **unrestricted real value**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad \\text{Notations:} \\quad X = (1,X_1, \\ldots, X_k) \\quad \\text{and} \\quad \\beta = (\\beta_0,\\beta_1, \\ldots, \\beta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1 - p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta .X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{log}\\left( \\dfrac{p}{1-p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p = \\frac{1}{1 + \\exp(-\\beta .X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions for Generalizability of the logit model\n",
    "\n",
    "* **Linearity of Log-Odds:** The relationship between each continuous predictor and the log-odds of $Y=1$ is linear. If this assumption is violated (e.g., non-linear effects), the interpretation of $\\beta_1$â€‹ may not hold.  \n",
    "* **No Multicollinearity:** Predictors should not be highly correlated, as this can distort the interpretation of individual coefficients.  \n",
    "* **Additivity:** The effect of each predictor on the log-odds is additive. There should be no significant interaction effects unless explicitly modeled.  \n",
    "* **Independence of Observations:** The model assumes that observations are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Odds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds are defined by:  \n",
    "\n",
    "$$\\text{Odds} = \\dfrac{p}{1-p}$$\n",
    "\n",
    "\n",
    "$\\text{Where} \\quad p = P(target=1|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are '3 to 1':_ $\\text{Odds} = \\dfrac{3/4}{1/4}=3$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Notation:**\n",
    "$$\\text{Odds}(Y=1|X=0)=\\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Odds Ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio comparing the **probability of $target=1$** between individuals with value $X$ and those without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{\\text{Odds}(Y=1|X=1)}{\\text{Odds}(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \\dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that logit is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1-p}) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting $\\beta_0$â€‹ the Intercept**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept $\\beta_0$â€‹ represents the **log-odds of the outcome $Y=1$ when all predictors are equal to zero**.  \n",
    "$\\beta_0$â€‹ defines the **baseline probability** of the outcome when all predictors are zero.  \n",
    "\n",
    "âš ï¸ **Caveat**: This interpretation of $\\beta_0$ is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, $\\beta_0$â€‹ is primarily a mathematical component of the model and is rarely interpreted in isolation.  \n",
    "  \n",
    "  \n",
    "* **Odds for the baseline group:**  \n",
    "\n",
    "$$\\text{Odds}(Y=1âˆ£X_1=0)=\\expâ¡(\\beta_0)$$\n",
    "\n",
    "* **Probability for the baseline group:**\n",
    "$$P(Y=1âˆ£X_1=0)=\\dfrac{\\expâ¡(\\beta_0)}{1 + \\exp(\\beta_0)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If $X_1$â€‹ is \"smoking status\" ($0$ = non-smoker, $1$ = smoker), then \n",
    ">* $\\beta_0$â€‹ gives the **log-odds** of the outcome for non-smokers  \n",
    ">* $\\exp(\\beta_0)$ gives the **odds** of the outcome for non-smokers.  \n",
    ">\n",
    ">If $\\beta_0 = -1$, then: $$\\exp(\\beta_0) = \\exp(-1) \\approx 0.37$$\n",
    ">\n",
    ">$$P(Y=1âˆ£X_1=0)= \\dfrac{0.37}{1 + 0.37} \\approx 0.27$$\n",
    ">\n",
    ">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.  \n",
    ">It is the observed proportion of lung cancer for non-smokers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting $\\beta_1$â€‹ the Slope**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a model with multiple predictors, each $\\beta_i$â€‹ (and its corresponding odds ratio $\\exp(\\beta_i)$ represents the effect of that predictor on the log-odds of $Y=1$, holding all other predictors constant.  \n",
    "This is the key assumption of multivariable regression: ceteris paribus (all else being equal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient $\\beta_1$â€‹ represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$â€‹. The odds ratio $\\exp(\\beta_1)$â€‹ quantifies how the odds of $Y=1$ change with $X_1$â€‹.\n",
    "\n",
    "**General Formula for Odds Ratio**\n",
    "\n",
    "For any type of predictor $X_1$, the odds ratio for a one-unit increase is:  \n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)$$\n",
    "\n",
    "ðŸ“Œ **$\\exp(\\beta_1â€‹)$ compares the odds of $Y=1$ between $X_1=1$ and $X_1=0$, controlling for all other variables in the model (all others features constant).**\n",
    "\n",
    "* ##### **Case: $X_1$ is Binary**\n",
    "\n",
    "For a binary predictor $X_1$â€‹ (e.g., $0$ = non-smoker, $1$ = smoker), the odds ratio $\\exp(\\beta_1)$â€‹ compares the odds of $Y=1$ between the two groups.\n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 1_{\\{X_1 = 1\\}}â€‹$$\n",
    "\n",
    "* **Odds ratio:**\n",
    "$$\\text{Odds Ratio} = \\dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \\dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \\exp(\\beta_1)$$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* If $\\exp(\\beta_1) = 1$: No effect of the feature $X_1$â€‹ on the odds of $Y=1$.\n",
    "* If $\\exp(\\beta_1)>1$: The odds of $Y=1$ are higher when $X_1â€‹=1$. The feature $X_1$â€‹ is **positively associated** with the outcome.\n",
    "* If $\\exp(\\beta_1) < 1$: The odds of $Y=1$ are lower when $X_1â€‹=1$. The feature $X_1$â€‹ is **negatively associated** with the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Example:**  \n",
    ">**If $\\beta_1 = 0.7 \\rightarrow \\exp(\\beta_1) \\approx 2.01$. The odds of lung cancer for smokers $(X_1=1)$ are twice as high as for non-smokers $(X_1=0)$.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### **Case: $X_1$ is Categorical**\n",
    "\n",
    "For a categorical predictor $X_1$ with more than two levels (e.g., color = red, green, blue), you use **dummy variables**. \n",
    "\n",
    "* **The logistic regression model becomes:**\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}1_{\\{X_1 = \\text{green}\\}} + \\beta_{blue}1_{\\{X_1 = \\text{blue}\\}}$$\n",
    "\n",
    "* **Reference Category (\"red\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0$$\n",
    "\n",
    "**Interpretation:** This means $\\beta_0$â€‹ represents the log-odds of $Y=1$ for the reference category (\"red\").\n",
    "\n",
    "* **Category (\"green\"):** When $1_{\\{X_1 = \\text{green}\\}}=1$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}$$\n",
    "\n",
    "* **Category (\"blue\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=1$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{blue}$$\n",
    "\n",
    "The **odds ratio for \"blue\" relative to the reference \"red\"** is:\n",
    "$$\\exp(\\beta_{blue}) = \\dfrac{\\text{Odds}(Y=1 | \\text{blue})}{\\text{Odds}(Y=1 | \\text{red})}$$\n",
    "\n",
    "The same way, $\\exp(\\beta_{\\text{green}})$â€‹ compares the odds for \"green\" vs. the \"red\" reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Interpretation:**\n",
    ">\n",
    ">**If $\\exp(\\beta_{\\text{green}})â€‹=1.5$, the odds of $Y=1$ are $1.5$ times higher for \"green\" compared to \"red\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### **Case: $X_1$ is Quantitative**\n",
    "\n",
    "For a continuous predictor $X_1$ (e.g., age, blood pressure), the odds ratio $\\exp(\\beta_1)$â€‹ represents the multiplicative change in the odds of $Y=1$ for a one-unit increase in $X_1$â€‹.  \n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 X_1â€‹$$\n",
    "\n",
    "* **Odds ratio for a one-unit increase:**\n",
    "\n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)â€‹$$\n",
    "\n",
    "**In short**: $\\beta_1$â€‹ captures the **constant log-odds** change per unit increase in $X_1$, so $\\exp(\\beta_1)$â€‹ is the **odds ratio** for that one-unit change.\n",
    "\n",
    "This holds regardless of the starting value of $X_1$â€‹ because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Interpretation:**\n",
    ">\n",
    ">* If $\\beta_1=0.095 \\rightarrow \\exp(\\beta_1)=1.1$, the odds of $Y=1$ increase by 10% for each one-unit increase in $X_1$â€‹.\n",
    ">* If $X_1$â€‹ is \"years of smoking\" and $\\beta_1 = 0.7 \\rightarrow  \\exp(\\beta_1) \\approx 2.01$. For each additional year of smoking, the odds of lung cancer double.\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "| Type of $X_1$         | Interpretation of $\\exp(\\beta_1)$                                       |\n",
    "|-----------------------|-------------------------------------------------------------------------|\n",
    "| Binary                | Compares odds of $Y=1$ between $X_1=1$ and $X_1=0$                      |\n",
    "| Categorical           | Compares odds of $Y=1$ for a given category relative to the reference.  |\n",
    "| Quantitative          | Multiplicative change in odds of $Y=1$ for a one-unit increase in $X_1$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The cost function to minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction de coÃ»t de la rÃ©gression logistique, Ã©galement appelÃ©e fonction de perte logistique ou **log loss**, est une mesure de l'erreur de prÃ©diction d'un modÃ¨le de rÃ©gression logistique.  \n",
    "Elle permet d'Ã©valuer la qualitÃ© de l'ajustement du modÃ¨le aux donnÃ©es d'entraÃ®nement et est utilisÃ©e pour optimiser les paramÃ¨tres du modÃ¨le pendant l'apprentissage.\n",
    "\n",
    "La fonction de coÃ»t de la rÃ©gression logistique est dÃ©finie comme suit :\n",
    "\n",
    "$J(Î¸) = -1/n * \\sum(Cost(h_\\theta(x_i), y_i)$\n",
    "\n",
    "$J(Î¸) = -1/n * \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$\n",
    "\n",
    "oÃ¹ :\n",
    "\n",
    "- $n$ est le nombre d'exemples dans l'ensemble d'entraÃ®nement;  \n",
    "- $y_i$ est la valeur rÃ©elle de la variable dÃ©pendante (0 ou 1) pour l'exemple $i$;  \n",
    "- $h_\\theta(x_i)$ est la prÃ©diction du modÃ¨le de rÃ©gression logistique pour l'exemple $i$, en fonction des paramÃ¨tres $\\theta$ et des variables explicatives $x_i$;  \n",
    "\n",
    "$log$ dÃ©signe le logarithme naturel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvexitÃ©\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexitÃ© est une propriÃ©tÃ© importante en optimisation, car elle garantit qu'un minimum local est Ã©galement un minimum global. Cela signifie qu'il est plus facile de trouver la solution optimale lors de l'utilisation de mÃ©thodes d'optimisation telles que la descente de gradient.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, sa convexitÃ© provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexitÃ© est une propriÃ©tÃ© importante en optimisation, car elle garantit qu'un minimum local est Ã©galement un minimum global. Cela signifie qu'il est plus facile de trouver la solution optimale lors de l'utilisation de mÃ©thodes d'optimisation telles que la descente de gradient.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, sa convexitÃ© provient de la forme logarithmique de la fonction, qui est toujours convexe pour des valeurs positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La convexitÃ© d'une fonction garantit qu'un point stationnaire (c'est-Ã -dire un point oÃ¹ la dÃ©rivÃ©e est nulle) est un minimum global, mais elle ne garantit pas Ã  elle seule l'existence d'un tel point.\n",
    "\n",
    "Pour prouver l'existence d'un extremum, il faut Ã©galement montrer que la fonction est bornÃ©e infÃ©rieurement et qu'elle atteint cette borne infÃ©rieure. En d'autres termes, il faut montrer qu'il existe une valeur minimale que la fonction peut atteindre et qu'il existe au moins un point oÃ¹ la fonction prend cette valeur.\n",
    "\n",
    "Dans le cas de la fonction Log Loss, on peut montrer qu'elle est bornÃ©e infÃ©rieurement par zÃ©ro (puisque le logarithme d'un nombre positif est toujours positif) et qu'elle atteint cette borne infÃ©rieure lorsque les prÃ©dictions sont parfaitement correctes (c'est-Ã -dire lorsque la probabilitÃ© prÃ©dite pour la classe correcte est Ã©gale Ã  1).\n",
    "\n",
    "En combinant la convexitÃ© de la fonction Log Loss avec le fait qu'elle est bornÃ©e infÃ©rieurement et qu'elle atteint sa borne infÃ©rieure, on peut conclure que la fonction atteint un minimum global lorsque les prÃ©dictions sont parfaitement correctes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Log-Loss was determined ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-loss pÃ©nalise les erreurs de prÃ©diction en fonction de la probabilitÃ© estimÃ©e par le modÃ¨le.  \n",
    "La fonction de coÃ»t attribue une pÃ©nalitÃ© plus Ã©levÃ©e aux exemples pour lesquels la prÃ©diction est loin de la valeur rÃ©elle, c'est-Ã -dire lorsque la probabilitÃ© estimÃ©e est proche de 0 ou de 1 alors que la valeur rÃ©elle est respectivement 1 ou 0.\n",
    "\n",
    "- Si la valeur rÃ©elle $y_i$ est Ã©gale Ã  1 (c'est-Ã -dire que l'exemple appartient Ã  la classe positive), la pÃ©nalitÃ© est Ã©gale Ã  $-log(h_\\theta(x_i))$, oÃ¹ $h_\\theta(x_i)$ est la probabilitÃ© estimÃ©e par le modÃ¨le que l'exemple appartienne Ã  la classe positive. Plus la probabilitÃ© estimÃ©e $(h_\\theta(x_i))$ est proche de 0 (et donc s'Ã©loigne de l'observation'), plus la pÃ©nalitÃ© est Ã©levÃ©e ($-\\log(0^+) \\approx +\\infty$).  \n",
    " \n",
    "- Si la valeur rÃ©elle $y_i$ est Ã©gale Ã  0 (c'est-Ã -dire que l'exemple appartient Ã  la classe nÃ©gative), la pÃ©nalitÃ© est Ã©gale Ã  $-log(1 - h_\\theta(x_i))$, oÃ¹ $h_\\theta(x_i)$ est la probabilitÃ© estimÃ©e par le modÃ¨le que l'exemple appartienne Ã  la classe positive. Plus la probabilitÃ© estimÃ©e est proche de 1, plus la pÃ©nalitÃ© est Ã©levÃ©e ($-\\log(1 - 1^-) \\approx +\\infty$).  \n",
    "\n",
    "On rÃ©sume les deux conditions \"si\" prÃ©cÃ©dentes en une seule formule : $y_i * log(h_\\theta(x_i)) + (1 - y_i) * log(1 - h_\\theta(x_i))$ pour l'observation $i$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L'objectif de l'apprentissage d'un modÃ¨le de rÃ©gression logistique est de minimiser la fonction de coÃ»t en ajustant les paramÃ¨tres $\\theta$.**\n",
    "\n",
    "Pour ce faire, on utilisera une mÃ©thode d'optimisation itÃ©rative, **la descente de gradient**, pour trouver les valeurs de $\\theta$ qui minimisent la fonction de coÃ»t sur l'ensemble d'entraÃ®nement.\n",
    "\n",
    "La fonction de coÃ»t de la rÃ©gression logistique est alors dÃ©finie comme la moyenne des pÃ©nalitÃ©s sur tous les exemples de l'ensemble d'entraÃ®nement.\n",
    "\n",
    "En minimisant la fonction de coÃ»t, on cherche Ã  trouver les paramÃ¨tres $\\theta$ qui permettent de minimiser la somme des pÃ©nalitÃ©s $\\underset{\\theta} min(J(\\theta))$  sur tous les exemples, c'est-Ã -dire de maximiser la probabilitÃ© d'observer les donnÃ©es d'entraÃ®nement en fonction des paramÃ¨tres du modÃ¨le."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computation of the gradient of the cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient of $J(\\theta) = \\frac{\\partial}{\\partial \\theta}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by transforming the expression of  $J(\\theta) = -\\frac{1}{n} \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$\n",
    "\n",
    "$ \\log(h_\\theta(x_i)) = \\log(\\frac{1}{1 + \\exp(-\\theta^Tx_i)}) = -\\log(1 + \\exp(-\\theta^Tx_i))$  \n",
    "\n",
    "and \n",
    "\n",
    "$ \\log(1 - h_\\theta(x_i)) = \\log(1 - \\frac{1}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\frac{1 + \\exp(-\\theta^Tx_i) - 1}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\frac{\\exp(-\\theta^Tx_i)}{1 + \\exp(-\\theta^Tx_i)}) = \\log(\\exp(-\\theta^Tx_i)) - \\log({1 + \\exp(-\\theta^Tx_i)}) = -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log(1 + exp(-\\theta^Tx_i)))]$\n",
    "\n",
    "$ \\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) -\\theta^Tx_i - \\log(1 + \\exp(-\\theta^Tx_i)) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$\n",
    "\n",
    "$ \\iff J(\\theta) = -\\frac{1}{n} \\sum[- y_i \\log(1 + \\exp(-\\theta^Tx_i)) -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$\n",
    "\n",
    "$ \\iff J(\\theta) = -\\frac{1}{n} \\sum[\\cancel{- y_i \\log(1 + \\exp(-\\theta^Tx_i))} -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + \\cancel{y_i \\log(1 + \\exp(-\\theta^Tx_i))}]$\n",
    "\n",
    "$ \\iff J(\\theta) = -\\frac{1}{n} \\sum[-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  ]$\n",
    "\n",
    "$ \\iff J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) ]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with:\n",
    "\n",
    "$   -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = - \\log(\\exp(\\theta^T x_i)) - \\log(1 + \\exp(-\\theta^Tx_i)) = -(\\log(\\exp(\\theta^T x_i)) + \\log(1 + \\exp(-\\theta^Tx_i))) $  \n",
    "\n",
    "$= -\\log[\\exp(\\theta^T x_i)(1 + \\exp(-\\theta^Tx_i))] = -\\log(\\exp(\\theta^T x_i + 1)) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(\\exp(\\theta^T x_i + 1)) ]$\n",
    "\n",
    "$ \\Rightarrow J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(1 + \\exp(\\theta^T x_i)) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Rightarrow  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i)  - \\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that: $\\theta^Tx_i = \\theta_1 {x_i}^{(1)} + \\theta_2 {x_i}^{(2)} + \\ldots + \\theta_k {x_i}^{(k)}$\n",
    "\n",
    "$ \\Rightarrow   \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i) = x_i^{(j)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:  \n",
    "\n",
    "$ \\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) \\underset{\\log(u)^{'} = \\frac{u^{'}}{u}} = \\frac{\\frac{\\partial}{\\partial \\theta_j}(1 + \\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = \\frac{\\frac{\\partial}{\\partial \\theta_j}(\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} \\underset{\\exp(u)^{'} = u^{'}\\exp(u)} =  \\frac{\\frac{\\partial}{\\partial \\theta_j}(\\theta^T x_i) * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} $  \n",
    "\n",
    "$=  \\frac{x_i^{(j)} * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = x_i^{(j)} h_\\theta(x_i) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Rightarrow  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i x_i^{(j)}  - x_i^{(j)} h_\\theta(x_i) ] = -\\frac{1}{n} \\sum[y_i - h_\\theta(x_i) ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\iff  \\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sachant que l'expression de la descente de Gradient pour mettre Ã  jour les pondÃ©rations est pour le poids $\\theta_j$ :\n",
    "\n",
    "$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oÃ¹ $\\alpha$ est le learning rate, on obtient:\n",
    "\n",
    "$\\theta_j = \\theta_j - \\frac{\\alpha}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Algorithm steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $\\theta = (w,b)$   \n",
    "\n",
    "with $h_\\theta(x) = \\frac{1}{1 + \\exp(-w x + b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize weights as zero\n",
    "- Initialize bias as zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict result by using $\\hat{y} = \\frac{1}{1 + \\exp(-wx+b)}$\n",
    "- Calculate the error\n",
    "- Use Gradient descent to figure out new weights and bias values\n",
    "- Repeat n times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data point:  \n",
    "- Put the values from the data point into the equation $\\hat{y} = \\frac{1}{1 + \\exp(-w+b)}$\n",
    "- Choose the label based on the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs:\n",
    "\n",
    "$X=\n",
    "\\begin{pmatrix}\n",
    "   x_{1,1} & \\ldots & x_{1,k} \\\\\n",
    "   x_{2,1} & \\ldots & x_{2,k} \\\\ \n",
    "   \\ldots & x_{i,j} & \\ldots \\\\ \n",
    "   x_{n,1} & \\ldots & x_{n,k} \n",
    "\\end{pmatrix}\n",
    "$\n",
    ",\n",
    "$\n",
    "w=\n",
    "\\begin{pmatrix}\n",
    "   w_1 \\\\\n",
    "   w_2 \\\\ \n",
    "   \\ldots \\\\ \n",
    "   w_k \n",
    "\\end{pmatrix}\n",
    "$\n",
    ",\n",
    "$b = constant$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model:\n",
    "\n",
    "$X.w + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=$\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "   x_{1,1} & \\ldots & x_{1,k} \\\\\n",
    "   x_{2,1} & \\ldots & x_{2,k} \\\\ \n",
    "   \\ldots & x_{i,j} & \\ldots \\\\ \n",
    "   x_{n,1} & \\ldots & x_{n,k} \n",
    "\\end{pmatrix}\n",
    "$\n",
    ".\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "   w_1 \\\\\n",
    "   w_2 \\\\ \n",
    "   \\ldots \\\\ \n",
    "   w_k \n",
    "\\end{pmatrix}\n",
    "$\n",
    "+\n",
    "$b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$=\n",
    "\\begin{pmatrix}\n",
    "   x_{1,1}w_1 + & \\ldots & + x_{1,k}w_k + b \\\\\n",
    "   x_{2,1}w_1 + & \\ldots & + x_{2,k}w_k + b \\\\ \n",
    "   \\ldots     &   \\ldots &   \\ldots \\\\ \n",
    "   x_{n,1}w_1 + & \\ldots & + x_{n,k}w_k + b \n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction (output) is given by:\n",
    "\n",
    "$sigmoid(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p} = h_\\omega(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updates of the weights and bias are given by:\n",
    "\n",
    "$\\omega_j = \\omega_j - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ] x_{i,j}$  \n",
    "\n",
    "$b = b - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\omega$ using linear algebra formula:\n",
    "\n",
    "$\\omega = X^t.(\\hat{p} - y)$  \n",
    "\n",
    "$=$\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "   x_{1,1} & \\ldots & x_{1,n} \\\\\n",
    "   x_{2,1} & \\ldots & x_{2,n} \\\\ \n",
    "   \\ldots & x_{i,j} & \\ldots \\\\ \n",
    "   x_{k,1} & \\ldots & x_{k,n} \n",
    "\\end{pmatrix}\n",
    "$\n",
    ".\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "   \\hat{p_1} - y_1 \\\\\n",
    "   \\hat{p_2} - y_2 \\\\ \n",
    "   \\ldots \\\\ \n",
    "   \\hat{p_n} - y_n \n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $b$ using linear algebra formula:\n",
    "\n",
    "$b = \\sum(\\hat{p} - y)$  \n",
    "\n",
    "$= \\sum\n",
    "\\begin{pmatrix}\n",
    "   \\hat{p_1} - y_1 \\\\\n",
    "   \\hat{p_2} - y_2 \\\\ \n",
    "   \\ldots \\\\ \n",
    "   \\hat{p_n} - y_n \n",
    "\\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and bias are given by:\n",
    "\n",
    "$sigmoid(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â The Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
    "        '''Initiate the constructor\n",
    "            INPUT:\n",
    "                learning_rate: magnitude of the step\n",
    "                n_iter: number of iterations\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Train the model\n",
    "        INPUTS:\n",
    "            X: the dataset of the features\n",
    "            y: the target\n",
    "        OUTPUTS:\n",
    "            The model\n",
    "        '''\n",
    "\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # initialize the parameters:\n",
    "        self.weights = np.zeros(self.n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            return self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        '''Update of the weights with Gradient descent'''\n",
    "\n",
    "        # we compute the prediction (the probability)\n",
    "        y_pred = 1 / (1 + np.exp( - (np.dot(self.X, self.weights) + self.bias)))\n",
    "\n",
    "        # update the weights:\n",
    "        # w_j = w_j - (alpha / n) * S(p_hat - y_i)xij\n",
    "        # b = b - (alpha / n) * S(p_hat - y_i)\n",
    "        dw = (1 / self.n_samples) * np.dot(self.X.T, (y_pred - self.y))\n",
    "        db = (1 / self.n_samples) * np.sum(y_pred - self.y)\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate*dw\n",
    "        self.bias = self.bias - self.learning_rate*db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = 1 / (1 + np.exp( - (X.dot(self.weights) + self.bias)))\n",
    "        y_pred = np.where(y_pred > 0.5 , 1 , 0)\n",
    "        return y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "rep = '/Users/davidtbo/Documents/Data_Science/99_Data'\n",
    "\n",
    "filename = os.path.join(rep, 'diabetes.csv')\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(features)\n",
    "\n",
    "standardized_data = scaler.transform(features)\n",
    "\n",
    "features = standardized_data\n",
    "target= df['Outcome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Testing the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Logistic_Regression(learning_rate=0.01, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.754071661237785"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "training_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7272727272727273"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The person is diabetic\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/base.py:439: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Predictive system\n",
    "\n",
    "input_data = (5, 166, 72, 19, 175, 25.8, 0.587, 51)\n",
    "\n",
    "# to numpy array\n",
    "input_data_array = np.asarray(input_data)\n",
    "\n",
    "# Reshape\n",
    "input_data_reshape = input_data_array.reshape(1, -1)\n",
    "\n",
    "# Standardized the data\n",
    "input_data_std = scaler.transform(input_data_reshape)\n",
    "\n",
    "pred = classifier.predict(input_data_std)\n",
    "\n",
    "if pred:\n",
    "    print('The person is diabetic')\n",
    "else:\n",
    "    print('The person is not diabetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_breast_cancer()\n",
    "X, y = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = sum(y_true==y_pred)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Logistic_Regression(learning_rate=0.0001)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3986013986013986"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Demonstration:** The coefficient $\\beta_1$â€‹ represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$â€‹ quantitative feature.  \n",
    "\n",
    "Notations:  \n",
    "* $\\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))$ \n",
    "* $\\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))$   \n",
    "\n",
    "We know that:  \n",
    "\n",
    "* $\\text{log(Odds)}(Y=1|X=x+1)=\\beta_0 + \\beta_1 \\times (x+1)$\n",
    "* $\\text{log(Odds)}(Y=1|X=x)=\\beta_0 + \\beta_1 \\times x$   \n",
    "\n",
    "By difference:  \n",
    "* $\\text{log(Odds)}(Y=1|X=x+1) - \\text{log(Odds)}(Y=1|X=x) =\\beta_0 + \\beta_1 \\times (x+1) - (\\beta_0 + \\beta_1 \\times x) = \\beta_1$  \n",
    "\n",
    "* $\\text{log}\\left(\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)}\\right) =\\beta_1$  \n",
    "\n",
    "**CQFD**  \n",
    "\n",
    "Note:  \n",
    "\n",
    "* $\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)} = \\exp(\\beta_1)$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Newton-Raphson algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compute the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme de la mÃ©thode de Newton-Raphson est utilisÃ© pour trouver les coefficients de la rÃ©gression logistique en maximisant la fonction de vraisemblance.  \n",
    "La rÃ©gression logistique est un modÃ¨le de rÃ©gression utilisÃ© pour prÃ©dire la probabilitÃ© d'un Ã©vÃ©nement binaire en fonction d'une ou plusieurs variables prÃ©dictives.  \n",
    "\n",
    "Dans le cas de la rÃ©gression logistique, la fonction de vraisemblance est une fonction convexe et peut Ãªtre maximisÃ©e Ã  l'aide de l'algorithme de Newton-Raphson.   \n",
    "L'algorithme de Newton-Raphson est une mÃ©thode itÃ©rative pour trouver le maximum d'une fonction en utilisant la dÃ©rivÃ©e et la dÃ©rivÃ©e seconde de la fonction.\n",
    "\n",
    "Dans le cas de la rÃ©gression logistique, la fonction de vraisemblance est donnÃ©e par:\n",
    "\n",
    "$L(\\omega | X, y) = prod(p(yi | xi, beta)^{yi} * (1 - p(yi | xi, beta))^{(1 - yi)})$\n",
    "\n",
    "oÃ¹ $\\omega$ est le vecteur de coefficients de la rÃ©gression logistique, $X$ est la matrice de variables prÃ©dictives, $y$ est le vecteur de variables rÃ©ponses binaires et $p(yi | xi, \\omega)$ est la probabilitÃ© prÃ©dite de l'Ã©vÃ©nement binaire pour l'observation $i$.\n",
    "\n",
    "Pour maximiser la fonction de vraisemblance, nous pouvons utiliser l'algorithme de Newton-Raphson. Ã€ chaque itÃ©ration, l'algorithme met Ã  jour le vecteur de coefficients beta en utilisant la formule suivante:\n",
    "\n",
    "\n",
    "$\\omega_{i+1} = \\omega_i - H^{-1} * g$  \n",
    "\n",
    "oÃ¹  \n",
    "\n",
    "- $H = \\frac{\\partial^2L}{\\partial \\omega \\partial\\omega'}$ est la matrice hessienne de la fonction de vraisemblance,  \n",
    "\n",
    "- $g = \\frac{\\partial L}{\\partial \\omega }$ est le vecteur gradient de la fonction de vraisemblance et  \n",
    "\n",
    "- $\\omega_i$ est le vecteur de coefficients Ã  l'itÃ©ration $i$.\n",
    "\n",
    "La matrice hessienne et le vecteur gradient de la fonction de vraisemblance peuvent Ãªtre calculÃ©s Ã  l'aide des dÃ©rivÃ©es partielles de la fonction de vraisemblance par rapport aux coefficients beta.\n",
    "\n",
    "Donc, dans le cas de la rÃ©gression logistique, l'algorithme de la mÃ©thode de Newton-Raphson est utilisÃ© pour trouver les coefficients de la rÃ©gression logistique en maximisant la fonction de vraisemblance.  \n",
    "\n",
    "Cela permet d'estimer les probabilitÃ©s de l'Ã©vÃ©nement binaire en fonction des variables prÃ©dictives.  \n",
    "\n",
    "NB: les itÃ©rations sont interrompues lorsque la diffÃ©rence entre deux vecteurs de solution successifs est nÃ©gligeable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients significativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wald statistic allows to test the coefficients significativity $\\hat{w_j}$. Wald statistic is given by::    \n",
    "\n",
    "$(\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2$  \n",
    "\n",
    "Under $H_0 : \\{\\hat{w_j} = 0 \\} \\Longrightarrow \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} $ ~ $\\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added-value of the variable $X_j$ is only real if the Wald statistic > 4 $(3.84 = 1.96^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Wald > 4$    \n",
    "\n",
    "$\\iff (\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2 > 4$  \n",
    "\n",
    "$\\iff \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} > 2$  \n",
    "\n",
    "$\\iff \\hat{w_j} > 2\\sigma(\\hat{w_j}) $  \n",
    "\n",
    "$\\iff \\hat{w_j} - 2\\sigma(\\hat{w_j}) > 0$  \n",
    "\n",
    "$\\iff \\hat{w_j}$ se trouve Ã  plus de 2 Ã©carts-type de 0  \n",
    "\n",
    "$\\iff $ l'intervalle de confiance de $\\hat{w_j}$ ne contient pas 0 Ã  95%  \n",
    "\n",
    "CQFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality mesure (Deviance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cf. S.TuffÃ©ry p.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n:$ number of observations  \n",
    "$k:$ number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\omega_k)$ Likelihood of the \"modÃ¨le ajustÃ©\"  \n",
    "\n",
    "$L(\\omega_0)$ Likelihood of the \"modÃ¨le rÃ©duit Ã  la constante\"  \n",
    "\n",
    "$L(\\omega_{max})$ Likelihood of the \"modÃ¨le saturÃ©\". The one the model will compare.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance formula:  \n",
    "\n",
    "$D(\\omega_k) = -2[log(L(\\omega_k)) - log(L(\\omega_{max}))]$  $^{(*)}$\n",
    "\n",
    "As the target is 0 or 1 $\\Longrightarrow L(\\omega_{max})=1 \\Longrightarrow log(L(\\omega_{max}))=0$  \n",
    "\n",
    "$\\Longrightarrow D(\\omega_k) = -2[log(L(\\omega_k))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) $D(\\omega_k) = (\\frac{log(L(\\omega_k))}{log(L(\\omega_{max}))}^2)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance is equivalent to the SCE for the linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

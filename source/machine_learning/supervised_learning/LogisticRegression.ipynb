{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Logistic regression applies to cases where:\n",
    "\n",
    "* $Y$ is a random qualitative variable with 2 categories (a binary variable by convention, $Y = 0$ if the event does not occur, and $Y = 1$ if it does),\n",
    "* $X_1,\\ldots,X_k$ are non-random qualitative or quantitative variables ($K$ explanatory variables in total).\n",
    "\n",
    "* $(Y, X_1,\\ldots,X_k)$ represent the population variables, from which a sample of $n$ individuals $(i)$ is drawn, and $(y, x_i)$ is the vector of observed realizations of $(Y_i, X_i)$ for each individual in the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike simple linear regression, logistic regression estimates **the probability** of an event occurring, rather than predicting a specific numerical value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $Y_i$ follow a Bernoulli distribution with parameter $p_i$ representing the probability that $Y_i=1$.    \n",
    "\n",
    "$$Y_i \\sim B(p_i)$$\n",
    "\n",
    "\n",
    "$$P(Y_i=1) = p_i \\quad, \\quad P(Y_i = 0) = 1 - p_i$$\n",
    "\n",
    "which is equivalent to: \n",
    "\n",
    "$$P(Y_i = k) = {p_i}^k(1 - p_i)^{1-k} \\quad \\text{for k} \\in \\{0, 1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The linear LOGIT model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that the expected value of $Y, E(Y)$, only takes values between 0 and 1, we use the logistic function:  \n",
    "\n",
    "$$f(x) = \\dfrac{\\text{exp(x)}}{1 + \\text{exp(x)}} = p$$\n",
    "\n",
    "or similarly:  \n",
    "\n",
    "$$f(x) = \\dfrac{1}{1 + \\text{exp(-x)}} = p$$\n",
    "\n",
    "This guarantees that $0 < f(x) < 1$, so $E[Y]$ can represent a valid probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logit function is used to transform a probability $p$ into an **unrestricted real value**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\quad \\text{Notations:} \\quad X = (1,X_1, \\ldots, X_k) \\quad \\text{and} \\quad \\beta = (\\beta_0,\\beta_1, \\ldots, \\beta_k)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1 - p})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta .X$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{log}\\left( \\dfrac{p}{1-p} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\ldots + \\beta_k X_k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p = \\frac{1}{1 + \\exp(-\\beta .X)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration: \n",
    "\n",
    "$$p(x) = \\dfrac{1}{1 + \\exp(-\\beta x)}$$\n",
    "\n",
    "$$\\underset{inverse}   \\iff \\dfrac{1}{p} = 1 + \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - 1 = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1}{p} - \\dfrac{p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\dfrac{1-p}{p} = \\exp(-\\beta x)$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{1-p}{p}) = -\\beta x$$\n",
    "\n",
    "$$\\iff \\log(\\dfrac{p}{1-p}) = \\beta x$$\n",
    "\n",
    "To simplify the writing we have put $p$ rather than $p(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Assumptions for Generalizability of the logit model\n",
    "\n",
    "* **Linearity of Log-Odds:** The relationship between each continuous predictor and the log-odds of $Y=1$ is linear. If this assumption is violated (e.g., non-linear effects), the interpretation of $\\beta_1$â€‹ may not hold.  \n",
    "* **No Multicollinearity:** Predictors should not be highly correlated, as this can distort the interpretation of individual coefficients.  \n",
    "* **Additivity:** The effect of each predictor on the log-odds is additive. There should be no significant interaction effects unless explicitly modeled.  \n",
    "* **Independence of Observations:** The model assumes that observations are independent of each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Odds**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds are defined by:  \n",
    "\n",
    "$$\\text{Odds} = \\dfrac{p}{1-p}$$\n",
    "\n",
    "\n",
    "$\\text{Where} \\quad p = P(target=1|X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">_If a student has a 3 in 4 chance of passing and a 1 in 4 chance of failing, their odds are '3 to 1':_ $\\text{Odds} = \\dfrac{3/4}{1/4}=3$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Notation:**\n",
    "$$\\text{Odds}(Y=1|X=0)=\\dfrac{P(Y=1|X=0)}{1-P(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **The Odds Ratio**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The odds ratio comparing the **probability of $target=1$** between individuals with value $X$ and those without it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{\\text{Odds}(Y=1|X=1)}{\\text{Odds}(Y=1|X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Odds Ratio} = \\dfrac{P(Y_i=1 | X=1)}{1 - P(Y_i=1 | X=1)} / \\dfrac{P(Y_i=1 | X=0)}{1 - P(Y_i=1 | X=0)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that logit is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{logit}(p) = \\text{log}(\\dfrac{p}{1-p}) = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\ldots + \\beta_k x_{ik}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting the Intercept**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intercept $\\beta_0$â€‹ represents the **log-odds of the outcome $Y=1$ when all predictors are equal to zero**.  \n",
    "$\\beta_0$â€‹ defines the **baseline probability** of the outcome when all predictors are zero.  \n",
    "\n",
    "âš ï¸ **Caveat**:  \n",
    "This interpretation of $\\beta_0$ is often not meaningful if some predictors cannot logically be zero (e.g., age=0, blood pressure). In such cases, $\\beta_0$â€‹ is primarily a mathematical component of the model and is rarely interpreted in isolation.  \n",
    "  \n",
    "  \n",
    "* **Odds for the baseline group:**  \n",
    "\n",
    "$$\\text{Odds}(Y=1âˆ£X_1=0)=\\expâ¡(\\beta_0)$$\n",
    "\n",
    "* **Probability for the baseline group:**\n",
    "$$P(Y=1âˆ£X_1=0)=\\dfrac{\\expâ¡(\\beta_0)}{1 + \\exp(\\beta_0)}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">If $X_1$â€‹ is \"smoking status\" ($0$ = non-smoker, $1$ = smoker), then \n",
    ">* $\\beta_0$â€‹ gives the **log-odds** of the outcome for non-smokers  \n",
    ">* $\\exp(\\beta_0)$ gives the **odds** of the outcome for non-smokers.  \n",
    ">\n",
    ">If $\\beta_0 = -1$, then: $$\\exp(\\beta_0) = \\exp(-1) \\approx 0.37$$\n",
    ">\n",
    ">$$P(Y=1âˆ£X_1=0)= \\dfrac{0.37}{1 + 0.37} \\approx 0.27$$\n",
    ">\n",
    ">27% of non-smokers are predicted to have the outcome (e.g., lung cancer), assuming no other predictors.  \n",
    ">It is the observed proportion of lung cancer for non-smokers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Interpreting the Slope**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a model with multiple predictors, each $\\beta_i$â€‹ (and its corresponding odds ratio $\\exp(\\beta_i)$ represents the effect of that predictor on the log-odds of $Y=1$, holding all other predictors constant.  \n",
    "This is the key assumption of multivariable regression: ceteris paribus (all else being equal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient $\\beta_1$â€‹ represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$â€‹. The odds ratio $\\exp(\\beta_1)$â€‹ quantifies how the odds of $Y=1$ change with $X_1$â€‹.\n",
    "\n",
    "**General Formula for Odds Ratio**\n",
    "\n",
    "For any type of predictor $X_1$, the odds ratio for a one-unit increase is:  \n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)$$\n",
    "\n",
    "ðŸ“Œ **Note:**  \n",
    "$\\exp(\\beta_1â€‹)$ compares the odds of $Y=1$ between $X_1=1$ and $X_1=0$, controlling for all other variables in the model (all others features constant).\n",
    "\n",
    "* Case: $X_1$ is Binary\n",
    "\n",
    "For a binary predictor $X_1$â€‹ (e.g., $0$ = non-smoker, $1$ = smoker), the odds ratio $\\exp(\\beta_1)$â€‹ compares the odds of $Y=1$ between the two groups.\n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 1_{\\{X_1 = 1\\}}â€‹$$\n",
    "\n",
    "* **Odds ratio:**\n",
    "$$\\text{Odds Ratio} = \\dfrac{P(Y=1 | X_1=1)}{1 - P(Y=1 | X_1=1)} / \\dfrac{P(Y=1 | X_1=0)}{1 - P(Y=1 | X_1=0)} = \\exp(\\beta_1)$$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "* If $\\exp(\\beta_1) = 1$: No effect of the feature $X_1$â€‹ on the odds of $Y=1$.\n",
    "* If $\\exp(\\beta_1)>1$: The odds of $Y=1$ are higher when $X_1â€‹=1$. The feature $X_1$â€‹ is **positively associated** with the outcome.\n",
    "* If $\\exp(\\beta_1) < 1$: The odds of $Y=1$ are lower when $X_1â€‹=1$. The feature $X_1$â€‹ is **negatively associated** with the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Example:**  \n",
    ">If $\\beta_1 = 0.7 \\rightarrow \\exp(\\beta_1) \\approx 2.01$. The odds of lung cancer for smokers $(X_1=1)$ are twice as high as for non-smokers $(X_1=0)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Case: $X_1$ is Categorical\n",
    "\n",
    "For a categorical predictor $X_1$ with more than two levels (e.g., color = red, green, blue), you use **dummy variables**. \n",
    "\n",
    "* **The logistic regression model becomes:**\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}1_{\\{X_1 = \\text{green}\\}} + \\beta_{blue}1_{\\{X_1 = \\text{blue}\\}}$$\n",
    "\n",
    "* **Reference Category (\"red\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0$$\n",
    "\n",
    "**Interpretation:** This means $\\beta_0$â€‹ represents the log-odds of $Y=1$ for the reference category (\"red\").\n",
    "\n",
    "* **Category (\"green\"):** When $1_{\\{X_1 = \\text{green}\\}}=1$ and $1_{\\{X_1 = \\text{blue}\\}}=0$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{green}$$\n",
    "\n",
    "* **Category (\"blue\"):** When $1_{\\{X_1 = \\text{green}\\}}=0$ and $1_{\\{X_1 = \\text{blue}\\}}=1$, the log-odds are:\n",
    "\n",
    "$$\\text{log}\\left( \\dfrac{P(Y=1)}{1 - P(Y=1)} \\right) = \\beta_0 + \\beta_{blue}$$\n",
    "\n",
    "The **odds ratio for \"blue\" relative to the reference \"red\"** is:\n",
    "$$\\exp(\\beta_{blue}) = \\dfrac{\\text{Odds}(Y=1 | \\text{blue})}{\\text{Odds}(Y=1 | \\text{red})}$$\n",
    "\n",
    "The same way, $\\exp(\\beta_{\\text{green}})$â€‹ compares the odds for \"green\" vs. the \"red\" reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Interpretation:**\n",
    ">\n",
    ">If $\\exp(\\beta_{\\text{green}})â€‹=1.5$, the odds of $Y=1$ are $1.5$ times higher for \"green\" compared to \"red\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ##### Case: $X_1$ is Quantitative\n",
    "\n",
    "For a continuous predictor $X_1$ (e.g., age, blood pressure), the odds ratio $\\exp(\\beta_1)$â€‹ represents the multiplicative change in the odds of $Y=1$ for a one-unit increase in $X_1$â€‹.  \n",
    "\n",
    "* **Logistic regression equation:**\n",
    "\n",
    "$$\\log\\left(\\dfrac{P(Y=1 | X_1)}{1 - P(Y=1 | X_1)}\\right) = \\beta_0 + \\beta_1 X_1â€‹$$\n",
    "\n",
    "* **Odds ratio for a one-unit increase:**\n",
    "\n",
    "$$\\text{Odds Ratio} = \\frac{\\text{Odds}(Y=1 | X_1 = x+1)}{\\text{Odds}(Y=1 | X_1 = x)} = \\exp(\\beta_1)â€‹$$\n",
    "\n",
    "**In short**: $\\beta_1$â€‹ captures the **constant log-odds** change per unit increase in $X_1$, so $\\exp(\\beta_1)$â€‹ is the **odds ratio** for that one-unit change.\n",
    "\n",
    "This holds regardless of the starting value of $X_1$â€‹ because the model assumes a constant multiplicative effect on the odds (a key assumption of logistic regression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">**Interpretation:**\n",
    ">\n",
    ">* If $\\beta_1=0.095 \\rightarrow \\exp(\\beta_1)=1.1$, the odds of $Y=1$ increase by 10% for each one-unit increase in $X_1$â€‹.\n",
    ">* If $X_1$â€‹ is \"years of smoking\" and $\\beta_1 = 0.7 \\rightarrow  \\exp(\\beta_1) \\approx 2.01$. For each additional year of smoking, the odds of lung cancer double.\n",
    ">."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**\n",
    "\n",
    "| Type of $X_1$         | Interpretation of $\\exp(\\beta_1)$                                       |\n",
    "|-----------------------|-------------------------------------------------------------------------|\n",
    "| Binary                | Compares odds of $Y=1$ between $X_1=1$ and $X_1=0$                      |\n",
    "| Categorical           | Compares odds of $Y=1$ for a given category relative to the reference.  |\n",
    "| Quantitative          | Multiplicative change in odds of $Y=1$ for a one-unit increase in $X_1$ |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "rep = '/Users/davidtbo/Library/Mobile Documents/com~apple~CloudDocs/data/external'\n",
    "\n",
    "filename = os.path.join(rep, 'diabetes.csv')\n",
    "\n",
    "df = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bloodpressure</th>\n",
       "      <th>skinthickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>diabetespedigreefunction</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnancies  glucose  bloodpressure  skinthickness  insulin   bmi  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   diabetespedigreefunction  age  outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = [str.lower(col) for col in df.columns.str.strip()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "features = df.drop('outcome', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   pregnancies   glucose  bloodpressure  skinthickness   insulin       bmi  diabetespedigreefunction       age\n",
      "0     0.639947  0.848324       0.149641       0.907270 -0.692891  0.204013                  0.468492  1.425995\n",
      "1    -0.844885 -1.123396      -0.160546       0.530902 -0.692891 -0.684422                 -0.365061 -0.190672\n",
      "2     1.233880  1.943724      -0.263941      -1.288212 -0.692891 -1.103255                  0.604397 -0.105584\n",
      "3    -0.844885 -0.998208      -0.160546       0.154533  0.123302 -0.494043                 -0.920763 -1.041549\n",
      "4    -1.141852  0.504055      -1.504687       0.907270  0.765836  1.409746                  5.484909 -0.020496\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# separate the features of the target\n",
    "features = df.drop('outcome', axis=1)\n",
    "target = df['outcome']\n",
    "\n",
    "# Data standardization\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(features)\n",
    "standardized_data = scaler.transform(features)\n",
    "\n",
    "# Reassign the names of the columns of origin\n",
    "standardized_df = pd.DataFrame(standardized_data, columns=features.columns)\n",
    "\n",
    "# Display the head of the standardized DataFrame\n",
    "print(standardized_df.head().to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully    (Exit mode 0)\n",
      "            Current function value: 0.46842232325133\n",
      "            Iterations: 79\n",
      "            Function evaluations: 95\n",
      "            Gradient evaluations: 79\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                outcome   No. Observations:                  614\n",
      "Model:                          Logit   Df Residuals:                      605\n",
      "Method:                           MLE   Df Model:                            8\n",
      "Date:                Fri, 10 Oct 2025   Pseudo R-squ.:                  0.2875\n",
      "Time:                        13:56:35   Log-Likelihood:                -286.64\n",
      "converged:                       True   LL-Null:                       -402.31\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.532e-45\n",
      "============================================================================================\n",
      "                               coef    std err          z      P>|z|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------------------\n",
      "const                       -8.4317      0.802    -10.513      0.000     -10.004      -6.860\n",
      "pregnancies                  0.1604      0.037      4.321      0.000       0.088       0.233\n",
      "glucose                      0.0375      0.004      8.812      0.000       0.029       0.046\n",
      "bloodpressure               -0.0137      0.006     -2.338      0.019      -0.025      -0.002\n",
      "skinthickness                0.0038      0.008      0.499      0.618      -0.011       0.019\n",
      "insulin                     -0.0014      0.001     -1.421      0.155      -0.003       0.001\n",
      "bmi                          0.0858      0.017      5.054      0.000       0.053       0.119\n",
      "diabetespedigreefunction     1.0054      0.326      3.087      0.002       0.367       1.644\n",
      "age                          0.0063      0.011      0.575      0.565      -0.015       0.028\n",
      "============================================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Add a constant for statsmodels\n",
    "X_train_sm = sm.add_constant(X_train)\n",
    "\n",
    "# Logistic Regression with L2 regularization\n",
    "logit_model = sm.Logit(y_train, X_train_sm)\n",
    "result = logit_model.fit_regularized(method='l1', alpha=0.1)\n",
    "\n",
    "# Display the summary\n",
    "print(result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const                       0.000218\n",
      "pregnancies                 1.174037\n",
      "glucose                     1.038217\n",
      "bloodpressure               0.986439\n",
      "skinthickness               1.003852\n",
      "insulin                     0.998607\n",
      "bmi                         1.089609\n",
      "diabetespedigreefunction    2.732962\n",
      "age                         1.006341\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "odds_ratios = np.exp(result.params)\n",
    "print(odds_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpreting the coefficients:**\n",
    "\n",
    "For each feature, the exponentiated coefficient (exp(coef)) represents the change in odds for a one-unit increase in that feature, holding all other features constant.\n",
    "\n",
    "For example, has the coefficient for 'pregnancies' is 0.1604, the odds ratio is exp(0.1604) â‰ˆ 1.174037.  \n",
    "This means that for each one-unit increase in pregnancies, the odds of the outcome occurring (e.g., having diabetes) increase by approximately 17.4%, assuming all other features remain constant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,8) and (9,) not aligned: 8 (dim 1) != 9 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[91]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Standardized the data\u001b[39;00m\n\u001b[32m     12\u001b[39m input_data_std = scaler.transform(input_data_reshape)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m pred = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pred:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mThe person is diabetic\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/website_rtd/.venv/lib/python3.13/site-packages/statsmodels/base/model.py:1174\u001b[39m, in \u001b[36mResults.predict\u001b[39m\u001b[34m(self, exog, transform, *args, **kwargs)\u001b[39m\n\u001b[32m   1127\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1128\u001b[39m \u001b[33;03mCall self.model.predict with self.params as the first argument.\u001b[39;00m\n\u001b[32m   1129\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1169\u001b[39m \u001b[33;03mreturned prediction.\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1171\u001b[39m exog, exog_index = \u001b[38;5;28mself\u001b[39m._transform_predict_exog(exog,\n\u001b[32m   1172\u001b[39m                                                 transform=transform)\n\u001b[32m-> \u001b[39m\u001b[32m1174\u001b[39m predict_results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                                     \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exog_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(predict_results,\n\u001b[32m   1178\u001b[39m                                           \u001b[33m'\u001b[39m\u001b[33mpredicted_values\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1179\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m predict_results.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Mobile Documents/com~apple~CloudDocs/website_rtd/.venv/lib/python3.13/site-packages/statsmodels/discrete/discrete_model.py:543\u001b[39m, in \u001b[36mBinaryModel.predict\u001b[39m\u001b[34m(self, params, exog, which, linear, offset)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    541\u001b[39m     exog = \u001b[38;5;28mself\u001b[39m.exog\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m linpred = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexog\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m + offset\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m which == \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cdf(linpred)\n",
      "\u001b[31mValueError\u001b[39m: shapes (1,8) and (9,) not aligned: 8 (dim 1) != 9 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Predictive system\n",
    "\n",
    "input_data = (5, 166, 72, 19, 175, 25.8, 0.587, 51)\n",
    "\n",
    "# to numpy array\n",
    "input_data_array = np.asarray(input_data)\n",
    "\n",
    "# Reshape\n",
    "input_data_reshape = input_data_array.reshape(1, -1)\n",
    "\n",
    "# Standardized the data\n",
    "input_data_std = scaler.transform(input_data_reshape)\n",
    "\n",
    "pred = result.predict(input_data)\n",
    "\n",
    "if pred:\n",
    "    print('The person is diabetic')\n",
    "else:\n",
    "    print('The person is not diabetic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APPENDIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the coefficients\n",
    "\n",
    "**Demonstration:** The coefficient $\\beta_1$â€‹ represents the **change in the log-odds of** $Y=1$ for a **one-unit change** in $X_1$â€‹ quantitative feature.  \n",
    "\n",
    "Notations:  \n",
    "$$\\text{Odds}(Y=1|X=x+1)=P(Y=1|X=x+1) / (1-P(Y=1|X=x+1))$$\n",
    "\n",
    "$$\\text{Odds}(Y=1|X=x)=P(Y=1|X=x) / (1-P(Y=1|X=x))$$\n",
    "\n",
    "We know that:  \n",
    "\n",
    "$$\\text{log(Odds)}(Y=1|X=x+1)=\\beta_0 + \\beta_1 \\times (x+1)$$\n",
    "\n",
    "$$\\text{log(Odds)}(Y=1|X=x)=\\beta_0 + \\beta_1 \\times x$$\n",
    "\n",
    "By difference:\n",
    "\n",
    "$$\\text{log(Odds)}(Y=1|X=x+1) - \\text{log(Odds)}(Y=1|X=x) =\\beta_0 + \\beta_1 \\times (x+1) - (\\beta_0 + \\beta_1 \\times x) = \\beta_1$$\n",
    "\n",
    "$$\\text{log}\\left(\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)}\\right) =\\beta_1$$\n",
    "\n",
    "**CQFD**\n",
    "\n",
    "Note:  \n",
    "\n",
    "$$\\dfrac{\\text{Odds}(Y=1|X=x+1)}{\\text{Odds}(Y=1|X=x)} = \\exp(\\beta_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model formulation\n",
    "\n",
    "The prediction $y_i=1$ of the logistic regression is defined:\n",
    "\n",
    "$$\\hat{y_i} = P(y_i=1 | x_i; \\theta) = \\frac{1}{1 + \\exp(-\\theta^Tx_i)} = h_{\\theta}(x_i)$$\n",
    "\n",
    "* If $y_i=1$, then $P(y_i|x_i; \\theta)=P(y_i=1|x_i; \\theta)$\n",
    "* If $y_i=0$, then $P(y_i|x_i; \\theta)=P(y_i=0|x_i; \\theta) = 1 - P(y_i=1|x_i; \\theta)$\n",
    "\n",
    "We can write these two equations into a single one:  \n",
    "\n",
    "$$P(y_i|x_i; \\theta)=P(y_i=1|x_i; \\theta)^{y_i}\\times (1 - P(y_i=1|x_i; \\theta))^{1-y_i}$$\n",
    "\n",
    "With the notations:\n",
    "\n",
    "$$P(y_i|x_i; \\theta)=h_{\\theta}(x_i)^{y_i}\\times (1 - h_{\\theta}(x_i))^{1-y_i}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood function\n",
    "\n",
    "The **likelihood** of the observations $y_i$ given the inputs $x_i$ and parameters $\\theta$ is defined as:  \n",
    "\n",
    "$$L(\\theta) = \\prod_{i=1}^n P(y_i|x_i;\\theta) = \\prod_{i=1}^n (h_{\\theta}(x_i))^{y_i} (1 -h_{\\theta}(x_i))^{1-y_i}$$\n",
    "\n",
    "where the prediction of the logistic regression is defined:\n",
    "\n",
    "$$h_{\\theta}(x_i) = P(y_i=1 | x_i; \\theta) = \\frac{1}{1 + \\exp(-\\theta^Tx_i)}$$\n",
    "\n",
    "The **log-likelihood** is defined as:  \n",
    "\n",
    "$$l(\\theta) = \\log(L(\\theta))=\\sum_{i=1}^n[y_i \\log (h_{\\theta}(x_i)) + (1 - y_i) \\log(1 - h_{\\theta}(x_i))]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective of Logistic Regession\n",
    "\n",
    "The goal of learning a **logistic regression model** is to **minimize the cost function** by adjusting the parameters $\\theta$.The cost function measures the average prediction error across all $n$ training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function (general definition)\n",
    "\n",
    "The cost function $J(\\theta)$ is defined as the average penalty for prediction errors across the training set. Mathematically, it is expressed as:\n",
    "\n",
    "$$J(Î¸) = -\\frac{1}{n} \\sum cost(h_\\theta(x_i), y_i)$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $h_\\theta(x_i)$ the model's prediction for sample $x_i$\n",
    "* $y_i$ the observed (true) value.\n",
    "\n",
    "Alternatively, it can be written as:\n",
    "\n",
    "$$J(Î¸) = \\frac{1}{n} \\sum cost(\\hat{y_i}, y_i)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\hat{y_i} = h_{\\theta}(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function **log loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, the cost function is called **log loss** (or logistic loss).  \n",
    "**Log loss** is derived from the log-likelihood $l(\\theta)$ and is defined as:\n",
    "\n",
    "$$J(\\theta) = -\\dfrac{1}{n}l(\\theta) = -\\frac{1}{n} \\sum(y_i \\log(h_\\theta(x_i)) + (1 - y_i) \\log(1 - h_\\theta(x_i)))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How Log-Loss penalizes prediction errors\n",
    "\n",
    "The log-loss function penalizes prediction errors based on the estimated probability from the model. It assigns higher penalties when predictions are far from the true labelsâ€”specifically:\n",
    "\n",
    "- If the label $y_i = 1$ (positive class), the penalty is $-log(h_{\\theta}(x_i))$. The closer $h_{\\theta}(x_i)$ is to 0 (far from the true label), the higher the penalty ($-\\log(0^+) \\approx + \\infty$).  \n",
    "\n",
    "- If the label $y_i = 0$ (negative class), the penalty is $-log(1-h_{\\theta}(x_i))$, the closer $h_{\\theta}(x_i)$ is to 1 (far from the true label), the higher the penalty ($-\\log(1 -1^+) \\approx + \\infty$).  \n",
    "\n",
    "The two cases are combined into a single formula for observation $i$:\n",
    "$$y_i log(h_\\theta(x_i)) + (1 - y_i) log(1 - h_\\theta(x_i))$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key insights:**\n",
    "\n",
    "* **Log loss** evaluates how well the model fits the training data.\n",
    "* The log loss is the negative average the log likelihood.\n",
    "* **Higher likelihood** leads to **lower log loss** (since $J(\\theta) = -\\frac{1}{n}l(\\theta)$).\n",
    "* The log-loss function heavily penalizes confident wrong predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By minimizing the cost function $J(\\theta)$, we aim to find the parameters $\\theta$ that maximize the likelihood of observing the training data given the model parameters.  \n",
    "\n",
    "To achieve this, we use an iterative optimization method, **gradient descent**, to find the values of $\\theta$ that minimize the cost function over the training set:\n",
    "\n",
    "$$\\underset{\\theta}{minimize}(J(\\theta))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter estimation methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent\n",
    "\n",
    "The gradient descent only uses the **gradient** (first derivative) of the cost function to update the parameters $\\theta$ in the opposite direction of the gradient, scaled by a learning rate $\\alpha$.  \n",
    "\n",
    "To compute the gradient $\\nabla J(\\theta)$ we start by transforming the expression of:\n",
    "\n",
    "$$\\log(h_\\theta(x_i)) = \\log\\left(\\frac{1}{1 + \\exp(-\\theta^Tx_i)}\\right) = -\\log(1 + \\exp(-\\theta^Tx_i))$$\n",
    "\n",
    "And:  \n",
    "\n",
    "$$\\log(1 - h_\\theta(x_i)) = \\log\\left(1 - \\frac{1}{1 + \\exp(-\\theta^Tx_i)}\\right)$$\n",
    "\n",
    "$$\\log(1 - h_\\theta(x_i)) = \\log\\left(\\frac{1 + \\exp(-\\theta^Tx_i) - 1}{1 + \\exp(-\\theta^Tx_i)}\\right)$$\n",
    "\n",
    "$$\\log(1 - h_\\theta(x_i)) = \\log\\left(\\frac{\\exp(-\\theta^Tx_i)}{1 + \\exp(-\\theta^Tx_i)}\\right)$$\n",
    "\n",
    "$$\\log(1 - h_\\theta(x_i)) = \\log(\\exp(-\\theta^Tx_i)) - \\log({1 + \\exp(-\\theta^Tx_i)})$$\n",
    "\n",
    "$$\\log(1 - h_\\theta(x_i)) = -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)})$$\n",
    "\n",
    "We integrate these modifications:  \n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}))]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) + (1 - y_i) (-\\theta^Tx_i - \\log(1 + exp(-\\theta^Tx_i)))]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[y_i (-\\log(1 + \\exp(-\\theta^Tx_i))) -\\theta^Tx_i - \\log(1 + \\exp(-\\theta^Tx_i)) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[- y_i \\log(1 + \\exp(-\\theta^Tx_i)) -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + y_i \\log(1 + \\exp(-\\theta^Tx_i))]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[\\cancel{- y_i \\log(1 + \\exp(-\\theta^Tx_i))} -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  + \\cancel{y_i \\log(1 + \\exp(-\\theta^Tx_i))}]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) + y_i \\theta^Tx_i  ]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) ]$$\n",
    "\n",
    "with:\n",
    "\n",
    "$$-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = - \\log(\\exp(\\theta^T x_i)) - \\log(1 + \\exp(-\\theta^Tx_i))$$\n",
    "\n",
    "$$-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = -(\\log(\\exp(\\theta^T x_i)) + \\log(1 + \\exp(-\\theta^Tx_i)))$$\n",
    "\n",
    "$$-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = -\\log[\\exp(\\theta^T x_i)(1 + \\exp(-\\theta^Tx_i))]$$\n",
    "\n",
    "$$-\\theta^Tx_i - \\log({1 + \\exp(-\\theta^Tx_i)}) = -\\log(\\exp(\\theta^T x_i) + 1)$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(\\exp(\\theta^T x_i + 1)) ]$$\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{n} \\sum[y_i \\theta^Tx_i  -\\log(1 + \\exp(\\theta^T x_i)) ]$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\frac{1}{n} \\sum[y_i \\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i)  - \\frac{\\partial}{\\partial \\theta_j}\\log(1 + \\exp(\\theta^T x_i)) ]$$\n",
    "\n",
    "Knowing that:\n",
    "\n",
    "$$\\theta^Tx_i = \\theta_1 {x_i}^{(1)} + \\theta_2 {x_i}^{(2)} + \\ldots + \\theta_k {x_i}^{(k)}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j} (\\theta^Tx_i) = x_i^{(j)}$$\n",
    "$$\\dfrac{\\partial}{\\partial \\theta_j}\\left(\\log(1 + \\exp(\\theta^T x_i))\\right) \\underset{\\log(u)^{'} = \\dfrac{u^{'}}{u}} = \\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(1 + \\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$$\n",
    "\n",
    "And:  \n",
    "\n",
    "$$\\dfrac{\\partial}{\\partial \\theta_j}\\left(\\log(1 + \\exp(\\theta^T x_i))\\right) \\underset{\\log(u)^{'} = \\dfrac{u^{'}}{u}} = \\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(1 + \\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$$\n",
    "$$\\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(1 + \\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = \\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$$\n",
    "$$\\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} \\underset{\\exp(u)^{'} = u^{'}\\exp(u)} =  \\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(\\theta^T x_i) * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$$\n",
    "$$\\dfrac{\\dfrac{\\partial}{\\partial \\theta_j}(\\theta^T x_i) * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = \\frac{x_i^{(j)} * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)}$$\n",
    "$$\\dfrac{x_i^{(j)} * (\\exp(\\theta^T x_i))} {1 + \\exp(\\theta x_i)} = x_i^{(j)} * h_\\theta(x_i)$$\n",
    "$$\\dfrac{\\partial}{\\partial \\theta_j}J(\\theta) = -\\dfrac{1}{n} \\sum[y_i x_i^{(j)}  - x_i^{(j)} h_\\theta(x_i)]$$\n",
    "\n",
    "$$-\\dfrac{1}{n} \\sum[y_i x_i^{(j)}  - x_i^{(j)} h_\\theta(x_i)] = -\\dfrac{1}{n} \\sum[y_i - h_\\theta(x_i) ] x_i^{(j)}$$\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\theta_j}J(\\theta) = \\frac{1}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$$\n",
    "\n",
    "We know that the expression of the Gradient descent to update the weights is for the weight $\\theta_j$ :  \n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j}J(\\theta)$$\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\frac{\\alpha}{n} \\sum[h_\\theta(x_i) - y_i ] x_i^{(j)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Newton-Raphson algorithm\n",
    "\n",
    "Others algorithms can be used to find the coefficients of the logistic regression. The Newton-Raphson algorithm is an alternative to the gradient descent.\n",
    "\n",
    "The Newton-Raphson method uses both the first derivative (gradient) and the second derivative (Hessian) of the cost function. It approximates the cost function as a quadratic form and finds the minimum of this approximation.  \n",
    "\n",
    "**Update rule**\n",
    "\n",
    "$$\\theta = \\theta - H^{-1} \\nabla J(\\theta)$$\n",
    "\n",
    "Logistic regression is a regression model used to predict the probability of a binary event based on one or more predictive variables. In logistic regression, the likelihood function is convex and can be maximized using the Newton-Raphson algorithm.  \n",
    "\n",
    "The Newton-Raphson algorithm is an iterative method for finding the maximum of a function using its first and second derivatives. For logistic regression, the likelihood function is given by:\n",
    "\n",
    "$L(\\theta | X, y) = \\prod(P(yi | x_i, \\theta)^{yi} (1 - P(y_i | x_i, \\theta))^{(1 - y_i)})$\n",
    "\n",
    "where: \n",
    "* $\\theta$ is the vector of logistic regression coefficients, \n",
    "* $X$ is the matrix of predictive variables, \n",
    "* $y$ is the vector of binary response variables, and\n",
    "* $P(y_i | x_i, \\theta)$ is the predicted probability of the binary event for observation $i$.\n",
    "\n",
    "To maximize the likelihood function, the Newton-Raphson algorithm updates the coefficient vector $\\theta$ at each iteration using the following formula:\n",
    "\n",
    "\n",
    "$\\theta_{i+1} = \\theta_i - H^{-1} . g$  \n",
    "\n",
    "where  \n",
    "\n",
    "- $H = \\dfrac{\\partial^2L}{\\partial \\theta \\partial\\theta'}$ is the Hessian matrix of the likelihood function.  \n",
    "\n",
    "- $g = \\dfrac{\\partial L}{\\partial \\theta }$ is the gradient vector of the likelihood function, and,  \n",
    "\n",
    "- $\\theta_i$ is the coefficient vector at iteration $i$. \n",
    "\n",
    "The Hessian matrix and gradient vector of the likelihood function are computed using the partial derivatives of the likelihood with respect to the coefficients $\\theta$.  \n",
    "\n",
    "Thus, in logistic regression, the Newton-Raphson algorithm is used to estimate the coefficients by maximizing the likelihood function. This allows the prediction of binary event probabilities based on the predictive variables.  \n",
    "\n",
    "**Note**: The iterations stop when the difference between two successive solution vectors becomes negligible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convexity\n",
    "\n",
    "Convexity is a crucial property in optimization, as it ensures that any local minimum is also a global minimum. This makes it easier to find the optimal solution using methods like gradient descent.  \n",
    "\n",
    "The Log Loss function is convex due to its logarithmic form, which is always convex for positive values. While convexity guarantees that a stationary point (where the derivative is zero) is a global minimum, it does not guarantee the existence of such a point.  \n",
    "\n",
    "To prove the existence of a minimum, the function must also be bounded below and attain this lower bound. For Log Loss, the function is bounded below by zero (since the logarithm of a positive number is always defined) and reaches this bound when predictions are perfectly accurate (i.e., the predicted probability for the correct class is 1).  \n",
    "\n",
    "Combining convexity with the fact that Log Loss is bounded below and attains its lower bound, we conclude that the function reaches a global minimum when predictions are perfectly correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to compute the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression from scratch with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Algorithm steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: $\\theta = (w,b)$   \n",
    "\n",
    "with $h_\\theta(x) = \\frac{1}{1 + \\exp(-w x + b)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize weights as zero\n",
    "- Initialize bias as zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given a data point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Predict result by using $\\hat{y} = \\frac{1}{1 + \\exp(-wx+b)}$\n",
    "- Calculate the error\n",
    "- Use Gradient descent to figure out new weights and bias values\n",
    "- Repeat n times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logistic_Regression:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, n_iter=1000):\n",
    "        '''Initiate the constructor\n",
    "            INPUT:\n",
    "                learning_rate: magnitude of the step\n",
    "                n_iter: number of iterations\n",
    "        '''\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Train the model\n",
    "        INPUTS:\n",
    "            X: the dataset of the features\n",
    "            y: the target\n",
    "        OUTPUTS:\n",
    "            The model\n",
    "        '''\n",
    "\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "        \n",
    "        # initialize the parameters:\n",
    "        self.weights = np.zeros(self.n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            return self.update_weights()\n",
    "\n",
    "    def update_weights(self):\n",
    "        '''Update of the weights with Gradient descent'''\n",
    "\n",
    "        # we compute the prediction (the probability)\n",
    "        y_pred = 1 / (1 + np.exp( - (np.dot(self.X, self.weights) + self.bias)))\n",
    "\n",
    "        # update the weights:\n",
    "        # w_j = w_j - (alpha / n) * S(p_hat - y_i)xij\n",
    "        # b = b - (alpha / n) * S(p_hat - y_i)\n",
    "        dw = (1 / self.n_samples) * np.dot(self.X.T, (y_pred - self.y))\n",
    "        db = (1 / self.n_samples) * np.sum(y_pred - self.y)\n",
    "\n",
    "        self.weights = self.weights - self.learning_rate*dw\n",
    "        self.bias = self.bias - self.learning_rate*db\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = 1 / (1 + np.exp( - (X.dot(self.weights) + self.bias)))\n",
    "        y_pred = np.where(y_pred > 0.5 , 1 , 0)\n",
    "        return y_pred\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasets.load_breast_cancer()\n",
    "X, y = df.data, df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    accuracy = sum(y_true==y_pred)/len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Logistic_Regression(learning_rate=0.01, n_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assess the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3544600938967136"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_train_pred = classifier.predict(X_train)\n",
    "training_data_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "training_data_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42657342657342656"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy on the training data\n",
    "y_test_pred = classifier.predict(X_test)\n",
    "test_data_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_data_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a data point:  \n",
    "- Put the values from the data point into the equation $\\hat{y} = \\frac{1}{1 + \\exp(-w+b)}$\n",
    "- Choose the label based on the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.616e+01, 2.154e+01, 1.062e+02, 8.098e+02, 1.008e-01, 1.284e-01,\n",
       "        1.043e-01, 5.613e-02, 2.160e-01, 5.891e-02, 4.332e-01, 1.265e+00,\n",
       "        2.844e+00, 4.368e+01, 4.877e-03, 1.952e-02, 2.219e-02, 9.231e-03,\n",
       "        1.535e-02, 2.373e-03, 1.947e+01, 3.168e+01, 1.297e+02, 1.175e+03,\n",
       "        1.395e-01, 3.055e-01, 2.992e-01, 1.312e-01, 3.480e-01, 7.619e-02],\n",
       "       [1.195e+01, 1.496e+01, 7.723e+01, 4.267e+02, 1.158e-01, 1.206e-01,\n",
       "        1.171e-02, 1.787e-02, 2.459e-01, 6.581e-02, 3.610e-01, 1.050e+00,\n",
       "        2.455e+00, 2.665e+01, 5.800e-03, 2.417e-02, 7.816e-03, 1.052e-02,\n",
       "        2.734e-02, 3.114e-03, 1.281e+01, 1.772e+01, 8.309e+01, 4.962e+02,\n",
       "        1.293e-01, 1.885e-01, 3.122e-02, 4.766e-02, 3.124e-01, 7.590e-02],\n",
       "       [1.340e+01, 2.052e+01, 8.864e+01, 5.567e+02, 1.106e-01, 1.469e-01,\n",
       "        1.445e-01, 8.172e-02, 2.116e-01, 7.325e-02, 3.906e-01, 9.306e-01,\n",
       "        3.093e+00, 3.367e+01, 5.414e-03, 2.265e-02, 3.452e-02, 1.334e-02,\n",
       "        1.705e-02, 4.005e-03, 1.641e+01, 2.966e+01, 1.133e+02, 8.444e+02,\n",
       "        1.574e-01, 3.856e-01, 5.106e-01, 2.051e-01, 3.585e-01, 1.109e-01],\n",
       "       [1.625e+01, 1.951e+01, 1.098e+02, 8.158e+02, 1.026e-01, 1.893e-01,\n",
       "        2.236e-01, 9.194e-02, 2.151e-01, 6.578e-02, 3.147e-01, 9.857e-01,\n",
       "        3.070e+00, 3.312e+01, 9.197e-03, 5.470e-02, 8.079e-02, 2.215e-02,\n",
       "        2.773e-02, 6.355e-03, 1.739e+01, 2.305e+01, 1.221e+02, 9.397e+02,\n",
       "        1.377e-01, 4.462e-01, 5.897e-01, 1.775e-01, 3.318e-01, 9.136e-02],\n",
       "       [7.691e+00, 2.544e+01, 4.834e+01, 1.704e+02, 8.668e-02, 1.199e-01,\n",
       "        9.252e-02, 1.364e-02, 2.037e-01, 7.751e-02, 2.196e-01, 1.479e+00,\n",
       "        1.445e+00, 1.173e+01, 1.547e-02, 6.457e-02, 9.252e-02, 1.364e-02,\n",
       "        2.105e-02, 7.551e-03, 8.678e+00, 3.189e+01, 5.449e+01, 2.236e+02,\n",
       "        1.596e-01, 3.064e-01, 3.393e-01, 5.000e-02, 2.790e-01, 1.066e-01]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Logistic_Regression(learning_rate=0.0001)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3986013986013986"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inputs:\n",
    "\n",
    "$$X=\\begin{pmatrix} x_{1,1} & \\ldots & x_{1,k} \\\\ x_{2,1} & \\ldots & x_{2,k} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{n,1} & \\ldots & x_{n,k} \\end{pmatrix}, w=\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_k \\end{pmatrix}, b = \\text{constant}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear model:\n",
    "\n",
    "$$X.w + b = \\begin{pmatrix} x_{1,1} & \\ldots & x_{1,k} \\\\ x_{2,1} & \\ldots & x_{2,k} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{n,1} & \\ldots & x_{n,k} \\end{pmatrix}.\\begin{pmatrix} w_1 \\\\ w_2 \\\\ \\ldots \\\\ w_k \\end{pmatrix} + b = \\begin{pmatrix} x_{1,1}w_1 + & \\ldots & + x_{1,k}w_k + b \\\\ x_{2,1}w_1 + & \\ldots & + x_{2,k}w_k + b \\\\ \\ldots & \\ldots &   \\ldots \\\\ x_{n,1}w_1 + & \\ldots & + x_{n,k}w_k + b \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model prediction (output) is given by:\n",
    "\n",
    "$$\\text{sigmoid}(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p} = h_\\omega(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updates of the weights and bias are given by:\n",
    "\n",
    "$$\\omega_j = \\omega_j - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ] x_{i,j}$$\n",
    "\n",
    "$$b = b - \\frac{\\alpha}{n} \\sum[h_\\omega(x_i) - y_i ]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\omega$ using linear algebra formula:\n",
    "\n",
    "$$\\omega = X^t.(\\hat{p} - y) = \\begin{pmatrix} x_{1,1} & \\ldots & x_{1,n} \\\\ x_{2,1} & \\ldots & x_{2,n} \\\\ \\ldots & x_{i,j} & \\ldots \\\\ x_{k,1} & \\ldots & x_{k,n} \\end{pmatrix}.\\begin{pmatrix} \\hat{p_1} - y_1 \\\\ \\hat{p_2} - y_2 \\\\ \\ldots \\\\ \\hat{p_n} - y_n \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $b$ using linear algebra formula:\n",
    "\n",
    "$$b = \\sum(\\hat{p} - y) = \\sum\\begin{pmatrix} \\hat{p_1} - y_1 \\\\ \\hat{p_2} - y_2 \\\\ \\ldots \\\\ \\hat{p_n} - y_n \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and bias are given by:\n",
    "\n",
    "$$\\text{sigmoid}(X.w+b) = \\frac{1}{1 + \\exp(-X.w+b)}= \\hat{p}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â The Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coefficients significativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wald statistic allows to test the coefficients significativity $\\hat{w_j}$. Wald statistic is given by::    \n",
    "\n",
    "$(\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2$  \n",
    "\n",
    "Under $H_0 : \\{\\hat{w_j} = 0 \\} \\Longrightarrow \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} $ ~ $\\mathcal{N}(0, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The added-value of the variable $X_j$ is only real if the Wald statistic > 4 $(3.84 = 1.96^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Wald > 4$    \n",
    "\n",
    "$\\iff (\\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})})^2 > 4$  \n",
    "\n",
    "$\\iff \\frac{\\hat{w_j}}{\\sigma(\\hat{w_j})} > 2$  \n",
    "\n",
    "$\\iff \\hat{w_j} > 2\\sigma(\\hat{w_j}) $  \n",
    "\n",
    "$\\iff \\hat{w_j} - 2\\sigma(\\hat{w_j}) > 0$  \n",
    "\n",
    "$\\iff \\hat{w_j}$ se trouve Ã  plus de 2 Ã©carts-type de 0  \n",
    "\n",
    "$\\iff $ l'intervalle de confiance de $\\hat{w_j}$ ne contient pas 0 Ã  95%  \n",
    "\n",
    "CQFD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model quality mesure (Deviance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cf. S.TuffÃ©ry p.315"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$n:$ number of observations  \n",
    "$k:$ number of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\omega_k)$ Likelihood of the \"modÃ¨le ajustÃ©\"  \n",
    "\n",
    "$L(\\omega_0)$ Likelihood of the \"modÃ¨le rÃ©duit Ã  la constante\"  \n",
    "\n",
    "$L(\\omega_{max})$ Likelihood of the \"modÃ¨le saturÃ©\". The one the model will compare.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance formula:  \n",
    "\n",
    "$D(\\omega_k) = -2[log(L(\\omega_k)) - log(L(\\omega_{max}))]$  $^{(*)}$\n",
    "\n",
    "As the target is 0 or 1 $\\Longrightarrow L(\\omega_{max})=1 \\Longrightarrow log(L(\\omega_{max}))=0$  \n",
    "\n",
    "$\\Longrightarrow D(\\omega_k) = -2[log(L(\\omega_k))]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(*) $D(\\omega_k) = (\\frac{log(L(\\omega_k))}{log(L(\\omega_{max}))}^2)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the logistic regression is to maximise the Likelihood which is equivalent to minimize the Deviance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Deviance is equivalent to the SCE for the linear regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
